# 4248-project

## Natural Language Inference with Explanation-augmented Dataset: An investigation into its merits and shortcomings

## Overview 

This project was done as part of CS4248: Natural Language Processing, a course in National University of Singapore (NUS).

Our project aims to analyze the shortcomings and performance of baseline models for the extended Stanford Natural Language Inference dataset [e-SNLI](https://github.com/OanaMariaCamburu/e-SNLI), as well as Facebook's [InferSent](https://github.com/facebookresearch/InferSent) for the Natural Language Inference (NLI) task. NLI relates to textual entailment and the project consists of two main components: (1) generation of explanations from given pairs of premise and hypothesis, and (2) prediction of a label based on the premise, hypothesis, and the generated explanation.

## Navigating this Repository

The main scripts are located in the `e-SNLI` and `InferSent_ST-InferSent` directories, each containing Python scripts and notebooks on analysis for e-SNLI and InferSent (also including SentenceTransformer-InferSent) respectively.

The `plots` directory contains various diagrams created from our analysis, and the `pred_outputs` directory contains the predicted outputs generated by the baseline models when run on the test (and sometimes validation) dataset.

## Setup

Make sure that you have Python version 3 or above before running the scripts, as well as the necessary dependencies.

You may refer to, or run, `sh bin/init.sh` to download necessary models and data.

## Exploratory Data Analysis (EDA)

All scripts and data related to EDA can be found in `e-SNLI/EDA`.

There are 2 directories in `e-SNLI/EDA`:
1. `pre-run`
- Content in this directory refers to any analysis done before running of pre-trained models. They involve the combined analysis of train, dev and test data, which amounts to a total of 500,000+ rows.
  - `pre-run-EDA-sentence-distribution.ipynb` analyses the premise-hypothesis label distribution across the dataset (contradiction / entailment / neutral)
  - `pre-run_all_EDA.ipynb` analyses the cosine similarities of premise-hypothesis pairs grouped by label. Additionally, it involves the manual inspection of datapoints which deviate significantly from the median cosine similarities.
  - `pre-run_all_data_0.csv` and `pre-run_all_data_1.csv` is data used by `pre-run_all_EDA.ipynb`.

2. `post-run`
- Content in this directory refers to any analyses done after running of pre-trained models on the test dataset, where predictions have been generated. The main analyses done here are:
  - Error analysis of running e-SNLI model (ExplainThenPredictAttention) in the notebook `post-run_esnli_error_analysis.ipynb`.
  - Human Evaluation and analysis of the explanations generated from running e-SNLI model (ExplainThenPredictAttention) in the directory `Predicted Explanation Evaluation`, in the 2 notebooks `EDA - Analyse Explanation Quality.ipynb` and `EDA - Sensibility of Predicted Explanation.ipynb`.
  - Analysis of ExplainThenPredictAttention model robustness through running it against the HANS dataset which tests 3 fallable syntactic heuristics: lexical overlap, subsequence and constituent, in the notebook `HANS Heuristics`/`post_run_analyze_test_heuristics_HANS.ipynb`.
  - Analysis of ExplainThenPredictAttention model robustness through examining its output after swapping premise and hypothesis inputs, which is located in the directory `Swap Sentences`.
  
## InferSent model
We used the pre-trained encoder from the [original repository](https://github.com/facebookresearch/InferSent) and place it in the `encoder` directory:
- InferSent encoder: https://drive.google.com/file/d/1csv3pP-tikFZHWLEVLDMqSVDnPx77AQz/view?usp=sharing

We had to train the NLI model from scratch since the original repository did not provide their trained NLI model (except the trained encoder).

Before proceeding, please download the [GloVe embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) and unzip and place the .txt file to the `dataset/GloVe/` directory.

Please run the notebook `InferSent_ST-InferSent/infersent_training.ipynb` and follow instructions in the notebook with default settings to train the InferSent NLI model.

The trained InferSent model can be downloaded at:
- trained InferSent NLI model: https://drive.google.com/file/d/19HBWkX-FY6inoAjseRJ4zhp2GrLM4DjB/view?usp=sharing  

You can either run the training notebook (and rename the output file to `model84.pickle`) or just download the above trained model `model84.pickle` and place in the `savedir` directory.

For batch inference/evaluation, please run the notebook `InferSent_ST-InferSent/eval_preds_is.ipynb`.

## SentenceTransformer-InferSent model
We used the pre-trained Sentence Transformer model ('all-mpnet-base-v2') from the [SBERT.NET](https://sbert.net/) as the encoder using the InferSent architecture.

Please run the notebook `InferSent_ST-InferSent/sentransformer_training.ipynb` and follow instructions in the notebook with default settings to train the SentenceTransformer-InferSent NLI model.

The trained InferSent model can be downloaded at:
- trained InferSent NLI model: https://drive.google.com/file/d/1wnKo6wgmQlW5rV5pvIajQsEoDOAQIHf7/view?usp=sharing

You can either run the training notebook (and rename the output file to `modelst.pickle`) or download the above trained model `modelst.pickle` and place it in the `savedir` directory.

For batch inference/evaluation, please run the notebook `InferSent_ST-InferSent/eval_preds_st.ipynb`.

## e-SNLI models
We used the pre-trained models from the [original repository](https://github.com/OanaMariaCamburu/e-SNLI):
- ExplainThenPredictAttention: https://drive.google.com/file/d/1l7dnml7mDnT72QrwZMmA7VGIsWjVpQT6/view?usp=sharing
- ExplainationsToLabels: https://drive.google.com/file/d/1_rFGlFYHSJ1xqjA2lDjzBvO5mf7INo1A/view?usp=sharing

In order to run the models and produce the output for analysis, we used the code from [this repository](https://github.com/vibhavagarwal5/e-SNLI) (this is not the original repository since that does not support Python 3).

After cloning the repository above, go to `attention` folder where `launch_eval_attention_bottom.py` is located. To produce the output, run this command:
```sh
python launch_eval_attention_bottom.py [option]
```
Options:
- `--directory`: Path to the folder containing ExplainAndPredictAttention model. 
- `--state_path`: The file name of ExplainAndPredictAttention model.
- `-eval_batch_size`: The batch size used when evaluating the model. The default value is 32.
- `--directory_expl_to_labels`: Path to the folder containing ExplainationsToLabels model. 
- `--state_path_expl_to_labels`: The file name of ExplainationsToLabels model. 

We ran the this model against the e-SNLI test set (`dataset/esnli_test.csv`) and obtained the result (`e-SNLI/EDA/post-run/post-run_esnli_test_data.csv`).

In addition, we also ran the model against a subset of HANS dataset (`dataset/heuristic_eval.csv`) and obtained the result (`e-SNLI/EDA/post-run/HANS Heuristics/HANS_output.csv`). Link to the original HANS repository: https://github.com/tommccoy1/hans
