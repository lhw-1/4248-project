{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "040f171d",
   "metadata": {},
   "source": [
    "Run this notebook by setting the variable `snli_or_hans` to 'snli' or 'hans' to generate predictions using the SNLI dataset or Heuristic Analysis for NLI Systems (HANS) dataset using the network trained with SentenceTransformer as the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89cc90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (2.2.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (0.14.0+cu117)\n",
      "Requirement already satisfied: scipy in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (4.23.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (0.11.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (1.13.0+cu117)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from torchvision->sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ktjam\\.conda\\envs\\nlp202303\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584fc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import InferSent\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#eval using SNLI or HANS (heuristics dataset)\n",
    "snli_or_hans = 'hans'  #'snli' or 'hans'\n",
    "\n",
    "#specify filepath for predictions (relative to src)\n",
    "if snli_or_hans == 'snli':\n",
    "    valid_filepath = '../dataset/esnli_test.csv'\n",
    "elif snli_or_hans == 'hans':\n",
    "    valid_filepath = '../dataset/heuristic_eval.csv'\n",
    "#specify filepath for trained InferSent model\n",
    "net_filepath = '../encoder/modelst.pickle'\n",
    "#csv saved in '../pred_outputs/output_st+{snli_or_hans}+.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d161f839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ktjam\\\\YKT\\\\MComp AI Classes\\\\CS4248 Natural Language Processing\\\\Github_project\\\\4248-project\\\\src'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928848f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = model_st  #eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*self.enc_lstm_dim\n",
    "        self.inputdim = self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        # s1 : (s1, s1_len)\n",
    "        #u = self.encoder(s1)\n",
    "        #v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "def get_batch(batch):  #, word_vec, emb_dim=300):\n",
    "    # # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    # lengths = np.array([len(x) for x in batch])\n",
    "    # max_len = np.max(lengths)\n",
    "    # embed = np.zeros((max_len, len(batch), emb_dim))\n",
    "\n",
    "    # for i in range(len(batch)):\n",
    "    #     for j in range(len(batch[i])):\n",
    "    #         embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    # return torch.from_numpy(embed).float(), lengths\n",
    "    return torch.tensor(nli_net.encoder.encode(batch)).cuda()\n",
    "\n",
    "def evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output.csv', final_eval=True):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "    \n",
    "    input = pd.read_csv(input_filepath, usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "    #input.rename(columns={'Sentence1':'s1', 'Sentence2':'s2'}, inplace=True)\n",
    "    #print(input.columns)\n",
    "    #map label to int\n",
    "    #label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    input['label'] = input['gold_label'].apply(lambda x: label_to_int[x]).tolist()\n",
    "    \n",
    "    # s1 = input['Sentence1'].tolist()\n",
    "    # s2 = input['Sentence2'].tolist()\n",
    "    # s1 = np.array([\n",
    "    #                 ['<s>'] + \\\n",
    "    #                 [word for word in word_tokenize(str(sent)) if word in word_vec] + \\\n",
    "    #                 ['</s>'] for sent in s1\n",
    "    #               ])\n",
    "    # s2 = np.array([\n",
    "    #                 ['<s>'] + \\\n",
    "    #                 [word for word in word_tokenize(str(sent)) if word in word_vec] + \\\n",
    "    #                 ['</s>'] for sent in s2\n",
    "    #               ])\n",
    "    s1 = input['Sentence1']\n",
    "    s2 = input['Sentence2']\n",
    "    target = input['label']\n",
    "\n",
    "    #for generation of csv with predictions\n",
    "    preds = []\n",
    "    \n",
    "    for q in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        # s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        # s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        # s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        # tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        s1_batch = get_batch(s1[q:q + params.batch_size].tolist())\n",
    "        s2_batch = get_batch(s2[q:q + params.batch_size].tolist())\n",
    "        tgt_batch = torch.LongTensor(target[q:q + params.batch_size].tolist()).cuda()\n",
    "\n",
    "        # model forward\n",
    "        #output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        output = nli_net(s1_batch, s2_batch)\n",
    "        \n",
    "        preds.extend(output.data.max(1)[1].detach().cpu().tolist())\n",
    "        if snli_or_hans == 'hans':\n",
    "            preds = [0 if (pred == 0) else 1 for pred in preds]\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "    #save csv file\n",
    "    if not os.path.exists('../pred_outputs'):\n",
    "                os.makedirs('../pred_outputs')\n",
    "    pd.DataFrame({'label': target, 'Sentence1': s1, 'Sentence2': s2, 'prediction': preds}).to_csv(\n",
    "        '../pred_outputs/'+output_filename, index=False\n",
    "    )\n",
    "        \n",
    "    # save model\n",
    "    eval_acc = 100 * correct/len(s1)  #round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy: {0}'.format(eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : mean accuracy:\\\n",
    "              {1}'.format(eval_acc))\n",
    "\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77e291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLINet(\n",
       "  (encoder): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "    (2): Normalize()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.0, inplace=False)\n",
       "    (1): Linear(in_features=3072, out_features=512, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Dropout(p=0.0, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "# model_version = 1\n",
    "# MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "# params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "#                 'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "# model = InferSent(params_model)\n",
    "# model.load_state_dict(torch.load(MODEL_PATH))\n",
    "MODEL_PATH = \"../encoder/modelst.pickle.encoder.pkl\"\n",
    "model_st = SentenceTransformer('all-mpnet-base-v2')\n",
    "model_st.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#or force not to use cuda\n",
    "#use_cuda = False\n",
    "model_st = model_st.cuda() if use_cuda else model\n",
    "\n",
    "valid = pd.read_csv(valid_filepath, usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "#valid.info()\n",
    "\n",
    "if snli_or_hans == 'snli':\n",
    "    #map label to int\n",
    "    label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "elif snli_or_hans == 'hans':\n",
    "    #for heuristics dataset HANS\n",
    "    label_to_int = {'entailment': 0, 'non-entailment': 1}\n",
    "\n",
    "valid['label'] = valid['gold_label'].apply(lambda x: label_to_int[x])\n",
    "\n",
    "#converts DataFrames to dict\n",
    "valid = valid.to_dict(orient='list')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "parser.add_argument(\"--word_emb_path\", type=str, default=\"dataset/GloVe/glove.840B.300d.txt\", help=\"word embedding file path\")\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)  #64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSentV1', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=768, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default=300, help=\"word embedding dimension\")\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "config_nli_model = {\n",
    "    'n_words'        :  1          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "nli_net = NLINet(config_nli_model)\n",
    "\n",
    "# Run best model on test set.\n",
    "nli_net.load_state_dict(torch.load(net_filepath))\n",
    "\n",
    "nli_net.to('cuda')\n",
    "\n",
    "#print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "#evaluate(1e6, 'valid', True)\n",
    "#evaluate(0, 'test', True)\n",
    "\n",
    "#evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output_st.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a47550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalgrep : accuracy: 48.58271026611328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(48.5827)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate predictions and returns overall accuracy\n",
    "evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output_st_'+snli_or_hans+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5909555",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_csv(\"../pred_outputs/output_st_\"+snli_or_hans+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e64fbd5",
   "metadata": {},
   "source": [
    "Stop here if snli_or_hans == 'snli'. The following cell prints the accuracies for the different segments in HANS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52e9fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment:\n",
      "\t Accuracy for lexical_overlap: 85.90%\n",
      "\t Accuracy for subsequence: 90.78%\n",
      "\t Accuracy for constituent: 76.96%\n",
      "non-entailment:\n",
      "\t Accuracy for lexical_overlap: 12.86%\n",
      "\t Accuracy for subsequence: 20.44%\n",
      "\t Accuracy for constituent: 23.90%\n"
     ]
    }
   ],
   "source": [
    "#Run this cell if snli_or_hans == 'hans'\n",
    "add_cols = pd.read_csv(valid_filepath)[['gold_label', 'heuristic']]\n",
    "preds = pd.concat([preds, add_cols], axis=1)\n",
    "#for HANS dataset, sub labels = 'lexical_overlap', 'subsequence', 'constituent'\n",
    "heuristics = ['lexical_overlap', 'subsequence', 'constituent']\n",
    "#for HANS dataset, classes = 'entailment' and 'non-entailment'\n",
    "for label in ['entailment', 'non-entailment']:\n",
    "    tmp = preds.loc[preds['gold_label'] == label, :]\n",
    "    print(f\"{label}:\")\n",
    "    for heu in heuristics:\n",
    "        tmp1 = tmp.loc[tmp['heuristic'] == heu, :]\n",
    "        print(f\"\\t Accuracy for {heu}: {sum(tmp1['label'] == tmp1['prediction'])/(len(tmp1)+0.000001)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d7f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e3f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
