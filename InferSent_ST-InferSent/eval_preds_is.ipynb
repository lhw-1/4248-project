{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "807713db",
   "metadata": {},
   "source": [
    "Run this notebook by setting the variable `snli_or_hans` to 'snli' or 'hans' to generate predictions using the SNLI validation or test dataset, or Heuristics Analysis for NLI Systems (HANS) dataset using the InferSent network trained with the original InferSent encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "584fc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import InferSent\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#eval using SNLI or HANS (heuristics dataset)\n",
    "snli_or_hans = 'hans'  #'snli' or 'hans'\n",
    "\n",
    "#specify filepath for predictions (relative to src)\n",
    "if snli_or_hans == 'snli':\n",
    "    valid_filepath = '../dataset/esnli_test.csv'\n",
    "elif snli_or_hans == 'hans':\n",
    "    valid_filepath = '../dataset/heuristic_eval.csv'\n",
    "#specify filepath for trained InferSent model\n",
    "net_filepath = '../savedir/model84.pickle'\n",
    "glove_path = '../GloVe/glove.840B.300d.txt'\n",
    "#csv saved in '../pred_outputs/output_is_+{snli_or_hans}+.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d161f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            #break\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in word_tokenize(str(sent)):\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "928848f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = model  #eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "def get_batch(batch, word_vec, emb_dim=300):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), emb_dim))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "def evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output.csv', final_eval=True):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "    \n",
    "    input = pd.read_csv(input_filepath, usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "    #input.rename(columns={'Sentence1':'s1', 'Sentence2':'s2'}, inplace=True)\n",
    "    #print(input.columns)\n",
    "    #map label to int\n",
    "    #label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    target = input['gold_label'].apply(lambda x: label_to_int[x]).tolist()\n",
    "    \n",
    "    s1 = input['Sentence1'].tolist()\n",
    "    s2 = input['Sentence2'].tolist()\n",
    "    s1 = np.array([\n",
    "                    ['<s>'] + \\\n",
    "                    [word for word in word_tokenize(str(sent)) if word in word_vec] + \\\n",
    "                    ['</s>'] for sent in s1\n",
    "                  ])\n",
    "    s2 = np.array([\n",
    "                    ['<s>'] + \\\n",
    "                    [word for word in word_tokenize(str(sent)) if word in word_vec] + \\\n",
    "                    ['</s>'] for sent in s2\n",
    "                  ])\n",
    "\n",
    "    #for generation of csv with predictions\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        \n",
    "        preds.extend(output.data.max(1)[1].detach().cpu().tolist())\n",
    "        if snli_or_hans == 'hans':\n",
    "            preds = [0 if (pred == 0) else 1 for pred in preds]\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "    #save csv file\n",
    "    if not os.path.exists('../pred_outputs'):\n",
    "                os.makedirs('../pred_outputs')\n",
    "    pd.DataFrame({'label': target, 'Sentence1': s1, 'Sentence2': s2, 'prediction': preds}).to_csv(\n",
    "        '../pred_outputs/'+output_filename, index=False\n",
    "    )\n",
    "        \n",
    "    # save model\n",
    "    eval_acc = 100 * correct/len(s1)  #round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy: {0}'.format(eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : mean accuracy:\\\n",
    "              {1}'.format(eval_acc))\n",
    "\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d77e291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 183(/183) words with glove vectors\n",
      "Vocab size : 183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NLINet(\n",
       "  (encoder): InferSent(\n",
       "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.0, inplace=False)\n",
       "    (1): Linear(in_features=16384, out_features=512, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Dropout(p=0.0, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "#MODEL_PATH = \"../encoder/modelst.pickle.encoder.pkl\"\n",
    "#model_st = SentenceTransformer('all-mpnet-base-v2')\n",
    "#model_st.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#or force not to use cuda\n",
    "#use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../GloVe/glove.840B.300d.txt' if model_version == 1 else '../fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "valid = pd.read_csv(valid_filepath, usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "#valid.info()\n",
    "\n",
    "if snli_or_hans == 'snli':\n",
    "    #map label to int\n",
    "    label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "elif snli_or_hans == 'hans':\n",
    "    #for heuristics dataset HANS\n",
    "    label_to_int = {'entailment': 0, 'non-entailment': 1}\n",
    "\n",
    "valid['label'] = valid['gold_label'].apply(lambda x: label_to_int[x])\n",
    "\n",
    "#converts DataFrames to dict\n",
    "valid = valid.to_dict(orient='list')\n",
    "\n",
    "word_vec = build_vocab(valid['Sentence1'] + valid['Sentence2'], glove_path)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "parser.add_argument(\"--word_emb_path\", type=str, default=\"dataset/GloVe/glove.840B.300d.txt\", help=\"word embedding file path\")\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)  #64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSentV1', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default=300, help=\"word embedding dimension\")\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "config_nli_model = {\n",
    "    'n_words'        :  1          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "nli_net = NLINet(config_nli_model)\n",
    "\n",
    "# Run best model on test set.\n",
    "nli_net.load_state_dict(torch.load(net_filepath))\n",
    "\n",
    "nli_net.to('cuda')\n",
    "\n",
    "#print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "#evaluate(1e6, 'valid', True)\n",
    "#evaluate(0, 'test', True)\n",
    "\n",
    "#evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output_st.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a47550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktjam\\AppData\\Local\\Temp\\ipykernel_21604\\13107922.py:76: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  s1 = np.array([\n",
      "C:\\Users\\ktjam\\AppData\\Local\\Temp\\ipykernel_21604\\13107922.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  s2 = np.array([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalgrep : accuracy: 48.392330169677734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(48.3923)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate predictions and returns overall accuracy\n",
    "evaluate_preds(nli_net, input_filepath=valid_filepath, output_filename='output_is_'+snli_or_hans+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5909555",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_csv(\"../pred_outputs/output_is_\"+snli_or_hans+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26af81c",
   "metadata": {},
   "source": [
    "Stop here if snli_or_hans == 'snli'. The following cell prints the accuracies for different segments in HANS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c13f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment:\n",
      "\t Accuracy for lexical_overlap: 98.34%\n",
      "\t Accuracy for subsequence: 99.24%\n",
      "\t Accuracy for constituent: 96.82%\n",
      "non-entailment:\n",
      "\t Accuracy for lexical_overlap: 12.96%\n",
      "\t Accuracy for subsequence: 2.70%\n",
      "\t Accuracy for constituent: 6.14%\n"
     ]
    }
   ],
   "source": [
    "#Run this cell if snli_or_hans == 'hans'\n",
    "add_cols = pd.read_csv(valid_filepath)[['gold_label', 'heuristic']]\n",
    "preds = pd.concat([preds, add_cols], axis=1)\n",
    "#for HANS dataset, sub labels = 'lexical_overlap', 'subsequence', 'constituent'\n",
    "heuristics = ['lexical_overlap', 'subsequence', 'constituent']\n",
    "#for HANS dataset, classes = 'entailment' and 'non-entailment'\n",
    "for label in ['entailment', 'non-entailment']:\n",
    "    tmp = preds.loc[preds['gold_label'] == label, :]\n",
    "    print(f\"{label}:\")\n",
    "    for heu in heuristics:\n",
    "        tmp1 = tmp.loc[tmp['heuristic'] == heu, :]\n",
    "        print(f\"\\t Accuracy for {heu}: {sum(tmp1['label'] == tmp1['prediction'])/(len(tmp1)+0.000001)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d7f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
