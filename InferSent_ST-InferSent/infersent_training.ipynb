{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook with default settings to train the InferSent NLI model using the original trained InferSent encoder. The encoder and NLI model will be saved as \"../savedir/model.pickle.encoder.pkl\" and \"../savedir/model.pickle\", respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#or force not to use cuda\n",
    "#use_cuda = False\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../GloVe/glove.840B.300d.txt' if model_version == 1 else '../fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549367 entries, 0 to 549366\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   gold_label  549367 non-null  object\n",
      " 1   Sentence1   549367 non-null  object\n",
      " 2   Sentence2   549361 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tmp1 = pd.read_csv('../dataset/esnli_train_1.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "tmp2 = pd.read_csv('../dataset/esnli_train_2.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "train = pd.concat([tmp1, tmp2], ignore_index=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9842 entries, 0 to 9841\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9842 non-null   object\n",
      " 1   Sentence1   9842 non-null   object\n",
      " 2   Sentence2   9842 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.8+ KB\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('../dataset/esnli_dev.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9824 entries, 0 to 9823\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9824 non-null   object\n",
      " 1   Sentence1   9824 non-null   object\n",
      " 2   Sentence2   9824 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../dataset/esnli_test.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map label to int\n",
    "label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add label int\n",
    "train['label'] = train['gold_label'].apply(lambda x: label_to_int[x])\n",
    "valid['label'] = valid['gold_label'].apply(lambda x: label_to_int[x])\n",
    "test['label'] = test['gold_label'].apply(lambda x: label_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            #break\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in word_tokenize(str(sent)):\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "            #print(word)\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../GloVe/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts DataFrames to dict\n",
    "train = train.to_dict(orient='list')\n",
    "valid = valid.to_dict(orient='list')\n",
    "test = test.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38909(/43393) words with glove vectors\n",
      "Vocab size : 38909\n"
     ]
    }
   ],
   "source": [
    "word_vec = build_vocab(train['Sentence1'] + train['Sentence2'] +\n",
    "                       valid['Sentence1'] + valid['Sentence2'] +\n",
    "                       test['Sentence1'] + test['Sentence2'], glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = model  #eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktjam\\AppData\\Local\\Temp\\ipykernel_26588\\550645181.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  eval(data_type)[split] = np.array([['<s>'] + \\\n"
     ]
    }
   ],
   "source": [
    "for split in ['Sentence1', 'Sentence2']:\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] + \\\n",
    "            [word for word in word_tokenize(str(sent)) if word in word_vec] + \\\n",
    "            ['</s>'] for sent in eval(data_type)[split]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = np.array(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='../savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "parser.add_argument(\"--word_emb_path\", type=str, default=\"../dataset/GloVe/glove.840B.300d.txt\", help=\"word embedding file path\")\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)  #64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSentV1', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default=300, help=\"word embedding dimension\")\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "config_nli_model = {\n",
    "    'n_words'        :  len(word_vec)          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    #expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    #assert expected_args[:2] == ['self', 'params']\n",
    "    #if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "    #    raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "    #        str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['Sentence1']))\n",
    "\n",
    "    s1 = train['Sentence1'][permutation]\n",
    "    s2 = train['Sentence2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        #print(type(loss))\n",
    "        all_costs.append(loss.item())  #.data[0])\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm.cpu())\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "        if len(all_costs) == 100:\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)),\n",
    "                            100.*correct/(stidx+k)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = 100 * correct/len(s1)  #round(100 * correct/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch, word_vec, emb_dim=300):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), emb_dim))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['Sentence1'] if eval_type == 'valid' else test['Sentence1']\n",
    "    s2 = valid['Sentence2'] if eval_type == 'valid' else test['Sentence2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "        \n",
    "    # save model\n",
    "    eval_acc = 100 * correct/len(s1)  #round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n",
      "6336 ; loss 1.1 ; sentence/s 264 ; words/s 15283 ; accuracy train : 33.078125\n",
      "12736 ; loss 1.1 ; sentence/s 282 ; words/s 16184 ; accuracy train : 33.875\n",
      "19136 ; loss 1.1 ; sentence/s 277 ; words/s 16420 ; accuracy train : 34.30729293823242\n",
      "25536 ; loss 1.1 ; sentence/s 282 ; words/s 16326 ; accuracy train : 35.2265625\n",
      "31936 ; loss 1.1 ; sentence/s 279 ; words/s 16201 ; accuracy train : 35.22187423706055\n",
      "38336 ; loss 1.1 ; sentence/s 280 ; words/s 16479 ; accuracy train : 34.953125\n",
      "44736 ; loss 1.09 ; sentence/s 281 ; words/s 16371 ; accuracy train : 34.96651840209961\n",
      "51136 ; loss 1.09 ; sentence/s 281 ; words/s 16535 ; accuracy train : 34.916015625\n",
      "57536 ; loss 1.09 ; sentence/s 281 ; words/s 16055 ; accuracy train : 34.82465362548828\n",
      "63936 ; loss 1.09 ; sentence/s 276 ; words/s 16046 ; accuracy train : 34.80937576293945\n",
      "70336 ; loss 1.09 ; sentence/s 281 ; words/s 16079 ; accuracy train : 34.8636360168457\n",
      "76736 ; loss 1.09 ; sentence/s 284 ; words/s 15994 ; accuracy train : 35.08203125\n",
      "83136 ; loss 1.09 ; sentence/s 283 ; words/s 16052 ; accuracy train : 35.40745162963867\n",
      "89536 ; loss 1.09 ; sentence/s 275 ; words/s 16187 ; accuracy train : 35.95981979370117\n",
      "95936 ; loss 1.09 ; sentence/s 273 ; words/s 16023 ; accuracy train : 36.48125076293945\n",
      "102336 ; loss 1.09 ; sentence/s 268 ; words/s 16063 ; accuracy train : 36.748046875\n",
      "108736 ; loss 1.09 ; sentence/s 273 ; words/s 15906 ; accuracy train : 37.00275802612305\n",
      "115136 ; loss 1.09 ; sentence/s 271 ; words/s 15904 ; accuracy train : 37.43663024902344\n",
      "121536 ; loss 1.09 ; sentence/s 264 ; words/s 15734 ; accuracy train : 37.92434310913086\n",
      "127936 ; loss 1.09 ; sentence/s 275 ; words/s 15734 ; accuracy train : 38.5859375\n",
      "134336 ; loss 1.09 ; sentence/s 277 ; words/s 16169 ; accuracy train : 39.28571319580078\n",
      "140736 ; loss 1.08 ; sentence/s 274 ; words/s 15849 ; accuracy train : 39.76704406738281\n",
      "147136 ; loss 1.08 ; sentence/s 272 ; words/s 15744 ; accuracy train : 40.2404899597168\n",
      "153536 ; loss 1.08 ; sentence/s 272 ; words/s 15345 ; accuracy train : 40.81901168823242\n",
      "159936 ; loss 1.08 ; sentence/s 276 ; words/s 16029 ; accuracy train : 41.25312423706055\n",
      "166336 ; loss 1.08 ; sentence/s 280 ; words/s 16087 ; accuracy train : 41.78425598144531\n",
      "172736 ; loss 1.08 ; sentence/s 278 ; words/s 16172 ; accuracy train : 42.32002258300781\n",
      "179136 ; loss 1.08 ; sentence/s 283 ; words/s 16090 ; accuracy train : 42.79073715209961\n",
      "185536 ; loss 1.08 ; sentence/s 276 ; words/s 15909 ; accuracy train : 43.28071212768555\n",
      "191936 ; loss 1.08 ; sentence/s 276 ; words/s 16107 ; accuracy train : 43.712501525878906\n",
      "198336 ; loss 1.08 ; sentence/s 274 ; words/s 16107 ; accuracy train : 44.22328567504883\n",
      "204736 ; loss 1.08 ; sentence/s 279 ; words/s 16328 ; accuracy train : 44.70068359375\n",
      "211136 ; loss 1.07 ; sentence/s 280 ; words/s 16072 ; accuracy train : 45.20123291015625\n",
      "217536 ; loss 1.07 ; sentence/s 276 ; words/s 16204 ; accuracy train : 45.727020263671875\n",
      "223936 ; loss 1.07 ; sentence/s 273 ; words/s 16082 ; accuracy train : 46.25714111328125\n",
      "230336 ; loss 1.07 ; sentence/s 271 ; words/s 16074 ; accuracy train : 46.70269012451172\n",
      "236736 ; loss 1.07 ; sentence/s 264 ; words/s 15511 ; accuracy train : 47.0688362121582\n",
      "243136 ; loss 1.07 ; sentence/s 274 ; words/s 15600 ; accuracy train : 47.46916198730469\n",
      "249536 ; loss 1.07 ; sentence/s 276 ; words/s 16039 ; accuracy train : 47.752403259277344\n",
      "255936 ; loss 1.07 ; sentence/s 276 ; words/s 15695 ; accuracy train : 48.05781173706055\n",
      "262336 ; loss 1.06 ; sentence/s 275 ; words/s 16047 ; accuracy train : 48.37309265136719\n",
      "268736 ; loss 1.06 ; sentence/s 270 ; words/s 15968 ; accuracy train : 48.7414436340332\n",
      "275136 ; loss 1.06 ; sentence/s 275 ; words/s 16733 ; accuracy train : 49.125362396240234\n",
      "281536 ; loss 1.06 ; sentence/s 280 ; words/s 16307 ; accuracy train : 49.5042610168457\n",
      "287936 ; loss 1.06 ; sentence/s 278 ; words/s 16376 ; accuracy train : 49.870487213134766\n",
      "294336 ; loss 1.06 ; sentence/s 274 ; words/s 16114 ; accuracy train : 50.16474151611328\n",
      "300736 ; loss 1.05 ; sentence/s 275 ; words/s 15876 ; accuracy train : 50.43949508666992\n",
      "307136 ; loss 1.05 ; sentence/s 273 ; words/s 16111 ; accuracy train : 50.677734375\n",
      "313536 ; loss 1.05 ; sentence/s 274 ; words/s 16116 ; accuracy train : 50.899234771728516\n",
      "319936 ; loss 1.05 ; sentence/s 279 ; words/s 15938 ; accuracy train : 51.154998779296875\n",
      "326336 ; loss 1.05 ; sentence/s 279 ; words/s 16257 ; accuracy train : 51.39736557006836\n",
      "332736 ; loss 1.04 ; sentence/s 276 ; words/s 15841 ; accuracy train : 51.65024185180664\n",
      "339136 ; loss 1.04 ; sentence/s 274 ; words/s 16721 ; accuracy train : 51.87382125854492\n",
      "345536 ; loss 1.04 ; sentence/s 278 ; words/s 16007 ; accuracy train : 52.102142333984375\n",
      "351936 ; loss 1.04 ; sentence/s 277 ; words/s 15955 ; accuracy train : 52.35710144042969\n",
      "358336 ; loss 1.04 ; sentence/s 277 ; words/s 15945 ; accuracy train : 52.587890625\n",
      "364736 ; loss 1.03 ; sentence/s 277 ; words/s 15870 ; accuracy train : 52.82236862182617\n",
      "371136 ; loss 1.03 ; sentence/s 274 ; words/s 16179 ; accuracy train : 53.04660415649414\n",
      "377536 ; loss 1.03 ; sentence/s 276 ; words/s 15669 ; accuracy train : 53.268802642822266\n",
      "383936 ; loss 1.03 ; sentence/s 277 ; words/s 15979 ; accuracy train : 53.45885467529297\n",
      "390336 ; loss 1.02 ; sentence/s 285 ; words/s 15883 ; accuracy train : 53.6690559387207\n",
      "396736 ; loss 1.02 ; sentence/s 277 ; words/s 16090 ; accuracy train : 53.88608932495117\n",
      "403136 ; loss 1.01 ; sentence/s 276 ; words/s 15990 ; accuracy train : 54.11160659790039\n",
      "409536 ; loss 1.01 ; sentence/s 275 ; words/s 15992 ; accuracy train : 54.2998046875\n",
      "415936 ; loss 1.01 ; sentence/s 277 ; words/s 16265 ; accuracy train : 54.489662170410156\n",
      "422336 ; loss 1.0 ; sentence/s 274 ; words/s 16198 ; accuracy train : 54.678504943847656\n",
      "428736 ; loss 1.0 ; sentence/s 272 ; words/s 16101 ; accuracy train : 54.8591423034668\n",
      "435136 ; loss 1.0 ; sentence/s 276 ; words/s 16347 ; accuracy train : 55.02320861816406\n",
      "441536 ; loss 0.99 ; sentence/s 274 ; words/s 16067 ; accuracy train : 55.20267105102539\n",
      "447936 ; loss 0.99 ; sentence/s 275 ; words/s 16142 ; accuracy train : 55.37834930419922\n",
      "454336 ; loss 0.99 ; sentence/s 278 ; words/s 15955 ; accuracy train : 55.543575286865234\n",
      "460736 ; loss 0.98 ; sentence/s 276 ; words/s 16345 ; accuracy train : 55.70160675048828\n",
      "467136 ; loss 0.97 ; sentence/s 277 ; words/s 15951 ; accuracy train : 55.86365509033203\n",
      "473536 ; loss 0.97 ; sentence/s 279 ; words/s 15906 ; accuracy train : 56.0069694519043\n",
      "479936 ; loss 0.95 ; sentence/s 280 ; words/s 16024 ; accuracy train : 56.180625915527344\n",
      "486336 ; loss 0.96 ; sentence/s 281 ; words/s 16031 ; accuracy train : 56.323394775390625\n",
      "492736 ; loss 0.95 ; sentence/s 279 ; words/s 16183 ; accuracy train : 56.46733093261719\n",
      "499136 ; loss 0.94 ; sentence/s 275 ; words/s 15646 ; accuracy train : 56.6171875\n",
      "505536 ; loss 0.94 ; sentence/s 273 ; words/s 16155 ; accuracy train : 56.75395584106445\n",
      "511936 ; loss 0.94 ; sentence/s 275 ; words/s 15912 ; accuracy train : 56.883399963378906\n",
      "518336 ; loss 0.93 ; sentence/s 274 ; words/s 15937 ; accuracy train : 57.02256774902344\n",
      "524736 ; loss 0.93 ; sentence/s 278 ; words/s 15986 ; accuracy train : 57.1596794128418\n",
      "531136 ; loss 0.92 ; sentence/s 277 ; words/s 16068 ; accuracy train : 57.288028717041016\n",
      "537536 ; loss 0.91 ; sentence/s 285 ; words/s 16584 ; accuracy train : 57.41294479370117\n",
      "543936 ; loss 0.9 ; sentence/s 289 ; words/s 16883 ; accuracy train : 57.536949157714844\n",
      "results : epoch 1 ; mean accuracy train : 57.639976501464844\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              68.96971893310547\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.099\n",
      "6336 ; loss 0.89 ; sentence/s 256 ; words/s 15013 ; accuracy train : 68.53125\n",
      "12736 ; loss 0.88 ; sentence/s 269 ; words/s 15747 ; accuracy train : 68.7734375\n",
      "19136 ; loss 0.88 ; sentence/s 260 ; words/s 14947 ; accuracy train : 68.421875\n",
      "25536 ; loss 0.87 ; sentence/s 262 ; words/s 15119 ; accuracy train : 68.4296875\n",
      "31936 ; loss 0.87 ; sentence/s 261 ; words/s 15177 ; accuracy train : 68.44062805175781\n",
      "38336 ; loss 0.86 ; sentence/s 263 ; words/s 15388 ; accuracy train : 68.55989837646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44736 ; loss 0.85 ; sentence/s 255 ; words/s 14700 ; accuracy train : 68.72544860839844\n",
      "51136 ; loss 0.85 ; sentence/s 249 ; words/s 14755 ; accuracy train : 68.669921875\n",
      "57536 ; loss 0.84 ; sentence/s 252 ; words/s 14916 ; accuracy train : 68.70138549804688\n",
      "63936 ; loss 0.83 ; sentence/s 254 ; words/s 14633 ; accuracy train : 68.84375\n",
      "70336 ; loss 0.83 ; sentence/s 239 ; words/s 13997 ; accuracy train : 68.92613983154297\n",
      "76736 ; loss 0.82 ; sentence/s 228 ; words/s 13310 ; accuracy train : 68.91666412353516\n",
      "83136 ; loss 0.81 ; sentence/s 249 ; words/s 14241 ; accuracy train : 68.96394348144531\n",
      "89536 ; loss 0.81 ; sentence/s 252 ; words/s 14693 ; accuracy train : 68.99665069580078\n",
      "95936 ; loss 0.8 ; sentence/s 254 ; words/s 14944 ; accuracy train : 69.08125305175781\n",
      "102336 ; loss 0.8 ; sentence/s 253 ; words/s 14756 ; accuracy train : 69.0380859375\n",
      "108736 ; loss 0.8 ; sentence/s 251 ; words/s 14772 ; accuracy train : 69.03584289550781\n",
      "115136 ; loss 0.79 ; sentence/s 244 ; words/s 14148 ; accuracy train : 69.09982299804688\n",
      "121536 ; loss 0.78 ; sentence/s 250 ; words/s 14244 ; accuracy train : 69.11431121826172\n",
      "127936 ; loss 0.78 ; sentence/s 251 ; words/s 14316 ; accuracy train : 69.1546859741211\n",
      "134336 ; loss 0.77 ; sentence/s 244 ; words/s 14138 ; accuracy train : 69.16220092773438\n",
      "140736 ; loss 0.77 ; sentence/s 240 ; words/s 13976 ; accuracy train : 69.2024154663086\n",
      "147136 ; loss 0.76 ; sentence/s 238 ; words/s 14073 ; accuracy train : 69.2778549194336\n",
      "153536 ; loss 0.76 ; sentence/s 244 ; words/s 13782 ; accuracy train : 69.33724212646484\n",
      "159936 ; loss 0.75 ; sentence/s 240 ; words/s 14073 ; accuracy train : 69.37875366210938\n",
      "166336 ; loss 0.75 ; sentence/s 252 ; words/s 14819 ; accuracy train : 69.42488098144531\n",
      "172736 ; loss 0.74 ; sentence/s 247 ; words/s 14318 ; accuracy train : 69.45486450195312\n",
      "179136 ; loss 0.75 ; sentence/s 250 ; words/s 14616 ; accuracy train : 69.47711944580078\n",
      "185536 ; loss 0.73 ; sentence/s 249 ; words/s 14628 ; accuracy train : 69.56034851074219\n",
      "191936 ; loss 0.73 ; sentence/s 248 ; words/s 14652 ; accuracy train : 69.59583282470703\n",
      "198336 ; loss 0.72 ; sentence/s 253 ; words/s 14830 ; accuracy train : 69.6491928100586\n",
      "204736 ; loss 0.72 ; sentence/s 240 ; words/s 14370 ; accuracy train : 69.7197265625\n",
      "211136 ; loss 0.72 ; sentence/s 245 ; words/s 14394 ; accuracy train : 69.7774658203125\n",
      "217536 ; loss 0.72 ; sentence/s 243 ; words/s 14081 ; accuracy train : 69.81571960449219\n",
      "223936 ; loss 0.72 ; sentence/s 252 ; words/s 14209 ; accuracy train : 69.86830139160156\n",
      "230336 ; loss 0.71 ; sentence/s 250 ; words/s 13978 ; accuracy train : 69.91319274902344\n",
      "236736 ; loss 0.71 ; sentence/s 255 ; words/s 14275 ; accuracy train : 69.97634887695312\n",
      "243136 ; loss 0.71 ; sentence/s 256 ; words/s 14646 ; accuracy train : 70.03865051269531\n",
      "249536 ; loss 0.69 ; sentence/s 253 ; words/s 15064 ; accuracy train : 70.11097717285156\n",
      "255936 ; loss 0.7 ; sentence/s 250 ; words/s 14853 ; accuracy train : 70.15898132324219\n",
      "262336 ; loss 0.7 ; sentence/s 251 ; words/s 14495 ; accuracy train : 70.20121765136719\n",
      "268736 ; loss 0.7 ; sentence/s 252 ; words/s 14618 ; accuracy train : 70.23251342773438\n",
      "275136 ; loss 0.69 ; sentence/s 250 ; words/s 14851 ; accuracy train : 70.27325439453125\n",
      "281536 ; loss 0.68 ; sentence/s 261 ; words/s 14932 ; accuracy train : 70.3171157836914\n",
      "287936 ; loss 0.68 ; sentence/s 247 ; words/s 14383 ; accuracy train : 70.3857650756836\n",
      "294336 ; loss 0.68 ; sentence/s 245 ; words/s 14452 ; accuracy train : 70.44293212890625\n",
      "300736 ; loss 0.67 ; sentence/s 246 ; words/s 14533 ; accuracy train : 70.5069808959961\n",
      "307136 ; loss 0.67 ; sentence/s 251 ; words/s 14539 ; accuracy train : 70.55892181396484\n",
      "313536 ; loss 0.67 ; sentence/s 282 ; words/s 16095 ; accuracy train : 70.61096954345703\n",
      "319936 ; loss 0.67 ; sentence/s 255 ; words/s 14974 ; accuracy train : 70.65843963623047\n",
      "326336 ; loss 0.66 ; sentence/s 286 ; words/s 16253 ; accuracy train : 70.71139526367188\n",
      "332736 ; loss 0.66 ; sentence/s 285 ; words/s 16815 ; accuracy train : 70.75841522216797\n",
      "339136 ; loss 0.64 ; sentence/s 268 ; words/s 15747 ; accuracy train : 70.83255004882812\n",
      "345536 ; loss 0.65 ; sentence/s 285 ; words/s 16179 ; accuracy train : 70.88079071044922\n",
      "351936 ; loss 0.65 ; sentence/s 279 ; words/s 16383 ; accuracy train : 70.94943237304688\n",
      "358336 ; loss 0.65 ; sentence/s 276 ; words/s 15642 ; accuracy train : 71.00251007080078\n",
      "364736 ; loss 0.65 ; sentence/s 288 ; words/s 16609 ; accuracy train : 71.05400085449219\n",
      "371136 ; loss 0.66 ; sentence/s 274 ; words/s 15381 ; accuracy train : 71.08405303955078\n",
      "377536 ; loss 0.64 ; sentence/s 295 ; words/s 16772 ; accuracy train : 71.1422119140625\n",
      "383936 ; loss 0.65 ; sentence/s 288 ; words/s 16744 ; accuracy train : 71.17838287353516\n",
      "390336 ; loss 0.64 ; sentence/s 288 ; words/s 17202 ; accuracy train : 71.2338638305664\n",
      "396736 ; loss 0.63 ; sentence/s 281 ; words/s 16445 ; accuracy train : 71.2890625\n",
      "403136 ; loss 0.63 ; sentence/s 280 ; words/s 16167 ; accuracy train : 71.34672546386719\n",
      "409536 ; loss 0.64 ; sentence/s 288 ; words/s 16814 ; accuracy train : 71.38916015625\n",
      "415936 ; loss 0.62 ; sentence/s 288 ; words/s 16661 ; accuracy train : 71.44495391845703\n",
      "422336 ; loss 0.62 ; sentence/s 286 ; words/s 17232 ; accuracy train : 71.50592041015625\n",
      "428736 ; loss 0.63 ; sentence/s 290 ; words/s 16683 ; accuracy train : 71.55270385742188\n",
      "435136 ; loss 0.62 ; sentence/s 294 ; words/s 16840 ; accuracy train : 71.60730743408203\n",
      "441536 ; loss 0.62 ; sentence/s 283 ; words/s 16196 ; accuracy train : 71.64923095703125\n",
      "447936 ; loss 0.6 ; sentence/s 284 ; words/s 16516 ; accuracy train : 71.7095947265625\n",
      "454336 ; loss 0.61 ; sentence/s 287 ; words/s 16672 ; accuracy train : 71.75791931152344\n",
      "460736 ; loss 0.6 ; sentence/s 280 ; words/s 16555 ; accuracy train : 71.81033325195312\n",
      "467136 ; loss 0.62 ; sentence/s 274 ; words/s 16118 ; accuracy train : 71.85530853271484\n",
      "473536 ; loss 0.62 ; sentence/s 294 ; words/s 16875 ; accuracy train : 71.90413665771484\n",
      "479936 ; loss 0.61 ; sentence/s 294 ; words/s 16965 ; accuracy train : 71.95833587646484\n",
      "486336 ; loss 0.6 ; sentence/s 261 ; words/s 15375 ; accuracy train : 72.01007080078125\n",
      "492736 ; loss 0.6 ; sentence/s 286 ; words/s 16264 ; accuracy train : 72.06473541259766\n",
      "499136 ; loss 0.6 ; sentence/s 277 ; words/s 16132 ; accuracy train : 72.11197662353516\n",
      "505536 ; loss 0.61 ; sentence/s 284 ; words/s 16429 ; accuracy train : 72.1542739868164\n",
      "511936 ; loss 0.59 ; sentence/s 276 ; words/s 16117 ; accuracy train : 72.21015930175781\n",
      "518336 ; loss 0.6 ; sentence/s 277 ; words/s 16246 ; accuracy train : 72.25810241699219\n",
      "524736 ; loss 0.6 ; sentence/s 276 ; words/s 15532 ; accuracy train : 72.30278015136719\n",
      "531136 ; loss 0.59 ; sentence/s 284 ; words/s 16805 ; accuracy train : 72.35900115966797\n",
      "537536 ; loss 0.58 ; sentence/s 282 ; words/s 16112 ; accuracy train : 72.4116439819336\n",
      "543936 ; loss 0.59 ; sentence/s 279 ; words/s 16230 ; accuracy train : 72.46434020996094\n",
      "results : epoch 2 ; mean accuracy train : 72.50563049316406\n",
      "\n",
      "VALIDATION : Epoch 2\n",
      "togrep : results : epoch 2 ; mean accuracy valid :              76.66124725341797\n",
      "saving model at epoch 2\n",
      "\n",
      "TRAINING : Epoch 3\n",
      "Learning rate : 0.09801\n",
      "6336 ; loss 0.59 ; sentence/s 251 ; words/s 14883 ; accuracy train : 76.765625\n",
      "12736 ; loss 0.57 ; sentence/s 257 ; words/s 14655 ; accuracy train : 77.1796875\n",
      "19136 ; loss 0.58 ; sentence/s 256 ; words/s 14850 ; accuracy train : 77.109375\n",
      "25536 ; loss 0.58 ; sentence/s 261 ; words/s 15156 ; accuracy train : 77.1484375\n",
      "31936 ; loss 0.57 ; sentence/s 263 ; words/s 15071 ; accuracy train : 77.10624694824219\n",
      "38336 ; loss 0.58 ; sentence/s 260 ; words/s 14810 ; accuracy train : 77.1640625\n",
      "44736 ; loss 0.57 ; sentence/s 253 ; words/s 14382 ; accuracy train : 77.06696319580078\n",
      "51136 ; loss 0.59 ; sentence/s 252 ; words/s 14755 ; accuracy train : 77.005859375\n",
      "57536 ; loss 0.58 ; sentence/s 280 ; words/s 16701 ; accuracy train : 76.93228912353516\n",
      "63936 ; loss 0.56 ; sentence/s 291 ; words/s 16905 ; accuracy train : 77.0218734741211\n",
      "70336 ; loss 0.58 ; sentence/s 291 ; words/s 17012 ; accuracy train : 77.04403686523438\n",
      "76736 ; loss 0.57 ; sentence/s 291 ; words/s 16596 ; accuracy train : 77.08072662353516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83136 ; loss 0.56 ; sentence/s 288 ; words/s 16281 ; accuracy train : 77.13461303710938\n",
      "89536 ; loss 0.56 ; sentence/s 282 ; words/s 16560 ; accuracy train : 77.21540069580078\n",
      "95936 ; loss 0.56 ; sentence/s 287 ; words/s 16475 ; accuracy train : 77.27916717529297\n",
      "102336 ; loss 0.57 ; sentence/s 287 ; words/s 16801 ; accuracy train : 77.279296875\n",
      "108736 ; loss 0.57 ; sentence/s 288 ; words/s 16908 ; accuracy train : 77.30422973632812\n",
      "115136 ; loss 0.56 ; sentence/s 286 ; words/s 16617 ; accuracy train : 77.33246612548828\n",
      "121536 ; loss 0.55 ; sentence/s 291 ; words/s 17219 ; accuracy train : 77.36759948730469\n",
      "127936 ; loss 0.55 ; sentence/s 294 ; words/s 16938 ; accuracy train : 77.3984375\n",
      "134336 ; loss 0.56 ; sentence/s 282 ; words/s 16246 ; accuracy train : 77.42112731933594\n",
      "140736 ; loss 0.56 ; sentence/s 284 ; words/s 16496 ; accuracy train : 77.46235656738281\n",
      "147136 ; loss 0.56 ; sentence/s 245 ; words/s 14092 ; accuracy train : 77.48301696777344\n",
      "153536 ; loss 0.56 ; sentence/s 225 ; words/s 13068 ; accuracy train : 77.50260162353516\n",
      "159936 ; loss 0.54 ; sentence/s 221 ; words/s 12985 ; accuracy train : 77.54000091552734\n",
      "166336 ; loss 0.55 ; sentence/s 231 ; words/s 13551 ; accuracy train : 77.578125\n",
      "172736 ; loss 0.55 ; sentence/s 270 ; words/s 15486 ; accuracy train : 77.59143829345703\n",
      "179136 ; loss 0.55 ; sentence/s 290 ; words/s 17077 ; accuracy train : 77.61495208740234\n",
      "185536 ; loss 0.55 ; sentence/s 283 ; words/s 16594 ; accuracy train : 77.63793182373047\n",
      "191936 ; loss 0.54 ; sentence/s 293 ; words/s 16738 ; accuracy train : 77.6578140258789\n",
      "198336 ; loss 0.53 ; sentence/s 275 ; words/s 16402 ; accuracy train : 77.70967864990234\n",
      "204736 ; loss 0.53 ; sentence/s 293 ; words/s 16697 ; accuracy train : 77.767578125\n",
      "211136 ; loss 0.55 ; sentence/s 288 ; words/s 16991 ; accuracy train : 77.76751708984375\n",
      "217536 ; loss 0.53 ; sentence/s 289 ; words/s 16516 ; accuracy train : 77.80744171142578\n",
      "223936 ; loss 0.52 ; sentence/s 287 ; words/s 16869 ; accuracy train : 77.8687515258789\n",
      "230336 ; loss 0.54 ; sentence/s 291 ; words/s 16597 ; accuracy train : 77.90103912353516\n",
      "236736 ; loss 0.53 ; sentence/s 293 ; words/s 16654 ; accuracy train : 77.94087982177734\n",
      "243136 ; loss 0.53 ; sentence/s 282 ; words/s 16237 ; accuracy train : 77.98848724365234\n",
      "249536 ; loss 0.54 ; sentence/s 289 ; words/s 16616 ; accuracy train : 77.99839782714844\n",
      "255936 ; loss 0.53 ; sentence/s 282 ; words/s 16486 ; accuracy train : 78.02616882324219\n",
      "262336 ; loss 0.52 ; sentence/s 290 ; words/s 16880 ; accuracy train : 78.07279205322266\n",
      "268736 ; loss 0.51 ; sentence/s 285 ; words/s 17023 ; accuracy train : 78.12686157226562\n",
      "275136 ; loss 0.52 ; sentence/s 289 ; words/s 16934 ; accuracy train : 78.16715240478516\n",
      "281536 ; loss 0.52 ; sentence/s 294 ; words/s 16858 ; accuracy train : 78.20170593261719\n",
      "287936 ; loss 0.52 ; sentence/s 284 ; words/s 16816 ; accuracy train : 78.22396087646484\n",
      "294336 ; loss 0.52 ; sentence/s 285 ; words/s 16909 ; accuracy train : 78.24864196777344\n",
      "300736 ; loss 0.52 ; sentence/s 288 ; words/s 17030 ; accuracy train : 78.28158569335938\n",
      "307136 ; loss 0.52 ; sentence/s 282 ; words/s 16399 ; accuracy train : 78.30728912353516\n",
      "313536 ; loss 0.51 ; sentence/s 275 ; words/s 16052 ; accuracy train : 78.3488540649414\n",
      "319936 ; loss 0.51 ; sentence/s 287 ; words/s 16574 ; accuracy train : 78.3862533569336\n",
      "326336 ; loss 0.52 ; sentence/s 287 ; words/s 16582 ; accuracy train : 78.41421508789062\n",
      "332736 ; loss 0.51 ; sentence/s 269 ; words/s 15598 ; accuracy train : 78.45222473144531\n",
      "339136 ; loss 0.51 ; sentence/s 283 ; words/s 16544 ; accuracy train : 78.48673248291016\n",
      "345536 ; loss 0.5 ; sentence/s 283 ; words/s 16360 ; accuracy train : 78.53385162353516\n",
      "351936 ; loss 0.51 ; sentence/s 289 ; words/s 16568 ; accuracy train : 78.57499694824219\n",
      "358336 ; loss 0.51 ; sentence/s 288 ; words/s 17277 ; accuracy train : 78.60462951660156\n",
      "364736 ; loss 0.5 ; sentence/s 294 ; words/s 16769 ; accuracy train : 78.64089965820312\n",
      "371136 ; loss 0.51 ; sentence/s 294 ; words/s 16825 ; accuracy train : 78.6629867553711\n",
      "377536 ; loss 0.51 ; sentence/s 292 ; words/s 17084 ; accuracy train : 78.68643951416016\n",
      "383936 ; loss 0.5 ; sentence/s 290 ; words/s 17132 ; accuracy train : 78.71823120117188\n",
      "390336 ; loss 0.5 ; sentence/s 292 ; words/s 17076 ; accuracy train : 78.75\n",
      "396736 ; loss 0.5 ; sentence/s 290 ; words/s 17211 ; accuracy train : 78.78099822998047\n",
      "403136 ; loss 0.49 ; sentence/s 287 ; words/s 17070 ; accuracy train : 78.8048095703125\n",
      "409536 ; loss 0.49 ; sentence/s 290 ; words/s 17097 ; accuracy train : 78.841796875\n",
      "415936 ; loss 0.49 ; sentence/s 292 ; words/s 16915 ; accuracy train : 78.8733139038086\n",
      "422336 ; loss 0.5 ; sentence/s 292 ; words/s 16923 ; accuracy train : 78.89417266845703\n",
      "428736 ; loss 0.5 ; sentence/s 296 ; words/s 17012 ; accuracy train : 78.91627502441406\n",
      "435136 ; loss 0.49 ; sentence/s 291 ; words/s 16822 ; accuracy train : 78.94715118408203\n",
      "441536 ; loss 0.5 ; sentence/s 292 ; words/s 17135 ; accuracy train : 78.97441101074219\n",
      "447936 ; loss 0.49 ; sentence/s 287 ; words/s 16521 ; accuracy train : 79.01205444335938\n",
      "454336 ; loss 0.49 ; sentence/s 291 ; words/s 16914 ; accuracy train : 79.04313659667969\n",
      "460736 ; loss 0.48 ; sentence/s 294 ; words/s 17193 ; accuracy train : 79.08550262451172\n",
      "467136 ; loss 0.49 ; sentence/s 287 ; words/s 16678 ; accuracy train : 79.10337829589844\n",
      "473536 ; loss 0.47 ; sentence/s 289 ; words/s 17107 ; accuracy train : 79.14189147949219\n",
      "479936 ; loss 0.48 ; sentence/s 291 ; words/s 16906 ; accuracy train : 79.1727066040039\n",
      "486336 ; loss 0.49 ; sentence/s 295 ; words/s 16824 ; accuracy train : 79.1977767944336\n",
      "492736 ; loss 0.47 ; sentence/s 292 ; words/s 16854 ; accuracy train : 79.23052215576172\n",
      "499136 ; loss 0.48 ; sentence/s 293 ; words/s 17138 ; accuracy train : 79.25841522216797\n",
      "505536 ; loss 0.48 ; sentence/s 291 ; words/s 16844 ; accuracy train : 79.28797149658203\n",
      "511936 ; loss 0.47 ; sentence/s 292 ; words/s 17081 ; accuracy train : 79.318359375\n",
      "518336 ; loss 0.46 ; sentence/s 294 ; words/s 17040 ; accuracy train : 79.35648345947266\n",
      "524736 ; loss 0.48 ; sentence/s 291 ; words/s 17094 ; accuracy train : 79.38014221191406\n",
      "531136 ; loss 0.48 ; sentence/s 291 ; words/s 16796 ; accuracy train : 79.40605926513672\n",
      "537536 ; loss 0.48 ; sentence/s 290 ; words/s 16414 ; accuracy train : 79.43099212646484\n",
      "543936 ; loss 0.48 ; sentence/s 278 ; words/s 16371 ; accuracy train : 79.45294189453125\n",
      "results : epoch 3 ; mean accuracy train : 79.47874450683594\n",
      "\n",
      "VALIDATION : Epoch 3\n",
      "togrep : results : epoch 3 ; mean accuracy valid :              80.86770629882812\n",
      "saving model at epoch 3\n",
      "\n",
      "TRAINING : Epoch 4\n",
      "Learning rate : 0.0970299\n",
      "6336 ; loss 0.46 ; sentence/s 295 ; words/s 16830 ; accuracy train : 82.078125\n",
      "12736 ; loss 0.49 ; sentence/s 294 ; words/s 17355 ; accuracy train : 81.6015625\n",
      "19136 ; loss 0.47 ; sentence/s 293 ; words/s 17148 ; accuracy train : 81.640625\n",
      "25536 ; loss 0.48 ; sentence/s 295 ; words/s 16903 ; accuracy train : 81.7109375\n",
      "31936 ; loss 0.47 ; sentence/s 294 ; words/s 17231 ; accuracy train : 81.6937484741211\n",
      "38336 ; loss 0.47 ; sentence/s 298 ; words/s 17124 ; accuracy train : 81.78125\n",
      "44736 ; loss 0.46 ; sentence/s 286 ; words/s 17251 ; accuracy train : 81.87946319580078\n",
      "51136 ; loss 0.47 ; sentence/s 294 ; words/s 16967 ; accuracy train : 81.826171875\n",
      "57536 ; loss 0.46 ; sentence/s 294 ; words/s 16923 ; accuracy train : 81.87847137451172\n",
      "63936 ; loss 0.45 ; sentence/s 292 ; words/s 16899 ; accuracy train : 81.9390640258789\n",
      "70336 ; loss 0.46 ; sentence/s 294 ; words/s 16985 ; accuracy train : 81.953125\n",
      "76736 ; loss 0.47 ; sentence/s 292 ; words/s 16966 ; accuracy train : 81.9375\n",
      "83136 ; loss 0.47 ; sentence/s 291 ; words/s 17124 ; accuracy train : 81.95913696289062\n",
      "89536 ; loss 0.46 ; sentence/s 293 ; words/s 17147 ; accuracy train : 81.95982360839844\n",
      "95936 ; loss 0.46 ; sentence/s 290 ; words/s 16924 ; accuracy train : 81.99166870117188\n",
      "102336 ; loss 0.45 ; sentence/s 285 ; words/s 16333 ; accuracy train : 82.0185546875\n",
      "108736 ; loss 0.45 ; sentence/s 292 ; words/s 16923 ; accuracy train : 82.078125\n",
      "115136 ; loss 0.46 ; sentence/s 291 ; words/s 16981 ; accuracy train : 82.1171875\n",
      "121536 ; loss 0.47 ; sentence/s 290 ; words/s 17323 ; accuracy train : 82.1307601928711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127936 ; loss 0.45 ; sentence/s 283 ; words/s 16357 ; accuracy train : 82.1343765258789\n",
      "134336 ; loss 0.46 ; sentence/s 284 ; words/s 16864 ; accuracy train : 82.1547622680664\n",
      "140736 ; loss 0.46 ; sentence/s 292 ; words/s 16871 ; accuracy train : 82.15979766845703\n",
      "147136 ; loss 0.45 ; sentence/s 295 ; words/s 16872 ; accuracy train : 82.1793441772461\n",
      "153536 ; loss 0.44 ; sentence/s 296 ; words/s 16923 ; accuracy train : 82.22721099853516\n",
      "159936 ; loss 0.45 ; sentence/s 282 ; words/s 16445 ; accuracy train : 82.26249694824219\n",
      "166336 ; loss 0.46 ; sentence/s 294 ; words/s 16811 ; accuracy train : 82.26382446289062\n",
      "172736 ; loss 0.44 ; sentence/s 299 ; words/s 16711 ; accuracy train : 82.30497741699219\n",
      "179136 ; loss 0.43 ; sentence/s 288 ; words/s 17014 ; accuracy train : 82.35491180419922\n",
      "185536 ; loss 0.45 ; sentence/s 286 ; words/s 16425 ; accuracy train : 82.36476135253906\n",
      "191936 ; loss 0.46 ; sentence/s 290 ; words/s 16826 ; accuracy train : 82.37239837646484\n",
      "198336 ; loss 0.43 ; sentence/s 292 ; words/s 17113 ; accuracy train : 82.41835021972656\n",
      "204736 ; loss 0.45 ; sentence/s 291 ; words/s 17285 ; accuracy train : 82.4462890625\n",
      "211136 ; loss 0.44 ; sentence/s 292 ; words/s 17346 ; accuracy train : 82.47679901123047\n",
      "217536 ; loss 0.45 ; sentence/s 271 ; words/s 16098 ; accuracy train : 82.48253631591797\n",
      "223936 ; loss 0.44 ; sentence/s 296 ; words/s 17464 ; accuracy train : 82.50402069091797\n",
      "230336 ; loss 0.44 ; sentence/s 296 ; words/s 16868 ; accuracy train : 82.52213287353516\n",
      "236736 ; loss 0.43 ; sentence/s 299 ; words/s 17487 ; accuracy train : 82.54560852050781\n",
      "243136 ; loss 0.45 ; sentence/s 282 ; words/s 16203 ; accuracy train : 82.56620025634766\n",
      "249536 ; loss 0.44 ; sentence/s 272 ; words/s 16370 ; accuracy train : 82.5977554321289\n",
      "255936 ; loss 0.45 ; sentence/s 296 ; words/s 17211 ; accuracy train : 82.6097640991211\n",
      "262336 ; loss 0.42 ; sentence/s 299 ; words/s 17397 ; accuracy train : 82.65129852294922\n",
      "268736 ; loss 0.42 ; sentence/s 297 ; words/s 17515 ; accuracy train : 82.68415069580078\n",
      "275136 ; loss 0.43 ; sentence/s 303 ; words/s 17799 ; accuracy train : 82.70421600341797\n",
      "281536 ; loss 0.43 ; sentence/s 294 ; words/s 17024 ; accuracy train : 82.71235656738281\n",
      "287936 ; loss 0.44 ; sentence/s 302 ; words/s 16850 ; accuracy train : 82.73542022705078\n",
      "294336 ; loss 0.43 ; sentence/s 291 ; words/s 17440 ; accuracy train : 82.75713348388672\n",
      "300736 ; loss 0.43 ; sentence/s 279 ; words/s 15993 ; accuracy train : 82.77493286132812\n",
      "307136 ; loss 0.43 ; sentence/s 293 ; words/s 16926 ; accuracy train : 82.787109375\n",
      "313536 ; loss 0.44 ; sentence/s 291 ; words/s 17025 ; accuracy train : 82.80133819580078\n",
      "319936 ; loss 0.44 ; sentence/s 293 ; words/s 17009 ; accuracy train : 82.82437133789062\n",
      "326336 ; loss 0.45 ; sentence/s 141 ; words/s 8115 ; accuracy train : 82.81954956054688\n",
      "332736 ; loss 0.43 ; sentence/s 37 ; words/s 2155 ; accuracy train : 82.84044647216797\n",
      "339136 ; loss 0.42 ; sentence/s 38 ; words/s 2173 ; accuracy train : 82.86940002441406\n",
      "345536 ; loss 0.43 ; sentence/s 38 ; words/s 2229 ; accuracy train : 82.88136291503906\n",
      "351936 ; loss 0.43 ; sentence/s 36 ; words/s 2167 ; accuracy train : 82.89573669433594\n",
      "358336 ; loss 0.42 ; sentence/s 47 ; words/s 2815 ; accuracy train : 82.91461944580078\n",
      "364736 ; loss 0.43 ; sentence/s 298 ; words/s 17523 ; accuracy train : 82.92488861083984\n",
      "371136 ; loss 0.42 ; sentence/s 303 ; words/s 17363 ; accuracy train : 82.9377670288086\n",
      "377536 ; loss 0.42 ; sentence/s 298 ; words/s 17453 ; accuracy train : 82.95683288574219\n",
      "383936 ; loss 0.42 ; sentence/s 304 ; words/s 17496 ; accuracy train : 82.97734069824219\n",
      "390336 ; loss 0.42 ; sentence/s 304 ; words/s 17281 ; accuracy train : 82.99871826171875\n",
      "396736 ; loss 0.41 ; sentence/s 307 ; words/s 17490 ; accuracy train : 83.02570343017578\n",
      "403136 ; loss 0.42 ; sentence/s 300 ; words/s 17433 ; accuracy train : 83.04290771484375\n",
      "409536 ; loss 0.42 ; sentence/s 305 ; words/s 17339 ; accuracy train : 83.060546875\n",
      "415936 ; loss 0.42 ; sentence/s 303 ; words/s 17363 ; accuracy train : 83.07524108886719\n",
      "422336 ; loss 0.42 ; sentence/s 300 ; words/s 17663 ; accuracy train : 83.08451843261719\n",
      "428736 ; loss 0.41 ; sentence/s 299 ; words/s 17651 ; accuracy train : 83.10540771484375\n",
      "435136 ; loss 0.42 ; sentence/s 293 ; words/s 17085 ; accuracy train : 83.12408447265625\n",
      "441536 ; loss 0.43 ; sentence/s 274 ; words/s 15927 ; accuracy train : 83.12703704833984\n",
      "447936 ; loss 0.42 ; sentence/s 291 ; words/s 17045 ; accuracy train : 83.140625\n",
      "454336 ; loss 0.41 ; sentence/s 297 ; words/s 17189 ; accuracy train : 83.158447265625\n",
      "460736 ; loss 0.41 ; sentence/s 293 ; words/s 17038 ; accuracy train : 83.17729949951172\n",
      "467136 ; loss 0.43 ; sentence/s 290 ; words/s 17134 ; accuracy train : 83.18279266357422\n",
      "473536 ; loss 0.41 ; sentence/s 300 ; words/s 16880 ; accuracy train : 83.19889831542969\n",
      "479936 ; loss 0.42 ; sentence/s 295 ; words/s 17109 ; accuracy train : 83.20958709716797\n",
      "486336 ; loss 0.41 ; sentence/s 290 ; words/s 16826 ; accuracy train : 83.23005676269531\n",
      "492736 ; loss 0.42 ; sentence/s 294 ; words/s 17035 ; accuracy train : 83.24005889892578\n",
      "499136 ; loss 0.4 ; sentence/s 290 ; words/s 16748 ; accuracy train : 83.26441955566406\n",
      "505536 ; loss 0.42 ; sentence/s 289 ; words/s 17714 ; accuracy train : 83.27294158935547\n",
      "511936 ; loss 0.41 ; sentence/s 293 ; words/s 16764 ; accuracy train : 83.2945327758789\n",
      "518336 ; loss 0.43 ; sentence/s 296 ; words/s 17865 ; accuracy train : 83.2988052368164\n",
      "524736 ; loss 0.41 ; sentence/s 301 ; words/s 17696 ; accuracy train : 83.31116485595703\n",
      "531136 ; loss 0.41 ; sentence/s 285 ; words/s 16830 ; accuracy train : 83.32379150390625\n",
      "537536 ; loss 0.41 ; sentence/s 282 ; words/s 16347 ; accuracy train : 83.33147430419922\n",
      "543936 ; loss 0.4 ; sentence/s 281 ; words/s 16542 ; accuracy train : 83.34706115722656\n",
      "results : epoch 4 ; mean accuracy train : 83.35247802734375\n",
      "\n",
      "VALIDATION : Epoch 4\n",
      "togrep : results : epoch 4 ; mean accuracy valid :              82.74740600585938\n",
      "saving model at epoch 4\n",
      "\n",
      "TRAINING : Epoch 5\n",
      "Learning rate : 0.096059601\n",
      "6336 ; loss 0.41 ; sentence/s 283 ; words/s 16345 ; accuracy train : 84.328125\n",
      "12736 ; loss 0.4 ; sentence/s 287 ; words/s 16702 ; accuracy train : 84.578125\n",
      "19136 ; loss 0.38 ; sentence/s 291 ; words/s 16815 ; accuracy train : 85.05208587646484\n",
      "25536 ; loss 0.4 ; sentence/s 289 ; words/s 16859 ; accuracy train : 84.98828125\n",
      "31936 ; loss 0.39 ; sentence/s 287 ; words/s 16662 ; accuracy train : 85.0062484741211\n",
      "38336 ; loss 0.41 ; sentence/s 288 ; words/s 16778 ; accuracy train : 85.015625\n",
      "44736 ; loss 0.4 ; sentence/s 295 ; words/s 16534 ; accuracy train : 85.01116180419922\n",
      "51136 ; loss 0.4 ; sentence/s 283 ; words/s 16581 ; accuracy train : 85.033203125\n",
      "57536 ; loss 0.41 ; sentence/s 282 ; words/s 16565 ; accuracy train : 84.94271087646484\n",
      "63936 ; loss 0.4 ; sentence/s 286 ; words/s 16355 ; accuracy train : 84.94844055175781\n",
      "70336 ; loss 0.4 ; sentence/s 291 ; words/s 16840 ; accuracy train : 84.96591186523438\n",
      "76736 ; loss 0.4 ; sentence/s 294 ; words/s 17315 ; accuracy train : 84.94400787353516\n",
      "83136 ; loss 0.4 ; sentence/s 292 ; words/s 16897 ; accuracy train : 84.99519348144531\n",
      "89536 ; loss 0.4 ; sentence/s 293 ; words/s 17234 ; accuracy train : 85.01451110839844\n",
      "95936 ; loss 0.41 ; sentence/s 294 ; words/s 17076 ; accuracy train : 84.99687194824219\n",
      "102336 ; loss 0.4 ; sentence/s 291 ; words/s 16889 ; accuracy train : 84.98046875\n",
      "108736 ; loss 0.39 ; sentence/s 290 ; words/s 16941 ; accuracy train : 85.03860473632812\n",
      "115136 ; loss 0.39 ; sentence/s 296 ; words/s 16795 ; accuracy train : 85.02864837646484\n",
      "121536 ; loss 0.4 ; sentence/s 291 ; words/s 17077 ; accuracy train : 85.0098648071289\n",
      "127936 ; loss 0.4 ; sentence/s 297 ; words/s 17479 ; accuracy train : 85.01484680175781\n",
      "134336 ; loss 0.41 ; sentence/s 303 ; words/s 17467 ; accuracy train : 84.98958587646484\n",
      "140736 ; loss 0.39 ; sentence/s 300 ; words/s 17414 ; accuracy train : 85.0\n",
      "147136 ; loss 0.4 ; sentence/s 302 ; words/s 17521 ; accuracy train : 84.98980712890625\n",
      "153536 ; loss 0.4 ; sentence/s 298 ; words/s 17333 ; accuracy train : 84.96939849853516\n",
      "159936 ; loss 0.4 ; sentence/s 273 ; words/s 15748 ; accuracy train : 84.96624755859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166336 ; loss 0.4 ; sentence/s 285 ; words/s 16489 ; accuracy train : 84.94711303710938\n",
      "172736 ; loss 0.4 ; sentence/s 284 ; words/s 16584 ; accuracy train : 84.94213104248047\n",
      "179136 ; loss 0.4 ; sentence/s 288 ; words/s 17081 ; accuracy train : 84.95368194580078\n",
      "185536 ; loss 0.39 ; sentence/s 290 ; words/s 16983 ; accuracy train : 84.95743560791016\n",
      "191936 ; loss 0.39 ; sentence/s 284 ; words/s 16634 ; accuracy train : 84.9671859741211\n",
      "198336 ; loss 0.41 ; sentence/s 283 ; words/s 16933 ; accuracy train : 84.97681427001953\n",
      "204736 ; loss 0.39 ; sentence/s 291 ; words/s 17101 ; accuracy train : 84.9853515625\n",
      "211136 ; loss 0.4 ; sentence/s 294 ; words/s 17136 ; accuracy train : 84.97774505615234\n",
      "217536 ; loss 0.39 ; sentence/s 291 ; words/s 17135 ; accuracy train : 84.97840118408203\n",
      "223936 ; loss 0.39 ; sentence/s 292 ; words/s 16888 ; accuracy train : 85.01026916503906\n",
      "230336 ; loss 0.39 ; sentence/s 291 ; words/s 16920 ; accuracy train : 85.01519012451172\n",
      "236736 ; loss 0.37 ; sentence/s 302 ; words/s 17386 ; accuracy train : 85.03462982177734\n",
      "243136 ; loss 0.38 ; sentence/s 300 ; words/s 17172 ; accuracy train : 85.0567398071289\n",
      "249536 ; loss 0.4 ; sentence/s 302 ; words/s 17687 ; accuracy train : 85.06049346923828\n",
      "255936 ; loss 0.39 ; sentence/s 299 ; words/s 17889 ; accuracy train : 85.05859375\n",
      "262336 ; loss 0.4 ; sentence/s 303 ; words/s 17330 ; accuracy train : 85.06059265136719\n",
      "268736 ; loss 0.37 ; sentence/s 300 ; words/s 17611 ; accuracy train : 85.08184814453125\n",
      "275136 ; loss 0.38 ; sentence/s 301 ; words/s 17434 ; accuracy train : 85.10137939453125\n",
      "281536 ; loss 0.39 ; sentence/s 295 ; words/s 17463 ; accuracy train : 85.10120391845703\n",
      "287936 ; loss 0.38 ; sentence/s 303 ; words/s 17541 ; accuracy train : 85.11250305175781\n",
      "294336 ; loss 0.4 ; sentence/s 301 ; words/s 17480 ; accuracy train : 85.1019058227539\n",
      "300736 ; loss 0.39 ; sentence/s 296 ; words/s 17704 ; accuracy train : 85.11170196533203\n",
      "307136 ; loss 0.38 ; sentence/s 295 ; words/s 16886 ; accuracy train : 85.12076568603516\n",
      "313536 ; loss 0.39 ; sentence/s 282 ; words/s 16559 ; accuracy train : 85.1240463256836\n",
      "319936 ; loss 0.39 ; sentence/s 293 ; words/s 17430 ; accuracy train : 85.13093566894531\n",
      "326336 ; loss 0.39 ; sentence/s 291 ; words/s 16764 ; accuracy train : 85.13939666748047\n",
      "332736 ; loss 0.38 ; sentence/s 273 ; words/s 16107 ; accuracy train : 85.13761901855469\n",
      "339136 ; loss 0.41 ; sentence/s 292 ; words/s 16695 ; accuracy train : 85.12942504882812\n",
      "345536 ; loss 0.39 ; sentence/s 290 ; words/s 16889 ; accuracy train : 85.13628387451172\n",
      "351936 ; loss 0.39 ; sentence/s 287 ; words/s 16532 ; accuracy train : 85.12670135498047\n",
      "358336 ; loss 0.38 ; sentence/s 286 ; words/s 16957 ; accuracy train : 85.13616180419922\n",
      "364736 ; loss 0.38 ; sentence/s 296 ; words/s 16819 ; accuracy train : 85.14446258544922\n",
      "371136 ; loss 0.38 ; sentence/s 294 ; words/s 17062 ; accuracy train : 85.15463256835938\n",
      "377536 ; loss 0.38 ; sentence/s 295 ; words/s 16659 ; accuracy train : 85.16181182861328\n",
      "383936 ; loss 0.38 ; sentence/s 296 ; words/s 17115 ; accuracy train : 85.17369842529297\n",
      "390336 ; loss 0.36 ; sentence/s 299 ; words/s 17764 ; accuracy train : 85.19979858398438\n",
      "396736 ; loss 0.37 ; sentence/s 306 ; words/s 17553 ; accuracy train : 85.21370697021484\n",
      "403136 ; loss 0.38 ; sentence/s 303 ; words/s 17158 ; accuracy train : 85.22247314453125\n",
      "409536 ; loss 0.39 ; sentence/s 289 ; words/s 17139 ; accuracy train : 85.218017578125\n",
      "415936 ; loss 0.38 ; sentence/s 307 ; words/s 17700 ; accuracy train : 85.21634674072266\n",
      "422336 ; loss 0.38 ; sentence/s 297 ; words/s 17264 ; accuracy train : 85.21969604492188\n",
      "428736 ; loss 0.39 ; sentence/s 283 ; words/s 16630 ; accuracy train : 85.22248077392578\n",
      "435136 ; loss 0.38 ; sentence/s 295 ; words/s 17250 ; accuracy train : 85.23322296142578\n",
      "441536 ; loss 0.37 ; sentence/s 302 ; words/s 17710 ; accuracy train : 85.24478912353516\n",
      "447936 ; loss 0.37 ; sentence/s 292 ; words/s 16685 ; accuracy train : 85.2580337524414\n",
      "454336 ; loss 0.37 ; sentence/s 301 ; words/s 17806 ; accuracy train : 85.2665023803711\n",
      "460736 ; loss 0.39 ; sentence/s 301 ; words/s 17115 ; accuracy train : 85.26215362548828\n",
      "467136 ; loss 0.38 ; sentence/s 295 ; words/s 17289 ; accuracy train : 85.27418518066406\n",
      "473536 ; loss 0.37 ; sentence/s 300 ; words/s 17671 ; accuracy train : 85.28990936279297\n",
      "479936 ; loss 0.38 ; sentence/s 302 ; words/s 17676 ; accuracy train : 85.29895782470703\n",
      "486336 ; loss 0.37 ; sentence/s 294 ; words/s 17150 ; accuracy train : 85.30900573730469\n",
      "492736 ; loss 0.38 ; sentence/s 274 ; words/s 15899 ; accuracy train : 85.30924987792969\n",
      "499136 ; loss 0.38 ; sentence/s 279 ; words/s 16287 ; accuracy train : 85.31510162353516\n",
      "505536 ; loss 0.38 ; sentence/s 283 ; words/s 16440 ; accuracy train : 85.32258605957031\n",
      "511936 ; loss 0.37 ; sentence/s 295 ; words/s 17042 ; accuracy train : 85.32637023925781\n",
      "518336 ; loss 0.37 ; sentence/s 284 ; words/s 16936 ; accuracy train : 85.32889556884766\n",
      "524736 ; loss 0.37 ; sentence/s 287 ; words/s 17231 ; accuracy train : 85.33536529541016\n",
      "531136 ; loss 0.37 ; sentence/s 293 ; words/s 16973 ; accuracy train : 85.34600830078125\n",
      "537536 ; loss 0.37 ; sentence/s 293 ; words/s 16773 ; accuracy train : 85.35844421386719\n",
      "543936 ; loss 0.37 ; sentence/s 291 ; words/s 17011 ; accuracy train : 85.37168884277344\n",
      "results : epoch 5 ; mean accuracy train : 85.37407684326172\n",
      "\n",
      "VALIDATION : Epoch 5\n",
      "togrep : results : epoch 5 ; mean accuracy valid :              83.59073638916016\n",
      "saving model at epoch 5\n",
      "\n",
      "TRAINING : Epoch 6\n",
      "Learning rate : 0.09509900499\n",
      "6336 ; loss 0.36 ; sentence/s 290 ; words/s 16907 ; accuracy train : 86.421875\n",
      "12736 ; loss 0.38 ; sentence/s 295 ; words/s 17017 ; accuracy train : 85.921875\n",
      "19136 ; loss 0.38 ; sentence/s 294 ; words/s 16710 ; accuracy train : 85.86978912353516\n",
      "25536 ; loss 0.36 ; sentence/s 290 ; words/s 17173 ; accuracy train : 86.0390625\n",
      "31936 ; loss 0.37 ; sentence/s 292 ; words/s 17389 ; accuracy train : 86.0250015258789\n",
      "38336 ; loss 0.36 ; sentence/s 292 ; words/s 17310 ; accuracy train : 86.04166412353516\n",
      "44736 ; loss 0.37 ; sentence/s 290 ; words/s 16912 ; accuracy train : 86.01116180419922\n",
      "51136 ; loss 0.36 ; sentence/s 295 ; words/s 16834 ; accuracy train : 86.078125\n",
      "57536 ; loss 0.37 ; sentence/s 293 ; words/s 17353 ; accuracy train : 86.12673950195312\n",
      "63936 ; loss 0.37 ; sentence/s 289 ; words/s 17238 ; accuracy train : 86.1500015258789\n",
      "70336 ; loss 0.36 ; sentence/s 287 ; words/s 16206 ; accuracy train : 86.21875\n",
      "76736 ; loss 0.36 ; sentence/s 281 ; words/s 16263 ; accuracy train : 86.2421875\n",
      "83136 ; loss 0.36 ; sentence/s 295 ; words/s 16924 ; accuracy train : 86.24158477783203\n",
      "89536 ; loss 0.36 ; sentence/s 277 ; words/s 16291 ; accuracy train : 86.34151458740234\n",
      "95936 ; loss 0.37 ; sentence/s 285 ; words/s 16630 ; accuracy train : 86.32604217529297\n",
      "102336 ; loss 0.38 ; sentence/s 300 ; words/s 17091 ; accuracy train : 86.3017578125\n",
      "108736 ; loss 0.36 ; sentence/s 296 ; words/s 17197 ; accuracy train : 86.28768157958984\n",
      "115136 ; loss 0.37 ; sentence/s 307 ; words/s 17602 ; accuracy train : 86.2890625\n",
      "121536 ; loss 0.36 ; sentence/s 306 ; words/s 17766 ; accuracy train : 86.3026351928711\n",
      "127936 ; loss 0.37 ; sentence/s 304 ; words/s 17964 ; accuracy train : 86.28437805175781\n",
      "134336 ; loss 0.37 ; sentence/s 304 ; words/s 17488 ; accuracy train : 86.25595092773438\n",
      "140736 ; loss 0.36 ; sentence/s 306 ; words/s 17546 ; accuracy train : 86.29261016845703\n",
      "147136 ; loss 0.37 ; sentence/s 297 ; words/s 17888 ; accuracy train : 86.30162811279297\n",
      "153536 ; loss 0.36 ; sentence/s 303 ; words/s 17860 ; accuracy train : 86.32096099853516\n",
      "159936 ; loss 0.35 ; sentence/s 307 ; words/s 17555 ; accuracy train : 86.33999633789062\n",
      "166336 ; loss 0.36 ; sentence/s 305 ; words/s 17713 ; accuracy train : 86.35636901855469\n",
      "172736 ; loss 0.38 ; sentence/s 303 ; words/s 17714 ; accuracy train : 86.32754516601562\n",
      "179136 ; loss 0.37 ; sentence/s 303 ; words/s 17462 ; accuracy train : 86.31640625\n",
      "185536 ; loss 0.37 ; sentence/s 297 ; words/s 17495 ; accuracy train : 86.31465148925781\n",
      "191936 ; loss 0.36 ; sentence/s 305 ; words/s 17775 ; accuracy train : 86.31041717529297\n",
      "198336 ; loss 0.37 ; sentence/s 296 ; words/s 17710 ; accuracy train : 86.30897521972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204736 ; loss 0.35 ; sentence/s 285 ; words/s 16476 ; accuracy train : 86.33056640625\n",
      "211136 ; loss 0.36 ; sentence/s 282 ; words/s 16557 ; accuracy train : 86.3508529663086\n",
      "217536 ; loss 0.36 ; sentence/s 295 ; words/s 16874 ; accuracy train : 86.34099578857422\n",
      "223936 ; loss 0.37 ; sentence/s 279 ; words/s 15908 ; accuracy train : 86.328125\n",
      "230336 ; loss 0.36 ; sentence/s 296 ; words/s 16773 ; accuracy train : 86.33506774902344\n",
      "236736 ; loss 0.35 ; sentence/s 277 ; words/s 16670 ; accuracy train : 86.3492431640625\n",
      "243136 ; loss 0.37 ; sentence/s 289 ; words/s 16760 ; accuracy train : 86.33676147460938\n",
      "249536 ; loss 0.35 ; sentence/s 278 ; words/s 15949 ; accuracy train : 86.35416412353516\n",
      "255936 ; loss 0.36 ; sentence/s 285 ; words/s 16339 ; accuracy train : 86.3628921508789\n",
      "262336 ; loss 0.35 ; sentence/s 287 ; words/s 16165 ; accuracy train : 86.38300323486328\n",
      "268736 ; loss 0.36 ; sentence/s 282 ; words/s 16466 ; accuracy train : 86.38876342773438\n",
      "275136 ; loss 0.35 ; sentence/s 288 ; words/s 17322 ; accuracy train : 86.39680480957031\n",
      "281536 ; loss 0.37 ; sentence/s 294 ; words/s 17191 ; accuracy train : 86.39204406738281\n",
      "287936 ; loss 0.36 ; sentence/s 290 ; words/s 17122 ; accuracy train : 86.39027404785156\n",
      "294336 ; loss 0.37 ; sentence/s 284 ; words/s 16436 ; accuracy train : 86.38451385498047\n",
      "300736 ; loss 0.37 ; sentence/s 285 ; words/s 16718 ; accuracy train : 86.38563537597656\n",
      "307136 ; loss 0.36 ; sentence/s 283 ; words/s 16505 ; accuracy train : 86.39420318603516\n",
      "313536 ; loss 0.38 ; sentence/s 295 ; words/s 16658 ; accuracy train : 86.39126586914062\n",
      "319936 ; loss 0.37 ; sentence/s 290 ; words/s 16938 ; accuracy train : 86.38812255859375\n",
      "326336 ; loss 0.35 ; sentence/s 291 ; words/s 17168 ; accuracy train : 86.39859008789062\n",
      "332736 ; loss 0.35 ; sentence/s 294 ; words/s 17070 ; accuracy train : 86.40354919433594\n",
      "339136 ; loss 0.35 ; sentence/s 290 ; words/s 17024 ; accuracy train : 86.41597747802734\n",
      "345536 ; loss 0.37 ; sentence/s 295 ; words/s 16953 ; accuracy train : 86.4103012084961\n",
      "351936 ; loss 0.35 ; sentence/s 296 ; words/s 17049 ; accuracy train : 86.42642211914062\n",
      "358336 ; loss 0.36 ; sentence/s 296 ; words/s 16795 ; accuracy train : 86.42940521240234\n",
      "364736 ; loss 0.36 ; sentence/s 296 ; words/s 17176 ; accuracy train : 86.42681121826172\n",
      "371136 ; loss 0.35 ; sentence/s 295 ; words/s 16799 ; accuracy train : 86.4375\n",
      "377536 ; loss 0.36 ; sentence/s 292 ; words/s 16840 ; accuracy train : 86.43379211425781\n",
      "383936 ; loss 0.34 ; sentence/s 291 ; words/s 17044 ; accuracy train : 86.43958282470703\n",
      "390336 ; loss 0.35 ; sentence/s 286 ; words/s 16639 ; accuracy train : 86.44493103027344\n",
      "396736 ; loss 0.36 ; sentence/s 295 ; words/s 16979 ; accuracy train : 86.44833374023438\n",
      "403136 ; loss 0.34 ; sentence/s 295 ; words/s 16862 ; accuracy train : 86.46800231933594\n",
      "409536 ; loss 0.35 ; sentence/s 279 ; words/s 16305 ; accuracy train : 86.47412109375\n",
      "415936 ; loss 0.35 ; sentence/s 297 ; words/s 17103 ; accuracy train : 86.48750305175781\n",
      "422336 ; loss 0.34 ; sentence/s 288 ; words/s 16948 ; accuracy train : 86.49905395507812\n",
      "428736 ; loss 0.35 ; sentence/s 292 ; words/s 17149 ; accuracy train : 86.5109634399414\n",
      "435136 ; loss 0.36 ; sentence/s 293 ; words/s 17331 ; accuracy train : 86.51194763183594\n",
      "441536 ; loss 0.35 ; sentence/s 298 ; words/s 16815 ; accuracy train : 86.52264404296875\n",
      "447936 ; loss 0.36 ; sentence/s 291 ; words/s 17030 ; accuracy train : 86.52165222167969\n",
      "454336 ; loss 0.34 ; sentence/s 294 ; words/s 17058 ; accuracy train : 86.53388977050781\n",
      "460736 ; loss 0.34 ; sentence/s 295 ; words/s 17116 ; accuracy train : 86.54817962646484\n",
      "467136 ; loss 0.35 ; sentence/s 296 ; words/s 16874 ; accuracy train : 86.55243682861328\n",
      "473536 ; loss 0.35 ; sentence/s 295 ; words/s 17222 ; accuracy train : 86.55257415771484\n",
      "479936 ; loss 0.35 ; sentence/s 296 ; words/s 17063 ; accuracy train : 86.55728912353516\n",
      "486336 ; loss 0.33 ; sentence/s 296 ; words/s 17071 ; accuracy train : 86.58203125\n",
      "492736 ; loss 0.35 ; sentence/s 290 ; words/s 17216 ; accuracy train : 86.58340454101562\n",
      "499136 ; loss 0.35 ; sentence/s 289 ; words/s 17091 ; accuracy train : 86.59415435791016\n",
      "505536 ; loss 0.36 ; sentence/s 286 ; words/s 16707 ; accuracy train : 86.59078216552734\n",
      "511936 ; loss 0.35 ; sentence/s 292 ; words/s 17135 ; accuracy train : 86.5960922241211\n",
      "518336 ; loss 0.35 ; sentence/s 291 ; words/s 16904 ; accuracy train : 86.597412109375\n",
      "524736 ; loss 0.35 ; sentence/s 296 ; words/s 17017 ; accuracy train : 86.60118103027344\n",
      "531136 ; loss 0.35 ; sentence/s 295 ; words/s 17195 ; accuracy train : 86.60372924804688\n",
      "537536 ; loss 0.36 ; sentence/s 294 ; words/s 16879 ; accuracy train : 86.5991439819336\n",
      "543936 ; loss 0.35 ; sentence/s 298 ; words/s 17293 ; accuracy train : 86.60092163085938\n",
      "results : epoch 6 ; mean accuracy train : 86.609130859375\n",
      "\n",
      "VALIDATION : Epoch 6\n",
      "togrep : results : epoch 6 ; mean accuracy valid :              83.4992904663086\n",
      "Shrinking lr by : 5. New lr = 0.019019800998\n",
      "\n",
      "TRAINING : Epoch 7\n",
      "Learning rate : 0.01882960298802\n",
      "6336 ; loss 0.34 ; sentence/s 284 ; words/s 16661 ; accuracy train : 87.234375\n",
      "12736 ; loss 0.34 ; sentence/s 288 ; words/s 16913 ; accuracy train : 87.359375\n",
      "19136 ; loss 0.34 ; sentence/s 298 ; words/s 17091 ; accuracy train : 87.421875\n",
      "25536 ; loss 0.35 ; sentence/s 284 ; words/s 16652 ; accuracy train : 87.32421875\n",
      "31936 ; loss 0.33 ; sentence/s 279 ; words/s 15934 ; accuracy train : 87.42500305175781\n",
      "38336 ; loss 0.34 ; sentence/s 292 ; words/s 16645 ; accuracy train : 87.40625\n",
      "44736 ; loss 0.35 ; sentence/s 289 ; words/s 17108 ; accuracy train : 87.375\n",
      "51136 ; loss 0.34 ; sentence/s 278 ; words/s 16297 ; accuracy train : 87.404296875\n",
      "57536 ; loss 0.34 ; sentence/s 289 ; words/s 16688 ; accuracy train : 87.359375\n",
      "63936 ; loss 0.34 ; sentence/s 287 ; words/s 16437 ; accuracy train : 87.375\n",
      "70336 ; loss 0.34 ; sentence/s 288 ; words/s 17243 ; accuracy train : 87.3977279663086\n",
      "76736 ; loss 0.36 ; sentence/s 297 ; words/s 17366 ; accuracy train : 87.36328125\n",
      "83136 ; loss 0.34 ; sentence/s 286 ; words/s 16668 ; accuracy train : 87.40023803710938\n",
      "89536 ; loss 0.35 ; sentence/s 292 ; words/s 17003 ; accuracy train : 87.38616180419922\n",
      "95936 ; loss 0.33 ; sentence/s 301 ; words/s 17503 ; accuracy train : 87.45417022705078\n",
      "102336 ; loss 0.34 ; sentence/s 295 ; words/s 16749 ; accuracy train : 87.447265625\n",
      "108736 ; loss 0.33 ; sentence/s 301 ; words/s 17273 ; accuracy train : 87.46966552734375\n",
      "115136 ; loss 0.34 ; sentence/s 298 ; words/s 17158 ; accuracy train : 87.453125\n",
      "121536 ; loss 0.35 ; sentence/s 296 ; words/s 17448 ; accuracy train : 87.43009948730469\n",
      "127936 ; loss 0.35 ; sentence/s 282 ; words/s 16555 ; accuracy train : 87.43046569824219\n",
      "134336 ; loss 0.36 ; sentence/s 282 ; words/s 16869 ; accuracy train : 87.390625\n",
      "140736 ; loss 0.33 ; sentence/s 286 ; words/s 16305 ; accuracy train : 87.40483093261719\n",
      "147136 ; loss 0.34 ; sentence/s 272 ; words/s 15579 ; accuracy train : 87.40013885498047\n",
      "153536 ; loss 0.34 ; sentence/s 286 ; words/s 16131 ; accuracy train : 87.39778900146484\n",
      "159936 ; loss 0.33 ; sentence/s 267 ; words/s 16014 ; accuracy train : 87.40562438964844\n",
      "166336 ; loss 0.33 ; sentence/s 286 ; words/s 16557 ; accuracy train : 87.42548370361328\n",
      "172736 ; loss 0.35 ; sentence/s 282 ; words/s 16571 ; accuracy train : 87.40798950195312\n",
      "179136 ; loss 0.34 ; sentence/s 272 ; words/s 15999 ; accuracy train : 87.41796875\n",
      "185536 ; loss 0.33 ; sentence/s 284 ; words/s 16621 ; accuracy train : 87.421875\n",
      "191936 ; loss 0.34 ; sentence/s 273 ; words/s 16254 ; accuracy train : 87.40416717529297\n",
      "198336 ; loss 0.34 ; sentence/s 283 ; words/s 16734 ; accuracy train : 87.40120697021484\n",
      "204736 ; loss 0.34 ; sentence/s 275 ; words/s 16439 ; accuracy train : 87.39697265625\n",
      "211136 ; loss 0.34 ; sentence/s 280 ; words/s 15753 ; accuracy train : 87.40719604492188\n",
      "217536 ; loss 0.33 ; sentence/s 288 ; words/s 16606 ; accuracy train : 87.40900421142578\n",
      "223936 ; loss 0.34 ; sentence/s 279 ; words/s 16387 ; accuracy train : 87.4205322265625\n",
      "230336 ; loss 0.35 ; sentence/s 291 ; words/s 17064 ; accuracy train : 87.40408325195312\n",
      "236736 ; loss 0.33 ; sentence/s 295 ; words/s 17326 ; accuracy train : 87.4189224243164\n",
      "243136 ; loss 0.35 ; sentence/s 299 ; words/s 17142 ; accuracy train : 87.40707397460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249536 ; loss 0.35 ; sentence/s 278 ; words/s 16098 ; accuracy train : 87.390625\n",
      "255936 ; loss 0.34 ; sentence/s 298 ; words/s 17355 ; accuracy train : 87.38008117675781\n",
      "262336 ; loss 0.35 ; sentence/s 279 ; words/s 16146 ; accuracy train : 87.37461853027344\n",
      "268736 ; loss 0.35 ; sentence/s 282 ; words/s 16562 ; accuracy train : 87.36421203613281\n",
      "275136 ; loss 0.35 ; sentence/s 287 ; words/s 16700 ; accuracy train : 87.35865020751953\n",
      "281536 ; loss 0.34 ; sentence/s 290 ; words/s 16629 ; accuracy train : 87.35440063476562\n",
      "287936 ; loss 0.35 ; sentence/s 288 ; words/s 16661 ; accuracy train : 87.33715057373047\n",
      "294336 ; loss 0.34 ; sentence/s 296 ; words/s 17248 ; accuracy train : 87.33899688720703\n",
      "300736 ; loss 0.34 ; sentence/s 301 ; words/s 17664 ; accuracy train : 87.34906768798828\n",
      "307136 ; loss 0.34 ; sentence/s 289 ; words/s 16516 ; accuracy train : 87.34896087646484\n",
      "313536 ; loss 0.35 ; sentence/s 287 ; words/s 16867 ; accuracy train : 87.34629821777344\n",
      "319936 ; loss 0.35 ; sentence/s 301 ; words/s 17275 ; accuracy train : 87.34625244140625\n",
      "326336 ; loss 0.33 ; sentence/s 275 ; words/s 15813 ; accuracy train : 87.35079956054688\n",
      "332736 ; loss 0.34 ; sentence/s 290 ; words/s 16253 ; accuracy train : 87.35726928710938\n",
      "339136 ; loss 0.34 ; sentence/s 297 ; words/s 17465 ; accuracy train : 87.35436248779297\n",
      "345536 ; loss 0.33 ; sentence/s 288 ; words/s 16204 ; accuracy train : 87.36516571044922\n",
      "351936 ; loss 0.33 ; sentence/s 303 ; words/s 17805 ; accuracy train : 87.36619567871094\n",
      "358336 ; loss 0.35 ; sentence/s 306 ; words/s 17649 ; accuracy train : 87.36244201660156\n",
      "364736 ; loss 0.35 ; sentence/s 302 ; words/s 17576 ; accuracy train : 87.34786224365234\n",
      "371136 ; loss 0.35 ; sentence/s 306 ; words/s 17665 ; accuracy train : 87.34132385253906\n",
      "377536 ; loss 0.34 ; sentence/s 295 ; words/s 17193 ; accuracy train : 87.34322357177734\n",
      "383936 ; loss 0.34 ; sentence/s 280 ; words/s 16265 ; accuracy train : 87.33958435058594\n",
      "390336 ; loss 0.33 ; sentence/s 305 ; words/s 17744 ; accuracy train : 87.34835815429688\n",
      "396736 ; loss 0.34 ; sentence/s 300 ; words/s 17722 ; accuracy train : 87.35005187988281\n",
      "403136 ; loss 0.34 ; sentence/s 282 ; words/s 16665 ; accuracy train : 87.35267639160156\n",
      "409536 ; loss 0.33 ; sentence/s 288 ; words/s 16433 ; accuracy train : 87.37158203125\n",
      "415936 ; loss 0.34 ; sentence/s 283 ; words/s 16321 ; accuracy train : 87.37187194824219\n",
      "422336 ; loss 0.33 ; sentence/s 287 ; words/s 17012 ; accuracy train : 87.38233947753906\n",
      "428736 ; loss 0.34 ; sentence/s 286 ; words/s 16589 ; accuracy train : 87.37943267822266\n",
      "435136 ; loss 0.35 ; sentence/s 272 ; words/s 15747 ; accuracy train : 87.37247467041016\n",
      "441536 ; loss 0.34 ; sentence/s 286 ; words/s 16729 ; accuracy train : 87.3731918334961\n",
      "447936 ; loss 0.35 ; sentence/s 285 ; words/s 16801 ; accuracy train : 87.36161041259766\n",
      "454336 ; loss 0.35 ; sentence/s 288 ; words/s 17479 ; accuracy train : 87.35519409179688\n",
      "460736 ; loss 0.33 ; sentence/s 299 ; words/s 17178 ; accuracy train : 87.36263275146484\n",
      "467136 ; loss 0.34 ; sentence/s 288 ; words/s 16906 ; accuracy train : 87.36365509033203\n",
      "473536 ; loss 0.35 ; sentence/s 277 ; words/s 16122 ; accuracy train : 87.35832214355469\n",
      "479936 ; loss 0.33 ; sentence/s 293 ; words/s 16592 ; accuracy train : 87.36624908447266\n",
      "486336 ; loss 0.34 ; sentence/s 278 ; words/s 16081 ; accuracy train : 87.37171173095703\n",
      "492736 ; loss 0.35 ; sentence/s 275 ; words/s 15678 ; accuracy train : 87.3691177368164\n",
      "499136 ; loss 0.34 ; sentence/s 276 ; words/s 15832 ; accuracy train : 87.37259674072266\n",
      "505536 ; loss 0.34 ; sentence/s 284 ; words/s 16491 ; accuracy train : 87.37559509277344\n",
      "511936 ; loss 0.34 ; sentence/s 294 ; words/s 16729 ; accuracy train : 87.3695297241211\n",
      "518336 ; loss 0.34 ; sentence/s 267 ; words/s 15497 ; accuracy train : 87.37036895751953\n",
      "524736 ; loss 0.34 ; sentence/s 264 ; words/s 15454 ; accuracy train : 87.37519073486328\n",
      "531136 ; loss 0.33 ; sentence/s 265 ; words/s 15562 ; accuracy train : 87.3795166015625\n",
      "537536 ; loss 0.34 ; sentence/s 263 ; words/s 15333 ; accuracy train : 87.3828125\n",
      "543936 ; loss 0.35 ; sentence/s 260 ; words/s 15004 ; accuracy train : 87.37849426269531\n",
      "results : epoch 7 ; mean accuracy train : 87.38457489013672\n",
      "\n",
      "VALIDATION : Epoch 7\n",
      "togrep : results : epoch 7 ; mean accuracy valid :              84.12924194335938\n",
      "saving model at epoch 7\n",
      "\n",
      "TRAINING : Epoch 8\n",
      "Learning rate : 0.0186413069581398\n",
      "6336 ; loss 0.34 ; sentence/s 265 ; words/s 15303 ; accuracy train : 87.671875\n",
      "12736 ; loss 0.33 ; sentence/s 261 ; words/s 15075 ; accuracy train : 87.6875\n",
      "19136 ; loss 0.35 ; sentence/s 249 ; words/s 14621 ; accuracy train : 87.44791412353516\n",
      "25536 ; loss 0.35 ; sentence/s 257 ; words/s 14760 ; accuracy train : 87.37109375\n",
      "31936 ; loss 0.33 ; sentence/s 264 ; words/s 15420 ; accuracy train : 87.4375\n",
      "38336 ; loss 0.34 ; sentence/s 269 ; words/s 15902 ; accuracy train : 87.4921875\n",
      "44736 ; loss 0.34 ; sentence/s 271 ; words/s 15807 ; accuracy train : 87.515625\n",
      "51136 ; loss 0.32 ; sentence/s 284 ; words/s 16396 ; accuracy train : 87.564453125\n",
      "57536 ; loss 0.34 ; sentence/s 289 ; words/s 16466 ; accuracy train : 87.60763549804688\n",
      "63936 ; loss 0.33 ; sentence/s 293 ; words/s 16579 ; accuracy train : 87.6484375\n",
      "70336 ; loss 0.36 ; sentence/s 289 ; words/s 16769 ; accuracy train : 87.5383529663086\n",
      "76736 ; loss 0.34 ; sentence/s 290 ; words/s 17141 ; accuracy train : 87.4921875\n",
      "83136 ; loss 0.35 ; sentence/s 292 ; words/s 17149 ; accuracy train : 87.43389129638672\n",
      "89536 ; loss 0.33 ; sentence/s 291 ; words/s 17154 ; accuracy train : 87.46428680419922\n",
      "95936 ; loss 0.33 ; sentence/s 287 ; words/s 16915 ; accuracy train : 87.49583435058594\n",
      "102336 ; loss 0.35 ; sentence/s 294 ; words/s 16842 ; accuracy train : 87.4697265625\n",
      "108736 ; loss 0.32 ; sentence/s 275 ; words/s 16005 ; accuracy train : 87.50183868408203\n",
      "115136 ; loss 0.34 ; sentence/s 271 ; words/s 15909 ; accuracy train : 87.52864837646484\n",
      "121536 ; loss 0.34 ; sentence/s 277 ; words/s 16548 ; accuracy train : 87.51644897460938\n",
      "127936 ; loss 0.32 ; sentence/s 279 ; words/s 16123 ; accuracy train : 87.54374694824219\n",
      "134336 ; loss 0.33 ; sentence/s 279 ; words/s 16341 ; accuracy train : 87.56622314453125\n",
      "140736 ; loss 0.34 ; sentence/s 294 ; words/s 16954 ; accuracy train : 87.59375\n",
      "147136 ; loss 0.34 ; sentence/s 295 ; words/s 16957 ; accuracy train : 87.578125\n",
      "153536 ; loss 0.33 ; sentence/s 282 ; words/s 16370 ; accuracy train : 87.587890625\n",
      "159936 ; loss 0.33 ; sentence/s 285 ; words/s 16743 ; accuracy train : 87.60375213623047\n",
      "166336 ; loss 0.34 ; sentence/s 291 ; words/s 16859 ; accuracy train : 87.58654022216797\n",
      "172736 ; loss 0.34 ; sentence/s 285 ; words/s 16946 ; accuracy train : 87.58101654052734\n",
      "179136 ; loss 0.33 ; sentence/s 280 ; words/s 16534 ; accuracy train : 87.58649444580078\n",
      "185536 ; loss 0.34 ; sentence/s 291 ; words/s 16810 ; accuracy train : 87.55711364746094\n",
      "191936 ; loss 0.34 ; sentence/s 284 ; words/s 16157 ; accuracy train : 87.5562515258789\n",
      "198336 ; loss 0.33 ; sentence/s 295 ; words/s 16911 ; accuracy train : 87.58618927001953\n",
      "204736 ; loss 0.33 ; sentence/s 294 ; words/s 17178 ; accuracy train : 87.59765625\n",
      "211136 ; loss 0.32 ; sentence/s 288 ; words/s 16467 ; accuracy train : 87.60653686523438\n",
      "217536 ; loss 0.35 ; sentence/s 267 ; words/s 15671 ; accuracy train : 87.60431671142578\n",
      "223936 ; loss 0.33 ; sentence/s 295 ; words/s 16810 ; accuracy train : 87.62142944335938\n",
      "230336 ; loss 0.34 ; sentence/s 289 ; words/s 16537 ; accuracy train : 87.60763549804688\n",
      "236736 ; loss 0.34 ; sentence/s 302 ; words/s 17142 ; accuracy train : 87.60472869873047\n",
      "243136 ; loss 0.33 ; sentence/s 295 ; words/s 17067 ; accuracy train : 87.60772705078125\n",
      "249536 ; loss 0.34 ; sentence/s 289 ; words/s 16670 ; accuracy train : 87.59294891357422\n",
      "255936 ; loss 0.33 ; sentence/s 293 ; words/s 17054 ; accuracy train : 87.5999984741211\n",
      "262336 ; loss 0.32 ; sentence/s 302 ; words/s 17412 ; accuracy train : 87.6162338256836\n",
      "268736 ; loss 0.34 ; sentence/s 287 ; words/s 16945 ; accuracy train : 87.62760162353516\n",
      "275136 ; loss 0.33 ; sentence/s 288 ; words/s 16830 ; accuracy train : 87.62063598632812\n",
      "281536 ; loss 0.32 ; sentence/s 286 ; words/s 16890 ; accuracy train : 87.63529968261719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287936 ; loss 0.34 ; sentence/s 283 ; words/s 16310 ; accuracy train : 87.6295166015625\n",
      "294336 ; loss 0.34 ; sentence/s 259 ; words/s 15541 ; accuracy train : 87.63451385498047\n",
      "300736 ; loss 0.35 ; sentence/s 269 ; words/s 15479 ; accuracy train : 87.61835479736328\n",
      "307136 ; loss 0.34 ; sentence/s 267 ; words/s 15587 ; accuracy train : 87.62174224853516\n",
      "313536 ; loss 0.34 ; sentence/s 275 ; words/s 15800 ; accuracy train : 87.60969543457031\n",
      "319936 ; loss 0.34 ; sentence/s 266 ; words/s 15862 ; accuracy train : 87.60437774658203\n",
      "326336 ; loss 0.34 ; sentence/s 266 ; words/s 15780 ; accuracy train : 87.59803771972656\n",
      "332736 ; loss 0.34 ; sentence/s 269 ; words/s 16442 ; accuracy train : 87.59134674072266\n",
      "339136 ; loss 0.35 ; sentence/s 276 ; words/s 16095 ; accuracy train : 87.58785247802734\n",
      "345536 ; loss 0.35 ; sentence/s 277 ; words/s 16015 ; accuracy train : 87.57002258300781\n",
      "351936 ; loss 0.35 ; sentence/s 299 ; words/s 17284 ; accuracy train : 87.56647491455078\n",
      "358336 ; loss 0.33 ; sentence/s 303 ; words/s 17913 ; accuracy train : 87.57450103759766\n",
      "364736 ; loss 0.33 ; sentence/s 305 ; words/s 17613 ; accuracy train : 87.5723648071289\n",
      "371136 ; loss 0.33 ; sentence/s 307 ; words/s 17440 ; accuracy train : 87.57408142089844\n",
      "377536 ; loss 0.34 ; sentence/s 303 ; words/s 18008 ; accuracy train : 87.57283020019531\n",
      "383936 ; loss 0.33 ; sentence/s 306 ; words/s 17662 ; accuracy train : 87.57682037353516\n",
      "390336 ; loss 0.33 ; sentence/s 307 ; words/s 17196 ; accuracy train : 87.57991790771484\n",
      "396736 ; loss 0.34 ; sentence/s 306 ; words/s 17592 ; accuracy train : 87.57283020019531\n",
      "403136 ; loss 0.33 ; sentence/s 291 ; words/s 16712 ; accuracy train : 87.57241821289062\n",
      "409536 ; loss 0.33 ; sentence/s 279 ; words/s 16580 ; accuracy train : 87.574951171875\n",
      "415936 ; loss 0.33 ; sentence/s 283 ; words/s 16748 ; accuracy train : 87.5798110961914\n",
      "422336 ; loss 0.33 ; sentence/s 281 ; words/s 15987 ; accuracy train : 87.5811996459961\n",
      "428736 ; loss 0.34 ; sentence/s 278 ; words/s 15876 ; accuracy train : 87.57882690429688\n",
      "435136 ; loss 0.34 ; sentence/s 278 ; words/s 15993 ; accuracy train : 87.57008361816406\n",
      "441536 ; loss 0.34 ; sentence/s 284 ; words/s 16756 ; accuracy train : 87.57382202148438\n",
      "447936 ; loss 0.34 ; sentence/s 269 ; words/s 15770 ; accuracy train : 87.56295013427734\n",
      "454336 ; loss 0.33 ; sentence/s 297 ; words/s 17184 ; accuracy train : 87.57108306884766\n",
      "460736 ; loss 0.33 ; sentence/s 281 ; words/s 16261 ; accuracy train : 87.57443237304688\n",
      "467136 ; loss 0.36 ; sentence/s 295 ; words/s 16993 ; accuracy train : 87.55843353271484\n",
      "473536 ; loss 0.34 ; sentence/s 271 ; words/s 15533 ; accuracy train : 87.55342102050781\n",
      "479936 ; loss 0.33 ; sentence/s 264 ; words/s 15283 ; accuracy train : 87.55229187011719\n",
      "486336 ; loss 0.35 ; sentence/s 278 ; words/s 16216 ; accuracy train : 87.53885650634766\n",
      "492736 ; loss 0.34 ; sentence/s 300 ; words/s 17002 ; accuracy train : 87.53551483154297\n",
      "499136 ; loss 0.33 ; sentence/s 287 ; words/s 16417 ; accuracy train : 87.5352554321289\n",
      "505536 ; loss 0.34 ; sentence/s 296 ; words/s 16946 ; accuracy train : 87.53184509277344\n",
      "511936 ; loss 0.34 ; sentence/s 295 ; words/s 17077 ; accuracy train : 87.52949523925781\n",
      "518336 ; loss 0.33 ; sentence/s 297 ; words/s 17366 ; accuracy train : 87.53646087646484\n",
      "524736 ; loss 0.33 ; sentence/s 288 ; words/s 16804 ; accuracy train : 87.5400161743164\n",
      "531136 ; loss 0.32 ; sentence/s 292 ; words/s 17075 ; accuracy train : 87.54932403564453\n",
      "537536 ; loss 0.33 ; sentence/s 287 ; words/s 16614 ; accuracy train : 87.5522689819336\n",
      "543936 ; loss 0.34 ; sentence/s 291 ; words/s 16801 ; accuracy train : 87.55110168457031\n",
      "results : epoch 8 ; mean accuracy train : 87.54966735839844\n",
      "\n",
      "VALIDATION : Epoch 8\n",
      "togrep : results : epoch 8 ; mean accuracy valid :              84.03779602050781\n",
      "Shrinking lr by : 5. New lr = 0.00372826139162796\n",
      "\n",
      "TRAINING : Epoch 9\n",
      "Learning rate : 0.0036909787777116804\n",
      "6336 ; loss 0.33 ; sentence/s 298 ; words/s 17358 ; accuracy train : 87.890625\n",
      "12736 ; loss 0.34 ; sentence/s 307 ; words/s 17438 ; accuracy train : 87.8828125\n",
      "19136 ; loss 0.32 ; sentence/s 304 ; words/s 17932 ; accuracy train : 87.93228912353516\n",
      "25536 ; loss 0.33 ; sentence/s 306 ; words/s 17682 ; accuracy train : 87.96875\n",
      "31936 ; loss 0.33 ; sentence/s 305 ; words/s 17671 ; accuracy train : 87.9375\n",
      "38336 ; loss 0.34 ; sentence/s 281 ; words/s 16368 ; accuracy train : 87.88802337646484\n",
      "44736 ; loss 0.33 ; sentence/s 255 ; words/s 14927 ; accuracy train : 87.87723541259766\n",
      "51136 ; loss 0.34 ; sentence/s 244 ; words/s 13810 ; accuracy train : 87.875\n",
      "57536 ; loss 0.34 ; sentence/s 251 ; words/s 14393 ; accuracy train : 87.81423950195312\n",
      "63936 ; loss 0.33 ; sentence/s 247 ; words/s 14495 ; accuracy train : 87.8609390258789\n",
      "70336 ; loss 0.33 ; sentence/s 267 ; words/s 15635 ; accuracy train : 87.88920593261719\n",
      "76736 ; loss 0.34 ; sentence/s 248 ; words/s 14331 ; accuracy train : 87.82291412353516\n",
      "83136 ; loss 0.34 ; sentence/s 248 ; words/s 14488 ; accuracy train : 87.78966522216797\n",
      "89536 ; loss 0.33 ; sentence/s 267 ; words/s 16084 ; accuracy train : 87.81584930419922\n",
      "95936 ; loss 0.34 ; sentence/s 274 ; words/s 16017 ; accuracy train : 87.8062515258789\n",
      "102336 ; loss 0.32 ; sentence/s 248 ; words/s 14602 ; accuracy train : 87.84765625\n",
      "108736 ; loss 0.33 ; sentence/s 254 ; words/s 14641 ; accuracy train : 87.86672973632812\n",
      "115136 ; loss 0.34 ; sentence/s 265 ; words/s 14965 ; accuracy train : 87.859375\n",
      "121536 ; loss 0.34 ; sentence/s 256 ; words/s 14823 ; accuracy train : 87.86677551269531\n",
      "127936 ; loss 0.33 ; sentence/s 270 ; words/s 15450 ; accuracy train : 87.86405944824219\n",
      "134336 ; loss 0.33 ; sentence/s 259 ; words/s 14792 ; accuracy train : 87.86904907226562\n",
      "140736 ; loss 0.33 ; sentence/s 242 ; words/s 14374 ; accuracy train : 87.88139343261719\n",
      "147136 ; loss 0.33 ; sentence/s 256 ; words/s 14533 ; accuracy train : 87.86616516113281\n",
      "153536 ; loss 0.33 ; sentence/s 267 ; words/s 15369 ; accuracy train : 87.896484375\n",
      "159936 ; loss 0.32 ; sentence/s 256 ; words/s 14947 ; accuracy train : 87.8949966430664\n",
      "166336 ; loss 0.34 ; sentence/s 259 ; words/s 15384 ; accuracy train : 87.87079620361328\n",
      "172736 ; loss 0.33 ; sentence/s 262 ; words/s 15149 ; accuracy train : 87.88831329345703\n",
      "179136 ; loss 0.33 ; sentence/s 258 ; words/s 15053 ; accuracy train : 87.87276458740234\n",
      "185536 ; loss 0.33 ; sentence/s 273 ; words/s 15624 ; accuracy train : 87.86476135253906\n",
      "191936 ; loss 0.34 ; sentence/s 269 ; words/s 15823 ; accuracy train : 87.8421859741211\n",
      "198336 ; loss 0.35 ; sentence/s 253 ; words/s 14896 ; accuracy train : 87.81048583984375\n",
      "204736 ; loss 0.34 ; sentence/s 267 ; words/s 15239 ; accuracy train : 87.79296875\n",
      "211136 ; loss 0.33 ; sentence/s 258 ; words/s 14842 ; accuracy train : 87.79782104492188\n",
      "217536 ; loss 0.34 ; sentence/s 271 ; words/s 15390 ; accuracy train : 87.76884460449219\n",
      "223936 ; loss 0.33 ; sentence/s 246 ; words/s 14441 ; accuracy train : 87.76830291748047\n",
      "230336 ; loss 0.32 ; sentence/s 248 ; words/s 14595 ; accuracy train : 87.78732299804688\n",
      "236736 ; loss 0.33 ; sentence/s 269 ; words/s 15837 ; accuracy train : 87.77702331542969\n",
      "243136 ; loss 0.33 ; sentence/s 268 ; words/s 15698 ; accuracy train : 87.7759017944336\n",
      "249536 ; loss 0.34 ; sentence/s 261 ; words/s 15297 ; accuracy train : 87.76763153076172\n",
      "255936 ; loss 0.33 ; sentence/s 266 ; words/s 15490 ; accuracy train : 87.7640609741211\n",
      "262336 ; loss 0.33 ; sentence/s 261 ; words/s 15145 ; accuracy train : 87.75647735595703\n",
      "268736 ; loss 0.34 ; sentence/s 262 ; words/s 15056 ; accuracy train : 87.75856018066406\n",
      "275136 ; loss 0.34 ; sentence/s 267 ; words/s 15427 ; accuracy train : 87.7470932006836\n",
      "281536 ; loss 0.33 ; sentence/s 269 ; words/s 15553 ; accuracy train : 87.75497436523438\n",
      "287936 ; loss 0.32 ; sentence/s 264 ; words/s 15502 ; accuracy train : 87.76667022705078\n",
      "294336 ; loss 0.34 ; sentence/s 263 ; words/s 15101 ; accuracy train : 87.75305938720703\n",
      "300736 ; loss 0.33 ; sentence/s 269 ; words/s 15625 ; accuracy train : 87.7586441040039\n",
      "307136 ; loss 0.34 ; sentence/s 260 ; words/s 14797 ; accuracy train : 87.74088287353516\n",
      "313536 ; loss 0.33 ; sentence/s 258 ; words/s 14723 ; accuracy train : 87.74234771728516\n",
      "319936 ; loss 0.34 ; sentence/s 267 ; words/s 15546 ; accuracy train : 87.7387466430664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326336 ; loss 0.33 ; sentence/s 274 ; words/s 15755 ; accuracy train : 87.74264526367188\n",
      "332736 ; loss 0.34 ; sentence/s 301 ; words/s 17819 ; accuracy train : 87.74188995361328\n",
      "339136 ; loss 0.35 ; sentence/s 293 ; words/s 17047 ; accuracy train : 87.72994995117188\n",
      "345536 ; loss 0.33 ; sentence/s 284 ; words/s 16849 ; accuracy train : 87.73321533203125\n",
      "351936 ; loss 0.33 ; sentence/s 288 ; words/s 16680 ; accuracy train : 87.73892211914062\n",
      "358336 ; loss 0.35 ; sentence/s 296 ; words/s 17462 ; accuracy train : 87.71791076660156\n",
      "364736 ; loss 0.34 ; sentence/s 304 ; words/s 17359 ; accuracy train : 87.71189880371094\n",
      "371136 ; loss 0.32 ; sentence/s 276 ; words/s 15956 ; accuracy train : 87.72144317626953\n",
      "377536 ; loss 0.33 ; sentence/s 266 ; words/s 15780 ; accuracy train : 87.7190170288086\n",
      "383936 ; loss 0.32 ; sentence/s 285 ; words/s 17047 ; accuracy train : 87.73124694824219\n",
      "390336 ; loss 0.34 ; sentence/s 283 ; words/s 16283 ; accuracy train : 87.72003173828125\n",
      "396736 ; loss 0.34 ; sentence/s 282 ; words/s 16246 ; accuracy train : 87.71370697021484\n",
      "403136 ; loss 0.32 ; sentence/s 275 ; words/s 16152 ; accuracy train : 87.71924591064453\n",
      "409536 ; loss 0.33 ; sentence/s 278 ; words/s 16375 ; accuracy train : 87.725830078125\n",
      "415936 ; loss 0.34 ; sentence/s 289 ; words/s 16821 ; accuracy train : 87.72259521484375\n",
      "422336 ; loss 0.34 ; sentence/s 302 ; words/s 17326 ; accuracy train : 87.7227783203125\n",
      "428736 ; loss 0.33 ; sentence/s 282 ; words/s 16083 ; accuracy train : 87.72248077392578\n",
      "435136 ; loss 0.34 ; sentence/s 281 ; words/s 16015 ; accuracy train : 87.7265625\n",
      "441536 ; loss 0.33 ; sentence/s 266 ; words/s 15763 ; accuracy train : 87.72123718261719\n",
      "447936 ; loss 0.34 ; sentence/s 285 ; words/s 16509 ; accuracy train : 87.71450805664062\n",
      "454336 ; loss 0.35 ; sentence/s 265 ; words/s 15462 ; accuracy train : 87.70268249511719\n",
      "460736 ; loss 0.33 ; sentence/s 283 ; words/s 15817 ; accuracy train : 87.69986724853516\n",
      "467136 ; loss 0.33 ; sentence/s 259 ; words/s 15475 ; accuracy train : 87.70419311523438\n",
      "473536 ; loss 0.34 ; sentence/s 264 ; words/s 15745 ; accuracy train : 87.6982650756836\n",
      "479936 ; loss 0.34 ; sentence/s 286 ; words/s 16623 ; accuracy train : 87.69541931152344\n",
      "486336 ; loss 0.33 ; sentence/s 270 ; words/s 15806 ; accuracy train : 87.70600128173828\n",
      "492736 ; loss 0.33 ; sentence/s 263 ; words/s 15177 ; accuracy train : 87.71043395996094\n",
      "499136 ; loss 0.33 ; sentence/s 274 ; words/s 15785 ; accuracy train : 87.7119369506836\n",
      "505536 ; loss 0.34 ; sentence/s 282 ; words/s 16544 ; accuracy train : 87.70846557617188\n",
      "511936 ; loss 0.32 ; sentence/s 280 ; words/s 16079 ; accuracy train : 87.716796875\n",
      "518336 ; loss 0.34 ; sentence/s 254 ; words/s 15193 ; accuracy train : 87.71546936035156\n",
      "524736 ; loss 0.34 ; sentence/s 289 ; words/s 16623 ; accuracy train : 87.7099838256836\n",
      "531136 ; loss 0.33 ; sentence/s 277 ; words/s 15938 ; accuracy train : 87.71046447753906\n",
      "537536 ; loss 0.35 ; sentence/s 284 ; words/s 16686 ; accuracy train : 87.69735717773438\n",
      "543936 ; loss 0.33 ; sentence/s 287 ; words/s 16499 ; accuracy train : 87.69889831542969\n",
      "results : epoch 9 ; mean accuracy train : 87.69893646240234\n",
      "\n",
      "VALIDATION : Epoch 9\n",
      "togrep : results : epoch 9 ; mean accuracy valid :              84.11907958984375\n",
      "Shrinking lr by : 5. New lr = 0.000738195755542336\n",
      "\n",
      "TRAINING : Epoch 10\n",
      "Learning rate : 0.0007308137979869126\n",
      "6336 ; loss 0.33 ; sentence/s 305 ; words/s 17442 ; accuracy train : 87.28125\n",
      "12736 ; loss 0.33 ; sentence/s 301 ; words/s 17384 ; accuracy train : 87.4765625\n",
      "19136 ; loss 0.34 ; sentence/s 302 ; words/s 17475 ; accuracy train : 87.33333587646484\n",
      "25536 ; loss 0.34 ; sentence/s 304 ; words/s 17504 ; accuracy train : 87.42578125\n",
      "31936 ; loss 0.34 ; sentence/s 303 ; words/s 17378 ; accuracy train : 87.45625305175781\n",
      "38336 ; loss 0.33 ; sentence/s 302 ; words/s 17752 ; accuracy train : 87.5625\n",
      "44736 ; loss 0.33 ; sentence/s 300 ; words/s 17658 ; accuracy train : 87.61161041259766\n",
      "51136 ; loss 0.33 ; sentence/s 305 ; words/s 17433 ; accuracy train : 87.5625\n",
      "57536 ; loss 0.34 ; sentence/s 305 ; words/s 17569 ; accuracy train : 87.52603912353516\n",
      "63936 ; loss 0.33 ; sentence/s 302 ; words/s 17519 ; accuracy train : 87.55000305175781\n",
      "70336 ; loss 0.34 ; sentence/s 300 ; words/s 17766 ; accuracy train : 87.53408813476562\n",
      "76736 ; loss 0.33 ; sentence/s 304 ; words/s 17912 ; accuracy train : 87.57552337646484\n",
      "83136 ; loss 0.33 ; sentence/s 302 ; words/s 17697 ; accuracy train : 87.63581848144531\n",
      "89536 ; loss 0.33 ; sentence/s 302 ; words/s 17727 ; accuracy train : 87.63951110839844\n",
      "95936 ; loss 0.32 ; sentence/s 288 ; words/s 16597 ; accuracy train : 87.62812805175781\n",
      "102336 ; loss 0.34 ; sentence/s 273 ; words/s 15836 ; accuracy train : 87.623046875\n",
      "108736 ; loss 0.33 ; sentence/s 284 ; words/s 16527 ; accuracy train : 87.640625\n",
      "115136 ; loss 0.34 ; sentence/s 278 ; words/s 16941 ; accuracy train : 87.62847137451172\n",
      "121536 ; loss 0.35 ; sentence/s 286 ; words/s 16607 ; accuracy train : 87.61595153808594\n",
      "127936 ; loss 0.33 ; sentence/s 275 ; words/s 16019 ; accuracy train : 87.61250305175781\n",
      "134336 ; loss 0.34 ; sentence/s 282 ; words/s 16261 ; accuracy train : 87.59375\n",
      "140736 ; loss 0.32 ; sentence/s 285 ; words/s 16282 ; accuracy train : 87.62287139892578\n",
      "147136 ; loss 0.32 ; sentence/s 290 ; words/s 16868 ; accuracy train : 87.63587188720703\n",
      "153536 ; loss 0.34 ; sentence/s 282 ; words/s 16393 ; accuracy train : 87.62174224853516\n",
      "159936 ; loss 0.33 ; sentence/s 287 ; words/s 16957 ; accuracy train : 87.64562225341797\n",
      "166336 ; loss 0.33 ; sentence/s 293 ; words/s 16808 ; accuracy train : 87.66165924072266\n",
      "172736 ; loss 0.33 ; sentence/s 282 ; words/s 16559 ; accuracy train : 87.67418670654297\n",
      "179136 ; loss 0.33 ; sentence/s 286 ; words/s 16679 ; accuracy train : 87.67913055419922\n",
      "185536 ; loss 0.35 ; sentence/s 284 ; words/s 16686 ; accuracy train : 87.6637954711914\n",
      "191936 ; loss 0.34 ; sentence/s 289 ; words/s 16518 ; accuracy train : 87.65052032470703\n",
      "198336 ; loss 0.32 ; sentence/s 279 ; words/s 16120 ; accuracy train : 87.66431427001953\n",
      "204736 ; loss 0.35 ; sentence/s 286 ; words/s 16105 ; accuracy train : 87.64697265625\n",
      "211136 ; loss 0.33 ; sentence/s 301 ; words/s 17103 ; accuracy train : 87.6358871459961\n",
      "217536 ; loss 0.33 ; sentence/s 301 ; words/s 17300 ; accuracy train : 87.64752197265625\n",
      "223936 ; loss 0.33 ; sentence/s 299 ; words/s 17173 ; accuracy train : 87.63838958740234\n",
      "230336 ; loss 0.33 ; sentence/s 290 ; words/s 17264 ; accuracy train : 87.63628387451172\n",
      "236736 ; loss 0.34 ; sentence/s 279 ; words/s 16033 ; accuracy train : 87.63471221923828\n",
      "243136 ; loss 0.33 ; sentence/s 253 ; words/s 14693 ; accuracy train : 87.63897705078125\n",
      "249536 ; loss 0.34 ; sentence/s 256 ; words/s 15031 ; accuracy train : 87.63701629638672\n",
      "255936 ; loss 0.31 ; sentence/s 285 ; words/s 16467 ; accuracy train : 87.65937805175781\n",
      "262336 ; loss 0.33 ; sentence/s 297 ; words/s 17789 ; accuracy train : 87.65206146240234\n",
      "268736 ; loss 0.33 ; sentence/s 304 ; words/s 17554 ; accuracy train : 87.66741180419922\n",
      "275136 ; loss 0.33 ; sentence/s 294 ; words/s 17066 ; accuracy train : 87.66387939453125\n",
      "281536 ; loss 0.34 ; sentence/s 298 ; words/s 17346 ; accuracy train : 87.66193389892578\n",
      "287936 ; loss 0.33 ; sentence/s 297 ; words/s 17391 ; accuracy train : 87.66909790039062\n",
      "294336 ; loss 0.32 ; sentence/s 294 ; words/s 17650 ; accuracy train : 87.6871566772461\n",
      "300736 ; loss 0.35 ; sentence/s 299 ; words/s 17395 ; accuracy train : 87.67652893066406\n",
      "307136 ; loss 0.33 ; sentence/s 301 ; words/s 17503 ; accuracy train : 87.68814849853516\n",
      "313536 ; loss 0.33 ; sentence/s 297 ; words/s 17439 ; accuracy train : 87.69929504394531\n",
      "319936 ; loss 0.33 ; sentence/s 301 ; words/s 17372 ; accuracy train : 87.69906616210938\n",
      "326336 ; loss 0.34 ; sentence/s 302 ; words/s 17276 ; accuracy train : 87.70587921142578\n",
      "332736 ; loss 0.32 ; sentence/s 293 ; words/s 16793 ; accuracy train : 87.72145080566406\n",
      "339136 ; loss 0.32 ; sentence/s 153 ; words/s 9136 ; accuracy train : 87.73820495605469\n",
      "345536 ; loss 0.34 ; sentence/s 253 ; words/s 14367 ; accuracy train : 87.7303237915039\n",
      "351936 ; loss 0.34 ; sentence/s 293 ; words/s 16629 ; accuracy train : 87.72016906738281\n",
      "358336 ; loss 0.34 ; sentence/s 300 ; words/s 17414 ; accuracy train : 87.72544860839844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364736 ; loss 0.33 ; sentence/s 302 ; words/s 17507 ; accuracy train : 87.73272705078125\n",
      "371136 ; loss 0.35 ; sentence/s 294 ; words/s 17339 ; accuracy train : 87.7184829711914\n",
      "377536 ; loss 0.33 ; sentence/s 300 ; words/s 17600 ; accuracy train : 87.72854614257812\n",
      "383936 ; loss 0.33 ; sentence/s 280 ; words/s 16072 ; accuracy train : 87.73645782470703\n",
      "390336 ; loss 0.33 ; sentence/s 295 ; words/s 17315 ; accuracy train : 87.74411010742188\n",
      "396736 ; loss 0.32 ; sentence/s 297 ; words/s 16890 ; accuracy train : 87.7429428100586\n",
      "403136 ; loss 0.33 ; sentence/s 284 ; words/s 16412 ; accuracy train : 87.73983001708984\n",
      "409536 ; loss 0.34 ; sentence/s 295 ; words/s 16489 ; accuracy train : 87.739990234375\n",
      "415936 ; loss 0.34 ; sentence/s 243 ; words/s 14344 ; accuracy train : 87.73966217041016\n",
      "422336 ; loss 0.32 ; sentence/s 280 ; words/s 16240 ; accuracy train : 87.74384307861328\n",
      "428736 ; loss 0.33 ; sentence/s 292 ; words/s 17164 ; accuracy train : 87.75233459472656\n",
      "435136 ; loss 0.33 ; sentence/s 298 ; words/s 17377 ; accuracy train : 87.75482177734375\n",
      "441536 ; loss 0.34 ; sentence/s 301 ; words/s 17390 ; accuracy train : 87.75090789794922\n",
      "447936 ; loss 0.32 ; sentence/s 301 ; words/s 17549 ; accuracy train : 87.7591552734375\n",
      "454336 ; loss 0.33 ; sentence/s 301 ; words/s 17565 ; accuracy train : 87.76012420654297\n",
      "460736 ; loss 0.34 ; sentence/s 299 ; words/s 16857 ; accuracy train : 87.75521087646484\n",
      "467136 ; loss 0.32 ; sentence/s 304 ; words/s 17484 ; accuracy train : 87.75834655761719\n",
      "473536 ; loss 0.33 ; sentence/s 293 ; words/s 16810 ; accuracy train : 87.7601318359375\n",
      "479936 ; loss 0.33 ; sentence/s 281 ; words/s 16420 ; accuracy train : 87.76145935058594\n",
      "486336 ; loss 0.33 ; sentence/s 293 ; words/s 17212 ; accuracy train : 87.75740051269531\n",
      "492736 ; loss 0.35 ; sentence/s 293 ; words/s 16687 ; accuracy train : 87.74188232421875\n",
      "499136 ; loss 0.34 ; sentence/s 291 ; words/s 17121 ; accuracy train : 87.73857879638672\n",
      "505536 ; loss 0.33 ; sentence/s 297 ; words/s 16968 ; accuracy train : 87.74505615234375\n",
      "511936 ; loss 0.34 ; sentence/s 285 ; words/s 16610 ; accuracy train : 87.7427749633789\n",
      "518336 ; loss 0.34 ; sentence/s 294 ; words/s 17431 ; accuracy train : 87.73707580566406\n",
      "524736 ; loss 0.34 ; sentence/s 300 ; words/s 17680 ; accuracy train : 87.72942352294922\n",
      "531136 ; loss 0.33 ; sentence/s 299 ; words/s 17473 ; accuracy train : 87.73606872558594\n",
      "537536 ; loss 0.33 ; sentence/s 298 ; words/s 17810 ; accuracy train : 87.73828125\n",
      "543936 ; loss 0.34 ; sentence/s 305 ; words/s 17534 ; accuracy train : 87.734375\n",
      "results : epoch 10 ; mean accuracy train : 87.73188018798828\n",
      "\n",
      "VALIDATION : Epoch 10\n",
      "togrep : results : epoch 10 ; mean accuracy valid :              84.1902084350586\n",
      "saving model at epoch 10\n",
      "\n",
      "TRAINING : Epoch 11\n",
      "Learning rate : 0.0007235056600070435\n",
      "6336 ; loss 0.33 ; sentence/s 288 ; words/s 16697 ; accuracy train : 88.15625\n",
      "12736 ; loss 0.33 ; sentence/s 293 ; words/s 17269 ; accuracy train : 87.9140625\n",
      "19136 ; loss 0.33 ; sentence/s 302 ; words/s 17902 ; accuracy train : 87.88021087646484\n",
      "25536 ; loss 0.33 ; sentence/s 303 ; words/s 17727 ; accuracy train : 87.87890625\n",
      "31936 ; loss 0.33 ; sentence/s 299 ; words/s 16996 ; accuracy train : 87.9312515258789\n",
      "38336 ; loss 0.34 ; sentence/s 306 ; words/s 17498 ; accuracy train : 87.953125\n",
      "44736 ; loss 0.34 ; sentence/s 297 ; words/s 16864 ; accuracy train : 87.92857360839844\n",
      "51136 ; loss 0.33 ; sentence/s 284 ; words/s 16967 ; accuracy train : 87.900390625\n",
      "57536 ; loss 0.33 ; sentence/s 294 ; words/s 17407 ; accuracy train : 87.82986450195312\n",
      "63936 ; loss 0.33 ; sentence/s 288 ; words/s 16441 ; accuracy train : 87.8140640258789\n",
      "70336 ; loss 0.32 ; sentence/s 299 ; words/s 17267 ; accuracy train : 87.89346313476562\n",
      "76736 ; loss 0.33 ; sentence/s 288 ; words/s 16615 ; accuracy train : 87.88802337646484\n",
      "83136 ; loss 0.34 ; sentence/s 276 ; words/s 16026 ; accuracy train : 87.87980651855469\n",
      "89536 ; loss 0.33 ; sentence/s 300 ; words/s 17523 ; accuracy train : 87.83928680419922\n",
      "95936 ; loss 0.33 ; sentence/s 292 ; words/s 17262 ; accuracy train : 87.84062194824219\n",
      "102336 ; loss 0.34 ; sentence/s 285 ; words/s 16336 ; accuracy train : 87.8583984375\n",
      "108736 ; loss 0.33 ; sentence/s 293 ; words/s 17202 ; accuracy train : 87.859375\n",
      "115136 ; loss 0.34 ; sentence/s 298 ; words/s 17427 ; accuracy train : 87.82378387451172\n",
      "121536 ; loss 0.35 ; sentence/s 295 ; words/s 16871 ; accuracy train : 87.78947448730469\n",
      "127936 ; loss 0.32 ; sentence/s 295 ; words/s 17254 ; accuracy train : 87.80078125\n",
      "134336 ; loss 0.33 ; sentence/s 196 ; words/s 11331 ; accuracy train : 87.80059814453125\n",
      "140736 ; loss 0.35 ; sentence/s 190 ; words/s 11024 ; accuracy train : 87.75354766845703\n",
      "147136 ; loss 0.33 ; sentence/s 282 ; words/s 16306 ; accuracy train : 87.78192901611328\n",
      "153536 ; loss 0.34 ; sentence/s 294 ; words/s 17059 ; accuracy train : 87.7890625\n",
      "159936 ; loss 0.33 ; sentence/s 287 ; words/s 16772 ; accuracy train : 87.80562591552734\n",
      "166336 ; loss 0.33 ; sentence/s 271 ; words/s 16240 ; accuracy train : 87.80408477783203\n",
      "172736 ; loss 0.33 ; sentence/s 298 ; words/s 17447 ; accuracy train : 87.81192016601562\n",
      "179136 ; loss 0.33 ; sentence/s 291 ; words/s 16536 ; accuracy train : 87.82645416259766\n",
      "185536 ; loss 0.33 ; sentence/s 296 ; words/s 17016 ; accuracy train : 87.82327270507812\n",
      "191936 ; loss 0.32 ; sentence/s 297 ; words/s 17646 ; accuracy train : 87.83125305175781\n",
      "198336 ; loss 0.34 ; sentence/s 295 ; words/s 17391 ; accuracy train : 87.82207489013672\n",
      "204736 ; loss 0.33 ; sentence/s 303 ; words/s 17313 ; accuracy train : 87.82861328125\n",
      "211136 ; loss 0.34 ; sentence/s 283 ; words/s 16417 ; accuracy train : 87.80303192138672\n",
      "217536 ; loss 0.33 ; sentence/s 275 ; words/s 15602 ; accuracy train : 87.80836486816406\n",
      "223936 ; loss 0.33 ; sentence/s 298 ; words/s 17688 ; accuracy train : 87.80000305175781\n",
      "230336 ; loss 0.33 ; sentence/s 285 ; words/s 17032 ; accuracy train : 87.80555725097656\n",
      "236736 ; loss 0.34 ; sentence/s 287 ; words/s 16561 ; accuracy train : 87.80447387695312\n",
      "243136 ; loss 0.33 ; sentence/s 289 ; words/s 17058 ; accuracy train : 87.80592346191406\n",
      "249536 ; loss 0.33 ; sentence/s 304 ; words/s 17352 ; accuracy train : 87.81730651855469\n",
      "255936 ; loss 0.33 ; sentence/s 270 ; words/s 15630 ; accuracy train : 87.81953430175781\n",
      "262336 ; loss 0.34 ; sentence/s 282 ; words/s 16291 ; accuracy train : 87.80640411376953\n",
      "268736 ; loss 0.33 ; sentence/s 285 ; words/s 16327 ; accuracy train : 87.81026458740234\n",
      "275136 ; loss 0.34 ; sentence/s 282 ; words/s 16271 ; accuracy train : 87.80050659179688\n",
      "281536 ; loss 0.33 ; sentence/s 247 ; words/s 14358 ; accuracy train : 87.81143188476562\n",
      "287936 ; loss 0.33 ; sentence/s 245 ; words/s 14378 ; accuracy train : 87.81041717529297\n",
      "294336 ; loss 0.34 ; sentence/s 251 ; words/s 14423 ; accuracy train : 87.80774688720703\n",
      "300736 ; loss 0.34 ; sentence/s 293 ; words/s 16505 ; accuracy train : 87.79388427734375\n",
      "307136 ; loss 0.35 ; sentence/s 304 ; words/s 17429 ; accuracy train : 87.77474212646484\n",
      "313536 ; loss 0.33 ; sentence/s 286 ; words/s 16777 ; accuracy train : 87.7822036743164\n",
      "319936 ; loss 0.33 ; sentence/s 285 ; words/s 17040 ; accuracy train : 87.78062438964844\n",
      "326336 ; loss 0.34 ; sentence/s 301 ; words/s 17419 ; accuracy train : 87.77083587646484\n",
      "332736 ; loss 0.34 ; sentence/s 288 ; words/s 16810 ; accuracy train : 87.77073669433594\n",
      "339136 ; loss 0.34 ; sentence/s 298 ; words/s 17137 ; accuracy train : 87.7697525024414\n",
      "345536 ; loss 0.34 ; sentence/s 280 ; words/s 16390 ; accuracy train : 87.76678466796875\n",
      "351936 ; loss 0.33 ; sentence/s 294 ; words/s 17218 ; accuracy train : 87.76846313476562\n",
      "358336 ; loss 0.33 ; sentence/s 300 ; words/s 16934 ; accuracy train : 87.77008819580078\n",
      "364736 ; loss 0.32 ; sentence/s 294 ; words/s 16931 ; accuracy train : 87.7870101928711\n",
      "371136 ; loss 0.32 ; sentence/s 293 ; words/s 16898 ; accuracy train : 87.79444885253906\n",
      "377536 ; loss 0.35 ; sentence/s 295 ; words/s 16918 ; accuracy train : 87.7793960571289\n",
      "383936 ; loss 0.34 ; sentence/s 290 ; words/s 16231 ; accuracy train : 87.7750015258789\n",
      "390336 ; loss 0.33 ; sentence/s 289 ; words/s 16772 ; accuracy train : 87.78150939941406\n",
      "396736 ; loss 0.33 ; sentence/s 300 ; words/s 17392 ; accuracy train : 87.77444458007812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403136 ; loss 0.34 ; sentence/s 298 ; words/s 17293 ; accuracy train : 87.76984405517578\n",
      "409536 ; loss 0.34 ; sentence/s 302 ; words/s 17191 ; accuracy train : 87.76220703125\n",
      "415936 ; loss 0.33 ; sentence/s 299 ; words/s 17490 ; accuracy train : 87.75985717773438\n",
      "422336 ; loss 0.34 ; sentence/s 292 ; words/s 16785 ; accuracy train : 87.75521087646484\n",
      "428736 ; loss 0.33 ; sentence/s 300 ; words/s 17161 ; accuracy train : 87.75326538085938\n",
      "435136 ; loss 0.33 ; sentence/s 299 ; words/s 17130 ; accuracy train : 87.75252532958984\n",
      "441536 ; loss 0.34 ; sentence/s 298 ; words/s 16788 ; accuracy train : 87.74932098388672\n",
      "447936 ; loss 0.33 ; sentence/s 297 ; words/s 17488 ; accuracy train : 87.74508666992188\n",
      "454336 ; loss 0.31 ; sentence/s 279 ; words/s 16366 ; accuracy train : 87.75814056396484\n",
      "460736 ; loss 0.33 ; sentence/s 290 ; words/s 16824 ; accuracy train : 87.75824737548828\n",
      "467136 ; loss 0.34 ; sentence/s 261 ; words/s 15168 ; accuracy train : 87.75149536132812\n",
      "473536 ; loss 0.33 ; sentence/s 265 ; words/s 14951 ; accuracy train : 87.75126647949219\n",
      "479936 ; loss 0.32 ; sentence/s 284 ; words/s 16808 ; accuracy train : 87.75479125976562\n",
      "486336 ; loss 0.34 ; sentence/s 297 ; words/s 17345 ; accuracy train : 87.74609375\n",
      "492736 ; loss 0.34 ; sentence/s 290 ; words/s 16989 ; accuracy train : 87.74370574951172\n",
      "499136 ; loss 0.33 ; sentence/s 268 ; words/s 15674 ; accuracy train : 87.74579620361328\n",
      "505536 ; loss 0.35 ; sentence/s 284 ; words/s 16683 ; accuracy train : 87.7379379272461\n",
      "511936 ; loss 0.34 ; sentence/s 294 ; words/s 17449 ; accuracy train : 87.736328125\n",
      "518336 ; loss 0.32 ; sentence/s 292 ; words/s 17439 ; accuracy train : 87.74324798583984\n",
      "524736 ; loss 0.34 ; sentence/s 301 ; words/s 17736 ; accuracy train : 87.73780822753906\n",
      "531136 ; loss 0.33 ; sentence/s 301 ; words/s 17388 ; accuracy train : 87.73644256591797\n",
      "537536 ; loss 0.32 ; sentence/s 301 ; words/s 17887 ; accuracy train : 87.74311828613281\n",
      "543936 ; loss 0.33 ; sentence/s 298 ; words/s 17718 ; accuracy train : 87.74172973632812\n",
      "results : epoch 11 ; mean accuracy train : 87.74007415771484\n",
      "\n",
      "VALIDATION : Epoch 11\n",
      "togrep : results : epoch 11 ; mean accuracy valid :              84.24100494384766\n",
      "saving model at epoch 11\n",
      "\n",
      "TRAINING : Epoch 12\n",
      "Learning rate : 0.0007162706034069731\n",
      "6336 ; loss 0.34 ; sentence/s 304 ; words/s 17535 ; accuracy train : 87.390625\n",
      "12736 ; loss 0.33 ; sentence/s 304 ; words/s 17431 ; accuracy train : 87.6640625\n",
      "19136 ; loss 0.33 ; sentence/s 302 ; words/s 17762 ; accuracy train : 87.71875\n",
      "25536 ; loss 0.34 ; sentence/s 288 ; words/s 17173 ; accuracy train : 87.63671875\n",
      "31936 ; loss 0.33 ; sentence/s 291 ; words/s 16815 ; accuracy train : 87.6624984741211\n",
      "38336 ; loss 0.34 ; sentence/s 296 ; words/s 17044 ; accuracy train : 87.625\n",
      "44736 ; loss 0.33 ; sentence/s 291 ; words/s 17088 ; accuracy train : 87.71205139160156\n",
      "51136 ; loss 0.34 ; sentence/s 296 ; words/s 16830 ; accuracy train : 87.669921875\n",
      "57536 ; loss 0.32 ; sentence/s 304 ; words/s 17503 ; accuracy train : 87.74826049804688\n",
      "63936 ; loss 0.34 ; sentence/s 287 ; words/s 16965 ; accuracy train : 87.7249984741211\n",
      "70336 ; loss 0.32 ; sentence/s 303 ; words/s 17365 ; accuracy train : 87.79829406738281\n",
      "76736 ; loss 0.34 ; sentence/s 302 ; words/s 17315 ; accuracy train : 87.81640625\n",
      "83136 ; loss 0.33 ; sentence/s 294 ; words/s 17078 ; accuracy train : 87.83173370361328\n",
      "89536 ; loss 0.33 ; sentence/s 295 ; words/s 17017 ; accuracy train : 87.82142639160156\n",
      "95936 ; loss 0.33 ; sentence/s 295 ; words/s 17010 ; accuracy train : 87.81979370117188\n",
      "102336 ; loss 0.35 ; sentence/s 301 ; words/s 17547 ; accuracy train : 87.8017578125\n",
      "108736 ; loss 0.33 ; sentence/s 300 ; words/s 17827 ; accuracy train : 87.81525421142578\n",
      "115136 ; loss 0.33 ; sentence/s 304 ; words/s 17787 ; accuracy train : 87.82986450195312\n",
      "121536 ; loss 0.35 ; sentence/s 301 ; words/s 17737 ; accuracy train : 87.80098724365234\n",
      "127936 ; loss 0.32 ; sentence/s 284 ; words/s 17010 ; accuracy train : 87.8187484741211\n",
      "134336 ; loss 0.33 ; sentence/s 278 ; words/s 16303 ; accuracy train : 87.81845092773438\n",
      "140736 ; loss 0.34 ; sentence/s 299 ; words/s 17188 ; accuracy train : 87.8053970336914\n",
      "147136 ; loss 0.32 ; sentence/s 302 ; words/s 17320 ; accuracy train : 87.84850311279297\n",
      "153536 ; loss 0.33 ; sentence/s 293 ; words/s 17375 ; accuracy train : 87.853515625\n",
      "159936 ; loss 0.35 ; sentence/s 297 ; words/s 17097 ; accuracy train : 87.83687591552734\n",
      "166336 ; loss 0.34 ; sentence/s 299 ; words/s 17389 ; accuracy train : 87.81730651855469\n",
      "172736 ; loss 0.35 ; sentence/s 301 ; words/s 16805 ; accuracy train : 87.78588104248047\n",
      "179136 ; loss 0.33 ; sentence/s 305 ; words/s 17560 ; accuracy train : 87.78627014160156\n",
      "185536 ; loss 0.33 ; sentence/s 307 ; words/s 17694 ; accuracy train : 87.79633331298828\n",
      "191936 ; loss 0.33 ; sentence/s 301 ; words/s 17641 ; accuracy train : 87.7874984741211\n",
      "198336 ; loss 0.34 ; sentence/s 282 ; words/s 16778 ; accuracy train : 87.76158905029297\n",
      "204736 ; loss 0.33 ; sentence/s 279 ; words/s 16693 ; accuracy train : 87.76953125\n",
      "211136 ; loss 0.33 ; sentence/s 290 ; words/s 17223 ; accuracy train : 87.77793884277344\n",
      "217536 ; loss 0.33 ; sentence/s 294 ; words/s 17117 ; accuracy train : 87.77757263183594\n",
      "223936 ; loss 0.33 ; sentence/s 287 ; words/s 16547 ; accuracy train : 87.76786041259766\n",
      "230336 ; loss 0.34 ; sentence/s 290 ; words/s 16803 ; accuracy train : 87.76302337646484\n",
      "236736 ; loss 0.33 ; sentence/s 295 ; words/s 17152 ; accuracy train : 87.7584457397461\n",
      "243136 ; loss 0.33 ; sentence/s 297 ; words/s 17107 ; accuracy train : 87.76521301269531\n",
      "249536 ; loss 0.33 ; sentence/s 284 ; words/s 16956 ; accuracy train : 87.7684326171875\n",
      "255936 ; loss 0.35 ; sentence/s 284 ; words/s 16382 ; accuracy train : 87.7578125\n",
      "262336 ; loss 0.32 ; sentence/s 290 ; words/s 16767 ; accuracy train : 87.77019500732422\n",
      "268736 ; loss 0.32 ; sentence/s 296 ; words/s 17029 ; accuracy train : 87.77641296386719\n",
      "275136 ; loss 0.32 ; sentence/s 289 ; words/s 17025 ; accuracy train : 87.78524780273438\n",
      "281536 ; loss 0.33 ; sentence/s 297 ; words/s 17436 ; accuracy train : 87.7883529663086\n",
      "287936 ; loss 0.33 ; sentence/s 288 ; words/s 16952 ; accuracy train : 87.78402709960938\n",
      "294336 ; loss 0.33 ; sentence/s 288 ; words/s 16795 ; accuracy train : 87.79619598388672\n",
      "300736 ; loss 0.32 ; sentence/s 295 ; words/s 16907 ; accuracy train : 87.80917358398438\n",
      "307136 ; loss 0.34 ; sentence/s 303 ; words/s 17210 ; accuracy train : 87.798828125\n",
      "313536 ; loss 0.33 ; sentence/s 301 ; words/s 17403 ; accuracy train : 87.79081726074219\n",
      "319936 ; loss 0.34 ; sentence/s 298 ; words/s 17562 ; accuracy train : 87.78593444824219\n",
      "326336 ; loss 0.33 ; sentence/s 301 ; words/s 17357 ; accuracy train : 87.77818298339844\n",
      "332736 ; loss 0.34 ; sentence/s 292 ; words/s 16917 ; accuracy train : 87.76171875\n",
      "339136 ; loss 0.34 ; sentence/s 296 ; words/s 17565 ; accuracy train : 87.75382995605469\n",
      "345536 ; loss 0.34 ; sentence/s 294 ; words/s 16954 ; accuracy train : 87.74855041503906\n",
      "351936 ; loss 0.33 ; sentence/s 287 ; words/s 16865 ; accuracy train : 87.74829864501953\n",
      "358336 ; loss 0.33 ; sentence/s 297 ; words/s 17458 ; accuracy train : 87.74888610839844\n",
      "364736 ; loss 0.33 ; sentence/s 300 ; words/s 17273 ; accuracy train : 87.74177551269531\n",
      "371136 ; loss 0.34 ; sentence/s 299 ; words/s 16973 ; accuracy train : 87.74542236328125\n",
      "377536 ; loss 0.33 ; sentence/s 287 ; words/s 16479 ; accuracy train : 87.74231719970703\n",
      "383936 ; loss 0.33 ; sentence/s 289 ; words/s 16886 ; accuracy train : 87.74270629882812\n",
      "390336 ; loss 0.33 ; sentence/s 295 ; words/s 17443 ; accuracy train : 87.74718475341797\n",
      "396736 ; loss 0.34 ; sentence/s 296 ; words/s 17339 ; accuracy train : 87.74420166015625\n",
      "403136 ; loss 0.32 ; sentence/s 241 ; words/s 14051 ; accuracy train : 87.75025177001953\n",
      "409536 ; loss 0.33 ; sentence/s 244 ; words/s 13809 ; accuracy train : 87.758056640625\n",
      "415936 ; loss 0.35 ; sentence/s 239 ; words/s 13642 ; accuracy train : 87.75697326660156\n",
      "422336 ; loss 0.33 ; sentence/s 240 ; words/s 14071 ; accuracy train : 87.75875854492188\n",
      "428736 ; loss 0.33 ; sentence/s 279 ; words/s 16223 ; accuracy train : 87.7525634765625\n",
      "435136 ; loss 0.33 ; sentence/s 287 ; words/s 16716 ; accuracy train : 87.75115203857422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441536 ; loss 0.33 ; sentence/s 274 ; words/s 16288 ; accuracy train : 87.74864196777344\n",
      "447936 ; loss 0.34 ; sentence/s 275 ; words/s 16706 ; accuracy train : 87.75245666503906\n",
      "454336 ; loss 0.34 ; sentence/s 296 ; words/s 17106 ; accuracy train : 87.7486801147461\n",
      "460736 ; loss 0.35 ; sentence/s 294 ; words/s 17198 ; accuracy train : 87.74066925048828\n",
      "467136 ; loss 0.32 ; sentence/s 292 ; words/s 16991 ; accuracy train : 87.75106811523438\n",
      "473536 ; loss 0.34 ; sentence/s 302 ; words/s 17611 ; accuracy train : 87.74176788330078\n",
      "479936 ; loss 0.32 ; sentence/s 293 ; words/s 17079 ; accuracy train : 87.74166870117188\n",
      "486336 ; loss 0.32 ; sentence/s 286 ; words/s 16649 ; accuracy train : 87.74712371826172\n",
      "492736 ; loss 0.35 ; sentence/s 289 ; words/s 16217 ; accuracy train : 87.73741912841797\n",
      "499136 ; loss 0.33 ; sentence/s 306 ; words/s 17359 ; accuracy train : 87.73878479003906\n",
      "505536 ; loss 0.34 ; sentence/s 307 ; words/s 17557 ; accuracy train : 87.73120880126953\n",
      "511936 ; loss 0.32 ; sentence/s 300 ; words/s 17712 ; accuracy train : 87.7337875366211\n",
      "518336 ; loss 0.34 ; sentence/s 308 ; words/s 17313 ; accuracy train : 87.730712890625\n",
      "524736 ; loss 0.32 ; sentence/s 306 ; words/s 17952 ; accuracy train : 87.73609161376953\n",
      "531136 ; loss 0.34 ; sentence/s 307 ; words/s 17476 ; accuracy train : 87.73625946044922\n",
      "537536 ; loss 0.34 ; sentence/s 294 ; words/s 16656 ; accuracy train : 87.73362731933594\n",
      "543936 ; loss 0.32 ; sentence/s 301 ; words/s 17812 ; accuracy train : 87.73896789550781\n",
      "results : epoch 12 ; mean accuracy train : 87.73497772216797\n",
      "\n",
      "VALIDATION : Epoch 12\n",
      "togrep : results : epoch 12 ; mean accuracy valid :              84.11907958984375\n",
      "Shrinking lr by : 5. New lr = 0.0001432541206813946\n",
      "\n",
      "TRAINING : Epoch 13\n",
      "Learning rate : 0.00014182157947458066\n",
      "6336 ; loss 0.34 ; sentence/s 302 ; words/s 17616 ; accuracy train : 87.53125\n",
      "12736 ; loss 0.36 ; sentence/s 306 ; words/s 17751 ; accuracy train : 87.2578125\n",
      "19136 ; loss 0.33 ; sentence/s 291 ; words/s 17392 ; accuracy train : 87.49478912353516\n",
      "25536 ; loss 0.34 ; sentence/s 291 ; words/s 17031 ; accuracy train : 87.4765625\n",
      "31936 ; loss 0.33 ; sentence/s 300 ; words/s 17762 ; accuracy train : 87.5218734741211\n",
      "38336 ; loss 0.32 ; sentence/s 304 ; words/s 17630 ; accuracy train : 87.6953125\n",
      "44736 ; loss 0.34 ; sentence/s 272 ; words/s 15859 ; accuracy train : 87.67857360839844\n",
      "51136 ; loss 0.32 ; sentence/s 286 ; words/s 16365 ; accuracy train : 87.7265625\n",
      "57536 ; loss 0.33 ; sentence/s 291 ; words/s 17081 ; accuracy train : 87.76909637451172\n",
      "63936 ; loss 0.32 ; sentence/s 293 ; words/s 17155 ; accuracy train : 87.7984390258789\n",
      "70336 ; loss 0.33 ; sentence/s 297 ; words/s 16949 ; accuracy train : 87.8210220336914\n",
      "76736 ; loss 0.33 ; sentence/s 283 ; words/s 16509 ; accuracy train : 87.78385162353516\n",
      "83136 ; loss 0.33 ; sentence/s 285 ; words/s 16168 ; accuracy train : 87.78726196289062\n",
      "89536 ; loss 0.34 ; sentence/s 289 ; words/s 16821 ; accuracy train : 87.78459930419922\n",
      "95936 ; loss 0.34 ; sentence/s 293 ; words/s 16756 ; accuracy train : 87.74583435058594\n",
      "102336 ; loss 0.33 ; sentence/s 278 ; words/s 16216 ; accuracy train : 87.7392578125\n",
      "108736 ; loss 0.32 ; sentence/s 284 ; words/s 16294 ; accuracy train : 87.76103210449219\n",
      "115136 ; loss 0.35 ; sentence/s 301 ; words/s 17357 ; accuracy train : 87.71961975097656\n",
      "121536 ; loss 0.33 ; sentence/s 286 ; words/s 16542 ; accuracy train : 87.72532653808594\n",
      "127936 ; loss 0.33 ; sentence/s 291 ; words/s 16901 ; accuracy train : 87.71406555175781\n",
      "134336 ; loss 0.32 ; sentence/s 283 ; words/s 16658 ; accuracy train : 87.76041412353516\n",
      "140736 ; loss 0.34 ; sentence/s 280 ; words/s 16410 ; accuracy train : 87.75426483154297\n",
      "147136 ; loss 0.32 ; sentence/s 282 ; words/s 16519 ; accuracy train : 87.76155090332031\n",
      "153536 ; loss 0.34 ; sentence/s 282 ; words/s 16629 ; accuracy train : 87.75130462646484\n",
      "159936 ; loss 0.34 ; sentence/s 287 ; words/s 17092 ; accuracy train : 87.73937225341797\n",
      "166336 ; loss 0.32 ; sentence/s 288 ; words/s 16641 ; accuracy train : 87.77523803710938\n",
      "172736 ; loss 0.34 ; sentence/s 278 ; words/s 16349 ; accuracy train : 87.78182983398438\n",
      "179136 ; loss 0.34 ; sentence/s 291 ; words/s 17422 ; accuracy train : 87.77232360839844\n",
      "185536 ; loss 0.33 ; sentence/s 293 ; words/s 17243 ; accuracy train : 87.77801513671875\n",
      "191936 ; loss 0.32 ; sentence/s 296 ; words/s 17339 ; accuracy train : 87.79635620117188\n",
      "198336 ; loss 0.33 ; sentence/s 299 ; words/s 17065 ; accuracy train : 87.79486083984375\n",
      "204736 ; loss 0.33 ; sentence/s 299 ; words/s 17649 ; accuracy train : 87.7919921875\n",
      "211136 ; loss 0.33 ; sentence/s 330 ; words/s 19099 ; accuracy train : 87.78314208984375\n",
      "217536 ; loss 0.33 ; sentence/s 282 ; words/s 16469 ; accuracy train : 87.7945785522461\n",
      "223936 ; loss 0.33 ; sentence/s 223 ; words/s 12916 ; accuracy train : 87.80000305175781\n",
      "230336 ; loss 0.35 ; sentence/s 282 ; words/s 16350 ; accuracy train : 87.78602600097656\n",
      "236736 ; loss 0.33 ; sentence/s 295 ; words/s 16912 ; accuracy train : 87.80658721923828\n",
      "243136 ; loss 0.33 ; sentence/s 295 ; words/s 17203 ; accuracy train : 87.80551147460938\n",
      "249536 ; loss 0.33 ; sentence/s 298 ; words/s 17151 ; accuracy train : 87.81009674072266\n",
      "255936 ; loss 0.32 ; sentence/s 285 ; words/s 16315 ; accuracy train : 87.82539367675781\n",
      "262336 ; loss 0.33 ; sentence/s 294 ; words/s 17067 ; accuracy train : 87.82088470458984\n",
      "268736 ; loss 0.34 ; sentence/s 296 ; words/s 16952 ; accuracy train : 87.8125\n",
      "275136 ; loss 0.32 ; sentence/s 295 ; words/s 17043 ; accuracy train : 87.81903839111328\n",
      "281536 ; loss 0.32 ; sentence/s 303 ; words/s 17617 ; accuracy train : 87.81853485107422\n",
      "287936 ; loss 0.34 ; sentence/s 291 ; words/s 17354 ; accuracy train : 87.79861450195312\n",
      "294336 ; loss 0.34 ; sentence/s 299 ; words/s 17421 ; accuracy train : 87.79619598388672\n",
      "300736 ; loss 0.34 ; sentence/s 293 ; words/s 16513 ; accuracy train : 87.7958755493164\n",
      "307136 ; loss 0.33 ; sentence/s 296 ; words/s 17011 ; accuracy train : 87.798828125\n",
      "313536 ; loss 0.33 ; sentence/s 299 ; words/s 17198 ; accuracy train : 87.80389404296875\n",
      "319936 ; loss 0.33 ; sentence/s 296 ; words/s 17312 ; accuracy train : 87.79562377929688\n",
      "326336 ; loss 0.34 ; sentence/s 299 ; words/s 17392 ; accuracy train : 87.80514526367188\n",
      "332736 ; loss 0.33 ; sentence/s 299 ; words/s 17294 ; accuracy train : 87.81069946289062\n",
      "339136 ; loss 0.32 ; sentence/s 299 ; words/s 17600 ; accuracy train : 87.80778503417969\n",
      "345536 ; loss 0.35 ; sentence/s 296 ; words/s 17356 ; accuracy train : 87.79340362548828\n",
      "351936 ; loss 0.34 ; sentence/s 303 ; words/s 17447 ; accuracy train : 87.79176330566406\n",
      "358336 ; loss 0.33 ; sentence/s 305 ; words/s 17429 ; accuracy train : 87.79157257080078\n",
      "364736 ; loss 0.33 ; sentence/s 307 ; words/s 17564 ; accuracy train : 87.8004379272461\n",
      "371136 ; loss 0.33 ; sentence/s 303 ; words/s 17545 ; accuracy train : 87.79930114746094\n",
      "377536 ; loss 0.33 ; sentence/s 304 ; words/s 17773 ; accuracy train : 87.79554748535156\n",
      "383936 ; loss 0.33 ; sentence/s 307 ; words/s 17539 ; accuracy train : 87.78567504882812\n",
      "390336 ; loss 0.34 ; sentence/s 300 ; words/s 17681 ; accuracy train : 87.77766418457031\n",
      "396736 ; loss 0.34 ; sentence/s 307 ; words/s 17460 ; accuracy train : 87.7741928100586\n",
      "403136 ; loss 0.34 ; sentence/s 303 ; words/s 17529 ; accuracy train : 87.77232360839844\n",
      "409536 ; loss 0.34 ; sentence/s 307 ; words/s 17363 ; accuracy train : 87.766845703125\n",
      "415936 ; loss 0.33 ; sentence/s 307 ; words/s 17522 ; accuracy train : 87.76995086669922\n",
      "422336 ; loss 0.33 ; sentence/s 306 ; words/s 17541 ; accuracy train : 87.77556610107422\n",
      "428736 ; loss 0.33 ; sentence/s 302 ; words/s 17676 ; accuracy train : 87.77332305908203\n",
      "435136 ; loss 0.34 ; sentence/s 306 ; words/s 17674 ; accuracy train : 87.7679214477539\n",
      "441536 ; loss 0.33 ; sentence/s 302 ; words/s 17691 ; accuracy train : 87.76856994628906\n",
      "447936 ; loss 0.33 ; sentence/s 305 ; words/s 17851 ; accuracy train : 87.76941680908203\n",
      "454336 ; loss 0.33 ; sentence/s 304 ; words/s 17732 ; accuracy train : 87.76760864257812\n",
      "460736 ; loss 0.35 ; sentence/s 301 ; words/s 17818 ; accuracy train : 87.76041412353516\n",
      "467136 ; loss 0.34 ; sentence/s 304 ; words/s 17701 ; accuracy train : 87.75106811523438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473536 ; loss 0.35 ; sentence/s 307 ; words/s 17461 ; accuracy train : 87.73817443847656\n",
      "479936 ; loss 0.32 ; sentence/s 300 ; words/s 17612 ; accuracy train : 87.74500274658203\n",
      "486336 ; loss 0.32 ; sentence/s 306 ; words/s 17595 ; accuracy train : 87.75123596191406\n",
      "492736 ; loss 0.34 ; sentence/s 308 ; words/s 17600 ; accuracy train : 87.75\n",
      "499136 ; loss 0.34 ; sentence/s 300 ; words/s 17862 ; accuracy train : 87.74038696289062\n",
      "505536 ; loss 0.32 ; sentence/s 304 ; words/s 17694 ; accuracy train : 87.75019836425781\n",
      "511936 ; loss 0.34 ; sentence/s 308 ; words/s 17552 ; accuracy train : 87.74043273925781\n",
      "518336 ; loss 0.33 ; sentence/s 301 ; words/s 17814 ; accuracy train : 87.7440185546875\n",
      "524736 ; loss 0.33 ; sentence/s 306 ; words/s 17719 ; accuracy train : 87.74161529541016\n",
      "531136 ; loss 0.32 ; sentence/s 296 ; words/s 17368 ; accuracy train : 87.7454833984375\n",
      "537536 ; loss 0.33 ; sentence/s 308 ; words/s 17838 ; accuracy train : 87.73939514160156\n",
      "543936 ; loss 0.32 ; sentence/s 307 ; words/s 17986 ; accuracy train : 87.75\n",
      "results : epoch 13 ; mean accuracy train : 87.74443817138672\n",
      "\n",
      "VALIDATION : Epoch 13\n",
      "togrep : results : epoch 13 ; mean accuracy valid :              84.12924194335938\n",
      "Shrinking lr by : 5. New lr = 2.8364315894916133e-05\n",
      "\n",
      "TRAINING : Epoch 14\n",
      "Learning rate : 2.8080672735966972e-05\n",
      "6336 ; loss 0.34 ; sentence/s 307 ; words/s 17773 ; accuracy train : 87.3125\n",
      "12736 ; loss 0.34 ; sentence/s 308 ; words/s 17731 ; accuracy train : 87.34375\n",
      "19136 ; loss 0.32 ; sentence/s 309 ; words/s 17862 ; accuracy train : 87.41666412353516\n",
      "25536 ; loss 0.32 ; sentence/s 308 ; words/s 17887 ; accuracy train : 87.7109375\n",
      "31936 ; loss 0.34 ; sentence/s 306 ; words/s 17976 ; accuracy train : 87.640625\n",
      "38336 ; loss 0.34 ; sentence/s 310 ; words/s 17958 ; accuracy train : 87.55989837646484\n",
      "44736 ; loss 0.34 ; sentence/s 307 ; words/s 18000 ; accuracy train : 87.54463958740234\n",
      "51136 ; loss 0.33 ; sentence/s 305 ; words/s 18073 ; accuracy train : 87.513671875\n",
      "57536 ; loss 0.33 ; sentence/s 308 ; words/s 17856 ; accuracy train : 87.55208587646484\n",
      "63936 ; loss 0.33 ; sentence/s 308 ; words/s 17925 ; accuracy train : 87.5765609741211\n",
      "70336 ; loss 0.33 ; sentence/s 309 ; words/s 17791 ; accuracy train : 87.62216186523438\n",
      "76736 ; loss 0.34 ; sentence/s 307 ; words/s 17967 ; accuracy train : 87.63671875\n",
      "83136 ; loss 0.35 ; sentence/s 307 ; words/s 17986 ; accuracy train : 87.62019348144531\n",
      "89536 ; loss 0.33 ; sentence/s 281 ; words/s 16503 ; accuracy train : 87.60267639160156\n",
      "95936 ; loss 0.33 ; sentence/s 273 ; words/s 15618 ; accuracy train : 87.62708282470703\n",
      "102336 ; loss 0.32 ; sentence/s 296 ; words/s 16928 ; accuracy train : 87.671875\n",
      "108736 ; loss 0.32 ; sentence/s 290 ; words/s 17101 ; accuracy train : 87.72518157958984\n",
      "115136 ; loss 0.33 ; sentence/s 293 ; words/s 17215 ; accuracy train : 87.7109375\n",
      "121536 ; loss 0.33 ; sentence/s 289 ; words/s 16794 ; accuracy train : 87.71052551269531\n",
      "127936 ; loss 0.33 ; sentence/s 295 ; words/s 17207 ; accuracy train : 87.72969055175781\n",
      "134336 ; loss 0.34 ; sentence/s 284 ; words/s 16489 ; accuracy train : 87.74032592773438\n",
      "140736 ; loss 0.35 ; sentence/s 294 ; words/s 16786 ; accuracy train : 87.7180404663086\n",
      "147136 ; loss 0.32 ; sentence/s 294 ; words/s 17170 ; accuracy train : 87.7574691772461\n",
      "153536 ; loss 0.33 ; sentence/s 305 ; words/s 17848 ; accuracy train : 87.76822662353516\n",
      "159936 ; loss 0.33 ; sentence/s 306 ; words/s 17952 ; accuracy train : 87.77312469482422\n",
      "166336 ; loss 0.33 ; sentence/s 303 ; words/s 17753 ; accuracy train : 87.76802825927734\n",
      "172736 ; loss 0.32 ; sentence/s 308 ; words/s 17759 ; accuracy train : 87.79918670654297\n",
      "179136 ; loss 0.32 ; sentence/s 305 ; words/s 17459 ; accuracy train : 87.80189514160156\n",
      "185536 ; loss 0.33 ; sentence/s 307 ; words/s 17740 ; accuracy train : 87.81304168701172\n",
      "191936 ; loss 0.33 ; sentence/s 307 ; words/s 17807 ; accuracy train : 87.81198120117188\n",
      "198336 ; loss 0.32 ; sentence/s 305 ; words/s 17382 ; accuracy train : 87.82611083984375\n",
      "204736 ; loss 0.33 ; sentence/s 305 ; words/s 17913 ; accuracy train : 87.8212890625\n",
      "211136 ; loss 0.33 ; sentence/s 310 ; words/s 17445 ; accuracy train : 87.81771087646484\n",
      "217536 ; loss 0.34 ; sentence/s 308 ; words/s 17621 ; accuracy train : 87.81204223632812\n",
      "223936 ; loss 0.34 ; sentence/s 308 ; words/s 17740 ; accuracy train : 87.81071472167969\n",
      "230336 ; loss 0.33 ; sentence/s 304 ; words/s 17914 ; accuracy train : 87.82682037353516\n",
      "236736 ; loss 0.34 ; sentence/s 307 ; words/s 17658 ; accuracy train : 87.8196792602539\n",
      "243136 ; loss 0.33 ; sentence/s 306 ; words/s 17802 ; accuracy train : 87.81208801269531\n",
      "249536 ; loss 0.32 ; sentence/s 303 ; words/s 17941 ; accuracy train : 87.81049346923828\n",
      "255936 ; loss 0.32 ; sentence/s 305 ; words/s 17580 ; accuracy train : 87.8121109008789\n",
      "262336 ; loss 0.33 ; sentence/s 308 ; words/s 17502 ; accuracy train : 87.80906677246094\n",
      "268736 ; loss 0.33 ; sentence/s 307 ; words/s 17538 ; accuracy train : 87.80841064453125\n",
      "275136 ; loss 0.33 ; sentence/s 304 ; words/s 17658 ; accuracy train : 87.8128662109375\n",
      "281536 ; loss 0.34 ; sentence/s 307 ; words/s 17639 ; accuracy train : 87.8000717163086\n",
      "287936 ; loss 0.32 ; sentence/s 307 ; words/s 17752 ; accuracy train : 87.81214904785156\n",
      "294336 ; loss 0.33 ; sentence/s 304 ; words/s 17475 ; accuracy train : 87.81420135498047\n",
      "300736 ; loss 0.33 ; sentence/s 304 ; words/s 18053 ; accuracy train : 87.81083679199219\n",
      "307136 ; loss 0.34 ; sentence/s 305 ; words/s 17857 ; accuracy train : 87.802734375\n",
      "313536 ; loss 0.35 ; sentence/s 306 ; words/s 17485 ; accuracy train : 87.78252410888672\n",
      "319936 ; loss 0.32 ; sentence/s 305 ; words/s 17756 ; accuracy train : 87.80155944824219\n",
      "326336 ; loss 0.33 ; sentence/s 302 ; words/s 17903 ; accuracy train : 87.79901885986328\n",
      "332736 ; loss 0.34 ; sentence/s 309 ; words/s 17637 ; accuracy train : 87.79988098144531\n",
      "339136 ; loss 0.33 ; sentence/s 304 ; words/s 18055 ; accuracy train : 87.8004150390625\n",
      "345536 ; loss 0.34 ; sentence/s 303 ; words/s 17842 ; accuracy train : 87.79195404052734\n",
      "351936 ; loss 0.33 ; sentence/s 303 ; words/s 17907 ; accuracy train : 87.8039779663086\n",
      "358336 ; loss 0.33 ; sentence/s 304 ; words/s 17876 ; accuracy train : 87.80692291259766\n",
      "364736 ; loss 0.34 ; sentence/s 297 ; words/s 17171 ; accuracy train : 87.80317687988281\n",
      "371136 ; loss 0.35 ; sentence/s 294 ; words/s 16833 ; accuracy train : 87.79121398925781\n",
      "377536 ; loss 0.34 ; sentence/s 294 ; words/s 17388 ; accuracy train : 87.7828369140625\n",
      "383936 ; loss 0.33 ; sentence/s 298 ; words/s 17449 ; accuracy train : 87.78020477294922\n",
      "390336 ; loss 0.33 ; sentence/s 303 ; words/s 17522 ; accuracy train : 87.7817611694336\n",
      "396736 ; loss 0.33 ; sentence/s 307 ; words/s 17807 ; accuracy train : 87.78326416015625\n",
      "403136 ; loss 0.33 ; sentence/s 296 ; words/s 16648 ; accuracy train : 87.78497314453125\n",
      "409536 ; loss 0.32 ; sentence/s 288 ; words/s 17000 ; accuracy train : 87.7890625\n",
      "415936 ; loss 0.33 ; sentence/s 302 ; words/s 17822 ; accuracy train : 87.78990173339844\n",
      "422336 ; loss 0.33 ; sentence/s 305 ; words/s 17155 ; accuracy train : 87.79142761230469\n",
      "428736 ; loss 0.32 ; sentence/s 303 ; words/s 17629 ; accuracy train : 87.791748046875\n",
      "435136 ; loss 0.34 ; sentence/s 291 ; words/s 17572 ; accuracy train : 87.78515625\n",
      "441536 ; loss 0.34 ; sentence/s 297 ; words/s 17179 ; accuracy train : 87.78510284423828\n",
      "447936 ; loss 0.34 ; sentence/s 300 ; words/s 17099 ; accuracy train : 87.77098083496094\n",
      "454336 ; loss 0.33 ; sentence/s 293 ; words/s 16728 ; accuracy train : 87.76936340332031\n",
      "460736 ; loss 0.33 ; sentence/s 301 ; words/s 17310 ; accuracy train : 87.7734375\n",
      "467136 ; loss 0.34 ; sentence/s 291 ; words/s 17063 ; accuracy train : 87.77140045166016\n",
      "473536 ; loss 0.34 ; sentence/s 294 ; words/s 16989 ; accuracy train : 87.75865936279297\n",
      "479936 ; loss 0.35 ; sentence/s 304 ; words/s 17174 ; accuracy train : 87.75395965576172\n",
      "486336 ; loss 0.33 ; sentence/s 291 ; words/s 17201 ; accuracy train : 87.75657653808594\n",
      "492736 ; loss 0.32 ; sentence/s 298 ; words/s 17480 ; accuracy train : 87.7571029663086\n",
      "499136 ; loss 0.34 ; sentence/s 296 ; words/s 17119 ; accuracy train : 87.75740814208984\n",
      "505536 ; loss 0.33 ; sentence/s 295 ; words/s 16822 ; accuracy train : 87.75336456298828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511936 ; loss 0.34 ; sentence/s 292 ; words/s 17188 ; accuracy train : 87.74589538574219\n",
      "518336 ; loss 0.33 ; sentence/s 299 ; words/s 17178 ; accuracy train : 87.74942016601562\n",
      "524736 ; loss 0.34 ; sentence/s 292 ; words/s 17117 ; accuracy train : 87.74923706054688\n",
      "531136 ; loss 0.32 ; sentence/s 300 ; words/s 17538 ; accuracy train : 87.7513198852539\n",
      "537536 ; loss 0.33 ; sentence/s 292 ; words/s 17169 ; accuracy train : 87.74906921386719\n",
      "543936 ; loss 0.34 ; sentence/s 292 ; words/s 17684 ; accuracy train : 87.74393463134766\n",
      "results : epoch 14 ; mean accuracy train : 87.74552917480469\n",
      "\n",
      "VALIDATION : Epoch 14\n",
      "togrep : results : epoch 14 ; mean accuracy valid :              84.12924194335938\n",
      "Shrinking lr by : 5. New lr = 5.616134547193394e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST : Epoch 15\n",
      "\n",
      "VALIDATION : Epoch 1000000.0\n",
      "finalgrep : accuracy valid : 84.24100494384766\n",
      "finalgrep : accuracy test : 84.41571807861328\n"
     ]
    }
   ],
   "source": [
    "# Run best model on test set.\n",
    "nli_net.load_state_dict(torch.load(os.path.join(params.outputdir, params.outputmodelname)))\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "evaluate(1e6, 'valid', True)\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save encoder instead of full model\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
