{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#or force not to use cuda\n",
    "#use_cuda = False\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../GloVe/glove.840B.300d.txt' if model_version == 1 else '../fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "#model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some sentences\n",
    "#sentences = []\n",
    "#with open('samples.txt') as f:\n",
    "#    for line in f:\n",
    "#        sentences.append(line.strip())\n",
    "#print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu mode : >> 1000 sentences/s\n",
    "# cpu mode : ~100 sentences/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = model.encode(sentences, bsize=128, tokenize=False, verbose=True)\n",
    "#print('nb sentences encoded : {0}'.format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.norm(model.encode(['the cat eats.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cosine(u, v):\n",
    "#    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine(model.encode(['the cat eats.'])[0], model.encode(['the cat drinks.'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = randint(0, len(sentences))\n",
    "#_, _ = model.visualize(sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_sent = 'The cat is drinking milk.'\n",
    "#_, _ = model.visualize(my_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.build_vocab_k_words(500000) # getting 500K words vocab\n",
    "#my_sent = 'barack-obama is the former president of the United-States.'\n",
    "#_, _ = model.visualize(my_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InferSent inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is F0F5-7230\n",
      "\n",
      " Directory of C:\\Users\\ktjam\\YKT\\MComp AI Classes\\CS4248 Natural Language Processing\\Github_project\\4248-project\\src\n",
      "\n",
      "18/03/2023  06:38 pm    <DIR>          .\n",
      "18/03/2023  04:59 pm    <DIR>          ..\n",
      "18/03/2023  06:32 pm    <DIR>          .ipynb_checkpoints\n",
      "18/03/2023  04:50 pm    <DIR>          __pycache__\n",
      "18/03/2023  06:38 pm           123,974 demo_training.ipynb\n",
      "18/03/2023  06:33 pm            10,486 eval_preds.py\n",
      "01/03/2023  03:27 am            10,140 models.py\n",
      "01/03/2023  03:27 am           590,791 samples.txt\n",
      "18/03/2023  04:18 pm             4,395 test.ipynb\n",
      "18/03/2023  04:19 pm           449,448 visualize.ipynb\n",
      "               6 File(s)      1,189,234 bytes\n",
      "               4 Dir(s)  47,275,888,640 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549367 entries, 0 to 549366\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   gold_label  549367 non-null  object\n",
      " 1   Sentence1   549367 non-null  object\n",
      " 2   Sentence2   549361 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tmp1 = pd.read_csv('../dataset/esnli_train_1.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "tmp2 = pd.read_csv('../dataset/esnli_train_2.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "train = pd.concat([tmp1, tmp2], ignore_index=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9842 entries, 0 to 9841\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9842 non-null   object\n",
      " 1   Sentence1   9842 non-null   object\n",
      " 2   Sentence2   9842 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.8+ KB\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('../dataset/esnli_dev.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9824 entries, 0 to 9823\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9824 non-null   object\n",
      " 1   Sentence1   9824 non-null   object\n",
      " 2   Sentence2   9824 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../dataset/esnli_test.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map label to int\n",
    "label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add label int\n",
    "train['label'] = train['gold_label'].apply(lambda x: label_to_int[x])\n",
    "valid['label'] = valid['gold_label'].apply(lambda x: label_to_int[x])\n",
    "test['label'] = test['gold_label'].apply(lambda x: label_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            #break\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in str(sent).split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../GloVe/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts DataFrames to dict\n",
    "train = train.to_dict(orient='list')\n",
    "valid = valid.to_dict(orient='list')\n",
    "test = test.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is training his horse for a competition.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(train['Sentence2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37925(/64300) words with glove vectors\n",
      "Vocab size : 37925\n"
     ]
    }
   ],
   "source": [
    "word_vec = build_vocab(train['Sentence1'] + train['Sentence2'] +\n",
    "                       valid['Sentence1'] + valid['Sentence2'] +\n",
    "                       test['Sentence1'] + test['Sentence2'], glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = model  #eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktjam\\AppData\\Local\\Temp\\ipykernel_32976\\2901695174.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  eval(data_type)[split] = np.array([['<s>'] + \\\n"
     ]
    }
   ],
   "source": [
    "for split in ['Sentence1', 'Sentence2']:\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] + \\\n",
    "            [word for word in str(sent).split() if word in word_vec] + \\\n",
    "            ['</s>'] for sent in eval(data_type)[split]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = np.array(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='../savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "parser.add_argument(\"--word_emb_path\", type=str, default=\"../dataset/GloVe/glove.840B.300d.txt\", help=\"word embedding file path\")\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)  #64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=0, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSentV1', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default=300, help=\"word embedding dimension\")\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "config_nli_model = {\n",
    "    'n_words'        :  len(word_vec)          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    #expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    #assert expected_args[:2] == ['self', 'params']\n",
    "    #if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "    #    raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "    #        str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['Sentence1']))\n",
    "\n",
    "    s1 = train['Sentence1'][permutation]\n",
    "    s2 = train['Sentence2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        #print(type(loss))\n",
    "        all_costs.append(loss.item())  #.data[0])\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm.cpu())\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "        if len(all_costs) == 100:\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)),\n",
    "                            100.*correct/(stidx+k)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = 100 * correct/len(s1)  #round(100 * correct/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch, word_vec, emb_dim=300):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), emb_dim))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['Sentence1'] if eval_type == 'valid' else test['Sentence1']\n",
    "    s2 = valid['Sentence2'] if eval_type == 'valid' else test['Sentence2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "        \n",
    "    # save model\n",
    "    eval_acc = 100 * correct/len(s1)  #round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n",
      "6336 ; loss 1.1 ; sentence/s 330 ; words/s 17361 ; accuracy train : 34.046875\n",
      "12736 ; loss 1.1 ; sentence/s 338 ; words/s 17634 ; accuracy train : 35.25\n",
      "19136 ; loss 1.1 ; sentence/s 336 ; words/s 17780 ; accuracy train : 35.86979293823242\n",
      "25536 ; loss 1.1 ; sentence/s 337 ; words/s 17631 ; accuracy train : 36.4140625\n",
      "31936 ; loss 1.1 ; sentence/s 336 ; words/s 17569 ; accuracy train : 36.724998474121094\n",
      "38336 ; loss 1.1 ; sentence/s 337 ; words/s 17707 ; accuracy train : 36.69791793823242\n",
      "44736 ; loss 1.09 ; sentence/s 335 ; words/s 17772 ; accuracy train : 36.96205520629883\n",
      "51136 ; loss 1.09 ; sentence/s 332 ; words/s 17605 ; accuracy train : 36.935546875\n",
      "57536 ; loss 1.09 ; sentence/s 105 ; words/s 5412 ; accuracy train : 36.80208206176758\n",
      "63936 ; loss 1.09 ; sentence/s 8 ; words/s 423 ; accuracy train : 36.85468673706055\n",
      "70336 ; loss 1.09 ; sentence/s 293 ; words/s 15089 ; accuracy train : 37.041194915771484\n",
      "76736 ; loss 1.09 ; sentence/s 340 ; words/s 17178 ; accuracy train : 37.44010543823242\n",
      "83136 ; loss 1.09 ; sentence/s 333 ; words/s 17179 ; accuracy train : 37.799278259277344\n",
      "89536 ; loss 1.09 ; sentence/s 326 ; words/s 17495 ; accuracy train : 38.21316909790039\n",
      "95936 ; loss 1.09 ; sentence/s 326 ; words/s 17367 ; accuracy train : 38.640625\n",
      "102336 ; loss 1.09 ; sentence/s 329 ; words/s 17778 ; accuracy train : 38.89453125\n",
      "108736 ; loss 1.09 ; sentence/s 326 ; words/s 17156 ; accuracy train : 39.13786697387695\n",
      "115136 ; loss 1.09 ; sentence/s 328 ; words/s 17360 ; accuracy train : 39.52690887451172\n",
      "121536 ; loss 1.09 ; sentence/s 321 ; words/s 17215 ; accuracy train : 39.90049362182617\n",
      "127936 ; loss 1.09 ; sentence/s 339 ; words/s 17476 ; accuracy train : 40.41328048706055\n",
      "134336 ; loss 1.09 ; sentence/s 336 ; words/s 17767 ; accuracy train : 40.85491180419922\n",
      "140736 ; loss 1.09 ; sentence/s 333 ; words/s 17541 ; accuracy train : 41.1875\n",
      "147136 ; loss 1.09 ; sentence/s 333 ; words/s 17268 ; accuracy train : 41.510868072509766\n",
      "153536 ; loss 1.09 ; sentence/s 333 ; words/s 17036 ; accuracy train : 41.88346481323242\n",
      "159936 ; loss 1.09 ; sentence/s 331 ; words/s 17435 ; accuracy train : 42.21687316894531\n",
      "166336 ; loss 1.08 ; sentence/s 335 ; words/s 17393 ; accuracy train : 42.54807662963867\n",
      "172736 ; loss 1.08 ; sentence/s 329 ; words/s 17273 ; accuracy train : 42.90161895751953\n",
      "179136 ; loss 1.08 ; sentence/s 333 ; words/s 17153 ; accuracy train : 43.23716354370117\n",
      "185536 ; loss 1.08 ; sentence/s 335 ; words/s 17485 ; accuracy train : 43.625\n",
      "191936 ; loss 1.08 ; sentence/s 329 ; words/s 17231 ; accuracy train : 43.90625\n",
      "198336 ; loss 1.08 ; sentence/s 323 ; words/s 17203 ; accuracy train : 44.225303649902344\n",
      "204736 ; loss 1.08 ; sentence/s 331 ; words/s 17402 ; accuracy train : 44.5224609375\n",
      "211136 ; loss 1.08 ; sentence/s 328 ; words/s 17156 ; accuracy train : 44.87831497192383\n",
      "217536 ; loss 1.08 ; sentence/s 331 ; words/s 17543 ; accuracy train : 45.211856842041016\n",
      "223936 ; loss 1.08 ; sentence/s 334 ; words/s 17524 ; accuracy train : 45.56562423706055\n",
      "230336 ; loss 1.08 ; sentence/s 331 ; words/s 17650 ; accuracy train : 45.85980987548828\n",
      "236736 ; loss 1.08 ; sentence/s 332 ; words/s 17398 ; accuracy train : 46.14189147949219\n",
      "243136 ; loss 1.08 ; sentence/s 326 ; words/s 16736 ; accuracy train : 46.4140625\n",
      "249536 ; loss 1.08 ; sentence/s 323 ; words/s 16828 ; accuracy train : 46.6205940246582\n",
      "255936 ; loss 1.08 ; sentence/s 329 ; words/s 16770 ; accuracy train : 46.783592224121094\n",
      "262336 ; loss 1.07 ; sentence/s 326 ; words/s 17122 ; accuracy train : 46.99352264404297\n",
      "268736 ; loss 1.07 ; sentence/s 323 ; words/s 17227 ; accuracy train : 47.228050231933594\n",
      "275136 ; loss 1.07 ; sentence/s 325 ; words/s 17665 ; accuracy train : 47.48691940307617\n",
      "281536 ; loss 1.07 ; sentence/s 325 ; words/s 17171 ; accuracy train : 47.72123718261719\n",
      "287936 ; loss 1.07 ; sentence/s 323 ; words/s 17192 ; accuracy train : 47.9243049621582\n",
      "294336 ; loss 1.07 ; sentence/s 323 ; words/s 17155 ; accuracy train : 48.07676696777344\n",
      "300736 ; loss 1.07 ; sentence/s 325 ; words/s 16826 ; accuracy train : 48.24567794799805\n",
      "307136 ; loss 1.07 ; sentence/s 328 ; words/s 17313 ; accuracy train : 48.384765625\n",
      "313536 ; loss 1.07 ; sentence/s 317 ; words/s 16788 ; accuracy train : 48.504783630371094\n",
      "319936 ; loss 1.07 ; sentence/s 326 ; words/s 16756 ; accuracy train : 48.66218566894531\n",
      "326336 ; loss 1.06 ; sentence/s 319 ; words/s 16952 ; accuracy train : 48.81219482421875\n",
      "332736 ; loss 1.06 ; sentence/s 329 ; words/s 17175 ; accuracy train : 48.97926712036133\n",
      "339136 ; loss 1.06 ; sentence/s 317 ; words/s 17195 ; accuracy train : 49.128536224365234\n",
      "345536 ; loss 1.06 ; sentence/s 331 ; words/s 17137 ; accuracy train : 49.271121978759766\n",
      "351936 ; loss 1.06 ; sentence/s 323 ; words/s 16881 ; accuracy train : 49.43465805053711\n",
      "358336 ; loss 1.06 ; sentence/s 330 ; words/s 17146 ; accuracy train : 49.58621597290039\n",
      "364736 ; loss 1.06 ; sentence/s 339 ; words/s 17461 ; accuracy train : 49.720394134521484\n",
      "371136 ; loss 1.05 ; sentence/s 334 ; words/s 17940 ; accuracy train : 49.84402084350586\n",
      "377536 ; loss 1.05 ; sentence/s 338 ; words/s 17320 ; accuracy train : 49.97219467163086\n",
      "383936 ; loss 1.05 ; sentence/s 332 ; words/s 17370 ; accuracy train : 50.08932113647461\n",
      "390336 ; loss 1.05 ; sentence/s 333 ; words/s 16829 ; accuracy train : 50.2215690612793\n",
      "396736 ; loss 1.05 ; sentence/s 328 ; words/s 17212 ; accuracy train : 50.37046432495117\n",
      "403136 ; loss 1.05 ; sentence/s 325 ; words/s 16949 ; accuracy train : 50.52455520629883\n",
      "409536 ; loss 1.05 ; sentence/s 324 ; words/s 17095 ; accuracy train : 50.635498046875\n",
      "415936 ; loss 1.04 ; sentence/s 318 ; words/s 16943 ; accuracy train : 50.755767822265625\n",
      "422336 ; loss 1.04 ; sentence/s 319 ; words/s 16963 ; accuracy train : 50.86458206176758\n",
      "428736 ; loss 1.04 ; sentence/s 321 ; words/s 16868 ; accuracy train : 50.97807693481445\n",
      "435136 ; loss 1.04 ; sentence/s 322 ; words/s 17073 ; accuracy train : 51.07697677612305\n",
      "441536 ; loss 1.04 ; sentence/s 323 ; words/s 17151 ; accuracy train : 51.17980194091797\n",
      "447936 ; loss 1.03 ; sentence/s 321 ; words/s 16920 ; accuracy train : 51.27656173706055\n",
      "454336 ; loss 1.04 ; sentence/s 325 ; words/s 16961 ; accuracy train : 51.36817932128906\n",
      "460736 ; loss 1.03 ; sentence/s 316 ; words/s 16803 ; accuracy train : 51.46657943725586\n",
      "467136 ; loss 1.03 ; sentence/s 319 ; words/s 16760 ; accuracy train : 51.58433151245117\n",
      "473536 ; loss 1.03 ; sentence/s 321 ; words/s 16533 ; accuracy train : 51.6811637878418\n",
      "479936 ; loss 1.02 ; sentence/s 320 ; words/s 16544 ; accuracy train : 51.796043395996094\n",
      "486336 ; loss 1.02 ; sentence/s 321 ; words/s 16684 ; accuracy train : 51.89638137817383\n",
      "492736 ; loss 1.02 ; sentence/s 317 ; words/s 16701 ; accuracy train : 52.001216888427734\n",
      "499136 ; loss 1.01 ; sentence/s 323 ; words/s 16481 ; accuracy train : 52.11057662963867\n",
      "505536 ; loss 1.01 ; sentence/s 317 ; words/s 16898 ; accuracy train : 52.2177619934082\n",
      "511936 ; loss 1.01 ; sentence/s 320 ; words/s 16579 ; accuracy train : 52.3115234375\n",
      "518336 ; loss 1.01 ; sentence/s 317 ; words/s 16681 ; accuracy train : 52.4012336730957\n",
      "524736 ; loss 1.01 ; sentence/s 321 ; words/s 16702 ; accuracy train : 52.47941970825195\n",
      "531136 ; loss 1.0 ; sentence/s 317 ; words/s 16787 ; accuracy train : 52.56381607055664\n",
      "537536 ; loss 1.0 ; sentence/s 314 ; words/s 16555 ; accuracy train : 52.64174270629883\n",
      "543936 ; loss 1.0 ; sentence/s 319 ; words/s 16709 ; accuracy train : 52.73033142089844\n",
      "results : epoch 1 ; mean accuracy train : 52.80022430419922\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              60.77016830444336\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.099\n",
      "6336 ; loss 0.99 ; sentence/s 318 ; words/s 16861 ; accuracy train : 60.34375\n",
      "12736 ; loss 0.99 ; sentence/s 318 ; words/s 16852 ; accuracy train : 60.484375\n",
      "19136 ; loss 0.98 ; sentence/s 318 ; words/s 16465 ; accuracy train : 60.33854293823242\n",
      "25536 ; loss 0.98 ; sentence/s 322 ; words/s 16852 ; accuracy train : 60.19921875\n",
      "31936 ; loss 0.98 ; sentence/s 320 ; words/s 16806 ; accuracy train : 60.296875\n",
      "38336 ; loss 0.98 ; sentence/s 327 ; words/s 17316 ; accuracy train : 60.32291793823242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44736 ; loss 0.97 ; sentence/s 336 ; words/s 17641 ; accuracy train : 60.47768020629883\n",
      "51136 ; loss 0.97 ; sentence/s 333 ; words/s 17941 ; accuracy train : 60.509765625\n",
      "57536 ; loss 0.97 ; sentence/s 334 ; words/s 17684 ; accuracy train : 60.54861068725586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_training \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn_epochs:\n\u001b[1;32m----> 7\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     eval_acc \u001b[38;5;241m=\u001b[39m evaluate(epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[83], line 33\u001b[0m, in \u001b[0;36mtrainepoch\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     30\u001b[0m k \u001b[38;5;241m=\u001b[39m s1_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# actual batch size\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# model forward\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnli_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     36\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39meq(tgt_batch\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\.conda\\envs\\nlp202303\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[75], line 39\u001b[0m, in \u001b[0;36mNLINet.forward\u001b[1;34m(self, s1, s2)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s1, s2):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# s1 : (s1, s1_len)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(s2)\n\u001b[0;32m     42\u001b[0m     features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((u, v, torch\u001b[38;5;241m.\u001b[39mabs(u\u001b[38;5;241m-\u001b[39mv), u\u001b[38;5;241m*\u001b[39mv), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\nlp202303\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\YKT\\MComp AI Classes\\CS4248 Natural Language Processing\\Github_project\\4248-project\\src\\models.py:71\u001b[0m, in \u001b[0;36mInferSent.forward\u001b[1;34m(self, sent_tuple)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Un-sort by length\u001b[39;00m\n\u001b[0;32m     69\u001b[0m idx_unsort \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(idx_unsort)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda() \\\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(idx_unsort)\n\u001b[1;32m---> 71\u001b[0m sent_output \u001b[38;5;241m=\u001b[39m \u001b[43msent_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_unsort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Pooling\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST : Epoch 2\n",
      "\n",
      "VALIDATION : Epoch 1000000.0\n",
      "finalgrep : accuracy valid : 60.77016830444336\n",
      "finalgrep : accuracy test : 60.78990173339844\n"
     ]
    }
   ],
   "source": [
    "# Run best model on test set.\n",
    "nli_net.load_state_dict(torch.load(os.path.join(params.outputdir, params.outputmodelname)))\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "evaluate(1e6, 'valid', True)\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save encoder instead of full model\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
