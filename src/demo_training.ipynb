{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#or force not to use cuda\n",
    "#use_cuda = False\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../GloVe/glove.840B.300d.txt' if model_version == 1 else '../fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "#model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some sentences\n",
    "#sentences = []\n",
    "#with open('samples.txt') as f:\n",
    "#    for line in f:\n",
    "#        sentences.append(line.strip())\n",
    "#print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu mode : >> 1000 sentences/s\n",
    "# cpu mode : ~100 sentences/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = model.encode(sentences, bsize=128, tokenize=False, verbose=True)\n",
    "#print('nb sentences encoded : {0}'.format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.norm(model.encode(['the cat eats.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cosine(u, v):\n",
    "#    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine(model.encode(['the cat eats.'])[0], model.encode(['the cat drinks.'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = randint(0, len(sentences))\n",
    "#_, _ = model.visualize(sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_sent = 'The cat is drinking milk.'\n",
    "#_, _ = model.visualize(my_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.build_vocab_k_words(500000) # getting 500K words vocab\n",
    "#my_sent = 'barack-obama is the former president of the United-States.'\n",
    "#_, _ = model.visualize(my_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InferSent inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is F0F5-7230\n",
      "\n",
      " Directory of C:\\Users\\ktjam\\YKT\\MComp AI Classes\\CS4248 Natural Language Processing\\Github_project\\4248-project\\src\n",
      "\n",
      "18/03/2023  11:33 pm    <DIR>          .\n",
      "18/03/2023  04:59 pm    <DIR>          ..\n",
      "18/03/2023  06:32 pm    <DIR>          .ipynb_checkpoints\n",
      "18/03/2023  04:50 pm    <DIR>          __pycache__\n",
      "18/03/2023  07:26 pm            50,356 demo_training.ipynb\n",
      "18/03/2023  11:33 pm            10,486 eval_preds.py\n",
      "01/03/2023  03:27 am            10,140 models.py\n",
      "01/03/2023  03:27 am           590,791 samples.txt\n",
      "18/03/2023  04:18 pm             4,395 test.ipynb\n",
      "18/03/2023  04:19 pm           449,448 visualize.ipynb\n",
      "               6 File(s)      1,115,616 bytes\n",
      "               4 Dir(s)  46,681,780,224 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549367 entries, 0 to 549366\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   gold_label  549367 non-null  object\n",
      " 1   Sentence1   549367 non-null  object\n",
      " 2   Sentence2   549361 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tmp1 = pd.read_csv('../dataset/esnli_train_1.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "tmp2 = pd.read_csv('../dataset/esnli_train_2.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "train = pd.concat([tmp1, tmp2], ignore_index=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9842 entries, 0 to 9841\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9842 non-null   object\n",
      " 1   Sentence1   9842 non-null   object\n",
      " 2   Sentence2   9842 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.8+ KB\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('../dataset/esnli_dev.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9824 entries, 0 to 9823\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   gold_label  9824 non-null   object\n",
      " 1   Sentence1   9824 non-null   object\n",
      " 2   Sentence2   9824 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 230.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../dataset/esnli_test.csv', usecols=['gold_label', 'Sentence1', 'Sentence2'])\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map label to int\n",
    "label_to_int = {'entailment': 0, 'neutral': 1, 'contradiction': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add label int\n",
    "train['label'] = train['gold_label'].apply(lambda x: label_to_int[x])\n",
    "valid['label'] = valid['gold_label'].apply(lambda x: label_to_int[x])\n",
    "test['label'] = test['gold_label'].apply(lambda x: label_to_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            #break\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in str(sent).split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../GloVe/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts DataFrames to dict\n",
    "train = train.to_dict(orient='list')\n",
    "valid = valid.to_dict(orient='list')\n",
    "test = test.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is training his horse for a competition.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(train['Sentence2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37925(/64300) words with glove vectors\n",
      "Vocab size : 37925\n"
     ]
    }
   ],
   "source": [
    "word_vec = build_vocab(train['Sentence1'] + train['Sentence2'] +\n",
    "                       valid['Sentence1'] + valid['Sentence2'] +\n",
    "                       test['Sentence1'] + test['Sentence2'], glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = model  #eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ktjam\\AppData\\Local\\Temp\\ipykernel_18416\\2901695174.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  eval(data_type)[split] = np.array([['<s>'] + \\\n"
     ]
    }
   ],
   "source": [
    "for split in ['Sentence1', 'Sentence2']:\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] + \\\n",
    "            [word for word in str(sent).split() if word in word_vec] + \\\n",
    "            ['</s>'] for sent in eval(data_type)[split]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = np.array(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='../savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "parser.add_argument(\"--word_emb_path\", type=str, default=\"../dataset/GloVe/glove.840B.300d.txt\", help=\"word embedding file path\")\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)  #64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSentV1', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=3, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default=300, help=\"word embedding dimension\")\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "config_nli_model = {\n",
    "    'n_words'        :  len(word_vec)          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    #expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    #assert expected_args[:2] == ['self', 'params']\n",
    "    #if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "    #    raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "    #        str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['Sentence1']))\n",
    "\n",
    "    s1 = train['Sentence1'][permutation]\n",
    "    s2 = train['Sentence2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        #print(type(loss))\n",
    "        all_costs.append(loss.item())  #.data[0])\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm.cpu())\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "        if len(all_costs) == 100:\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)),\n",
    "                            100.*correct/(stidx+k)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = 100 * correct/len(s1)  #round(100 * correct/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch, word_vec, emb_dim=300):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), emb_dim))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['Sentence1'] if eval_type == 'valid' else test['Sentence1']\n",
    "    s2 = valid['Sentence2'] if eval_type == 'valid' else test['Sentence2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec, params.word_emb_dim)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "        \n",
    "    # save model\n",
    "    eval_acc = 100 * correct/len(s1)  #round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n",
      "6336 ; loss 1.1 ; sentence/s 263 ; words/s 13801 ; accuracy train : 31.953125\n",
      "12736 ; loss 1.1 ; sentence/s 272 ; words/s 14194 ; accuracy train : 32.4140625\n",
      "19136 ; loss 1.1 ; sentence/s 271 ; words/s 14348 ; accuracy train : 33.04166793823242\n",
      "25536 ; loss 1.1 ; sentence/s 273 ; words/s 14259 ; accuracy train : 33.890625\n",
      "31936 ; loss 1.1 ; sentence/s 272 ; words/s 14260 ; accuracy train : 34.243751525878906\n",
      "38336 ; loss 1.1 ; sentence/s 273 ; words/s 14325 ; accuracy train : 34.15104293823242\n",
      "44736 ; loss 1.1 ; sentence/s 270 ; words/s 14316 ; accuracy train : 34.32143020629883\n",
      "51136 ; loss 1.1 ; sentence/s 272 ; words/s 14416 ; accuracy train : 34.345703125\n",
      "57536 ; loss 1.09 ; sentence/s 273 ; words/s 14045 ; accuracy train : 34.31770706176758\n",
      "63936 ; loss 1.09 ; sentence/s 271 ; words/s 14136 ; accuracy train : 34.34062576293945\n",
      "70336 ; loss 1.09 ; sentence/s 271 ; words/s 13987 ; accuracy train : 34.421875\n",
      "76736 ; loss 1.09 ; sentence/s 272 ; words/s 13782 ; accuracy train : 34.62239456176758\n",
      "83136 ; loss 1.09 ; sentence/s 270 ; words/s 13919 ; accuracy train : 34.93149185180664\n",
      "89536 ; loss 1.09 ; sentence/s 268 ; words/s 14365 ; accuracy train : 35.4765625\n",
      "95936 ; loss 1.09 ; sentence/s 267 ; words/s 14227 ; accuracy train : 35.96562576293945\n",
      "102336 ; loss 1.09 ; sentence/s 264 ; words/s 14290 ; accuracy train : 36.1806640625\n",
      "108736 ; loss 1.09 ; sentence/s 271 ; words/s 14236 ; accuracy train : 36.34099197387695\n",
      "115136 ; loss 1.09 ; sentence/s 269 ; words/s 14240 ; accuracy train : 36.72743225097656\n",
      "121536 ; loss 1.09 ; sentence/s 267 ; words/s 14354 ; accuracy train : 37.11266326904297\n",
      "127936 ; loss 1.09 ; sentence/s 273 ; words/s 14071 ; accuracy train : 37.69140625\n",
      "134336 ; loss 1.09 ; sentence/s 268 ; words/s 14149 ; accuracy train : 38.298362731933594\n",
      "140736 ; loss 1.09 ; sentence/s 238 ; words/s 12513 ; accuracy train : 38.724430084228516\n",
      "147136 ; loss 1.09 ; sentence/s 90 ; words/s 4675 ; accuracy train : 39.146060943603516\n",
      "153536 ; loss 1.09 ; sentence/s 268 ; words/s 13715 ; accuracy train : 39.63151168823242\n",
      "159936 ; loss 1.09 ; sentence/s 264 ; words/s 13897 ; accuracy train : 40.029998779296875\n",
      "166336 ; loss 1.09 ; sentence/s 89 ; words/s 4644 ; accuracy train : 40.47055435180664\n",
      "172736 ; loss 1.09 ; sentence/s 172 ; words/s 9044 ; accuracy train : 40.94039535522461\n",
      "179136 ; loss 1.09 ; sentence/s 271 ; words/s 14000 ; accuracy train : 41.33761215209961\n",
      "185536 ; loss 1.08 ; sentence/s 139 ; words/s 7265 ; accuracy train : 41.787715911865234\n",
      "191936 ; loss 1.09 ; sentence/s 86 ; words/s 4552 ; accuracy train : 42.12552261352539\n",
      "198336 ; loss 1.08 ; sentence/s 269 ; words/s 14340 ; accuracy train : 42.53377151489258\n",
      "204736 ; loss 1.08 ; sentence/s 197 ; words/s 10397 ; accuracy train : 42.9296875\n",
      "211136 ; loss 1.08 ; sentence/s 64 ; words/s 3389 ; accuracy train : 43.35464096069336\n",
      "217536 ; loss 1.08 ; sentence/s 257 ; words/s 13641 ; accuracy train : 43.764705657958984\n",
      "223936 ; loss 1.08 ; sentence/s 252 ; words/s 13228 ; accuracy train : 44.18705368041992\n",
      "230336 ; loss 1.08 ; sentence/s 207 ; words/s 11076 ; accuracy train : 44.53949737548828\n",
      "236736 ; loss 1.08 ; sentence/s 316 ; words/s 16570 ; accuracy train : 44.845863342285156\n",
      "243136 ; loss 1.08 ; sentence/s 323 ; words/s 16579 ; accuracy train : 45.16118240356445\n",
      "249536 ; loss 1.08 ; sentence/s 331 ; words/s 17232 ; accuracy train : 45.39743423461914\n",
      "255936 ; loss 1.08 ; sentence/s 323 ; words/s 16457 ; accuracy train : 45.605079650878906\n",
      "262336 ; loss 1.08 ; sentence/s 312 ; words/s 16430 ; accuracy train : 45.839176177978516\n",
      "268736 ; loss 1.08 ; sentence/s 309 ; words/s 16463 ; accuracy train : 46.10676956176758\n",
      "275136 ; loss 1.08 ; sentence/s 317 ; words/s 17216 ; accuracy train : 46.40625\n",
      "281536 ; loss 1.07 ; sentence/s 320 ; words/s 16902 ; accuracy train : 46.67684555053711\n",
      "287936 ; loss 1.07 ; sentence/s 320 ; words/s 17008 ; accuracy train : 46.92916488647461\n",
      "294336 ; loss 1.07 ; sentence/s 330 ; words/s 17501 ; accuracy train : 47.129756927490234\n",
      "300736 ; loss 1.07 ; sentence/s 317 ; words/s 16426 ; accuracy train : 47.32746124267578\n",
      "307136 ; loss 1.07 ; sentence/s 322 ; words/s 16967 ; accuracy train : 47.50260543823242\n",
      "313536 ; loss 1.07 ; sentence/s 312 ; words/s 16502 ; accuracy train : 47.64572525024414\n",
      "319936 ; loss 1.07 ; sentence/s 325 ; words/s 16663 ; accuracy train : 47.834686279296875\n",
      "326336 ; loss 1.07 ; sentence/s 320 ; words/s 16990 ; accuracy train : 48.0030632019043\n",
      "332736 ; loss 1.07 ; sentence/s 315 ; words/s 16465 ; accuracy train : 48.189903259277344\n",
      "339136 ; loss 1.07 ; sentence/s 310 ; words/s 16841 ; accuracy train : 48.34758377075195\n",
      "345536 ; loss 1.07 ; sentence/s 317 ; words/s 16397 ; accuracy train : 48.51041793823242\n",
      "351936 ; loss 1.06 ; sentence/s 329 ; words/s 17199 ; accuracy train : 48.70624923706055\n",
      "358336 ; loss 1.06 ; sentence/s 322 ; words/s 16736 ; accuracy train : 48.87862777709961\n",
      "364736 ; loss 1.06 ; sentence/s 331 ; words/s 17079 ; accuracy train : 49.04166793823242\n",
      "371136 ; loss 1.06 ; sentence/s 331 ; words/s 17795 ; accuracy train : 49.19315719604492\n",
      "377536 ; loss 1.06 ; sentence/s 332 ; words/s 16978 ; accuracy train : 49.34375\n",
      "383936 ; loss 1.06 ; sentence/s 328 ; words/s 17150 ; accuracy train : 49.47473907470703\n",
      "390336 ; loss 1.06 ; sentence/s 332 ; words/s 16770 ; accuracy train : 49.62909698486328\n",
      "396736 ; loss 1.05 ; sentence/s 326 ; words/s 17093 ; accuracy train : 49.79384994506836\n",
      "403136 ; loss 1.05 ; sentence/s 334 ; words/s 17372 ; accuracy train : 49.96775817871094\n",
      "409536 ; loss 1.05 ; sentence/s 333 ; words/s 17532 ; accuracy train : 50.110107421875\n",
      "415936 ; loss 1.05 ; sentence/s 328 ; words/s 17511 ; accuracy train : 50.25552749633789\n",
      "422336 ; loss 1.05 ; sentence/s 323 ; words/s 17178 ; accuracy train : 50.38423156738281\n",
      "428736 ; loss 1.05 ; sentence/s 326 ; words/s 17135 ; accuracy train : 50.511192321777344\n",
      "435136 ; loss 1.05 ; sentence/s 326 ; words/s 17266 ; accuracy train : 50.62867736816406\n",
      "441536 ; loss 1.04 ; sentence/s 317 ; words/s 16835 ; accuracy train : 50.74932098388672\n",
      "447936 ; loss 1.04 ; sentence/s 322 ; words/s 16957 ; accuracy train : 50.85870361328125\n",
      "454336 ; loss 1.04 ; sentence/s 328 ; words/s 17104 ; accuracy train : 50.957305908203125\n",
      "460736 ; loss 1.04 ; sentence/s 326 ; words/s 17357 ; accuracy train : 51.06748962402344\n",
      "467136 ; loss 1.03 ; sentence/s 331 ; words/s 17377 ; accuracy train : 51.19520568847656\n",
      "473536 ; loss 1.03 ; sentence/s 336 ; words/s 17255 ; accuracy train : 51.30658721923828\n",
      "479936 ; loss 1.03 ; sentence/s 331 ; words/s 17091 ; accuracy train : 51.43583297729492\n",
      "486336 ; loss 1.03 ; sentence/s 329 ; words/s 17092 ; accuracy train : 51.54111862182617\n",
      "492736 ; loss 1.03 ; sentence/s 327 ; words/s 17225 ; accuracy train : 51.66091537475586\n",
      "499136 ; loss 1.02 ; sentence/s 336 ; words/s 17121 ; accuracy train : 51.783653259277344\n",
      "505536 ; loss 1.02 ; sentence/s 327 ; words/s 17466 ; accuracy train : 51.90446853637695\n",
      "511936 ; loss 1.02 ; sentence/s 330 ; words/s 17093 ; accuracy train : 52.000587463378906\n",
      "518336 ; loss 1.02 ; sentence/s 330 ; words/s 17363 ; accuracy train : 52.1043586730957\n",
      "524736 ; loss 1.02 ; sentence/s 332 ; words/s 17277 ; accuracy train : 52.19550323486328\n",
      "531136 ; loss 1.01 ; sentence/s 330 ; words/s 17484 ; accuracy train : 52.28840255737305\n",
      "537536 ; loss 1.01 ; sentence/s 331 ; words/s 17444 ; accuracy train : 52.37016296386719\n",
      "543936 ; loss 1.01 ; sentence/s 330 ; words/s 17294 ; accuracy train : 52.462501525878906\n",
      "results : epoch 1 ; mean accuracy train : 52.53664779663086\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              61.054664611816406\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.099\n",
      "6336 ; loss 1.0 ; sentence/s 327 ; words/s 17352 ; accuracy train : 60.765625\n",
      "12736 ; loss 1.0 ; sentence/s 328 ; words/s 17396 ; accuracy train : 60.84375\n",
      "19136 ; loss 0.99 ; sentence/s 324 ; words/s 16772 ; accuracy train : 60.8125\n",
      "25536 ; loss 0.99 ; sentence/s 324 ; words/s 16975 ; accuracy train : 60.64453125\n",
      "31936 ; loss 0.99 ; sentence/s 333 ; words/s 17513 ; accuracy train : 60.743751525878906\n",
      "38336 ; loss 0.99 ; sentence/s 329 ; words/s 17445 ; accuracy train : 60.7734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44736 ; loss 0.98 ; sentence/s 331 ; words/s 17330 ; accuracy train : 60.90625\n",
      "51136 ; loss 0.98 ; sentence/s 329 ; words/s 17732 ; accuracy train : 60.90625\n",
      "57536 ; loss 0.98 ; sentence/s 334 ; words/s 17698 ; accuracy train : 60.89756774902344\n",
      "63936 ; loss 0.97 ; sentence/s 337 ; words/s 17515 ; accuracy train : 60.985939025878906\n",
      "70336 ; loss 0.97 ; sentence/s 333 ; words/s 17527 ; accuracy train : 61.01420593261719\n",
      "76736 ; loss 0.96 ; sentence/s 335 ; words/s 17565 ; accuracy train : 61.01823043823242\n",
      "83136 ; loss 0.96 ; sentence/s 336 ; words/s 17394 ; accuracy train : 61.03725814819336\n",
      "89536 ; loss 0.96 ; sentence/s 333 ; words/s 17340 ; accuracy train : 61.04352569580078\n",
      "95936 ; loss 0.95 ; sentence/s 332 ; words/s 17541 ; accuracy train : 61.11145782470703\n",
      "102336 ; loss 0.95 ; sentence/s 332 ; words/s 17503 ; accuracy train : 61.1435546875\n",
      "108736 ; loss 0.95 ; sentence/s 331 ; words/s 17614 ; accuracy train : 61.15900802612305\n",
      "115136 ; loss 0.95 ; sentence/s 333 ; words/s 17434 ; accuracy train : 61.21006774902344\n",
      "121536 ; loss 0.95 ; sentence/s 335 ; words/s 17282 ; accuracy train : 61.21134948730469\n",
      "127936 ; loss 0.94 ; sentence/s 332 ; words/s 17169 ; accuracy train : 61.23906326293945\n",
      "134336 ; loss 0.93 ; sentence/s 333 ; words/s 17455 ; accuracy train : 61.253719329833984\n",
      "140736 ; loss 0.93 ; sentence/s 334 ; words/s 17362 ; accuracy train : 61.3323860168457\n",
      "147136 ; loss 0.93 ; sentence/s 330 ; words/s 17619 ; accuracy train : 61.402854919433594\n",
      "153536 ; loss 0.93 ; sentence/s 334 ; words/s 17087 ; accuracy train : 61.45182418823242\n",
      "159936 ; loss 0.92 ; sentence/s 334 ; words/s 17537 ; accuracy train : 61.494998931884766\n",
      "166336 ; loss 0.92 ; sentence/s 336 ; words/s 17615 ; accuracy train : 61.53245162963867\n",
      "172736 ; loss 0.91 ; sentence/s 333 ; words/s 17428 ; accuracy train : 61.55786895751953\n",
      "179136 ; loss 0.91 ; sentence/s 333 ; words/s 17445 ; accuracy train : 61.56528854370117\n",
      "185536 ; loss 0.9 ; sentence/s 334 ; words/s 17429 ; accuracy train : 61.63308334350586\n",
      "191936 ; loss 0.9 ; sentence/s 329 ; words/s 17722 ; accuracy train : 61.68489456176758\n",
      "198336 ; loss 0.89 ; sentence/s 335 ; words/s 17550 ; accuracy train : 61.752017974853516\n",
      "204736 ; loss 0.89 ; sentence/s 330 ; words/s 17716 ; accuracy train : 61.8291015625\n",
      "211136 ; loss 0.89 ; sentence/s 332 ; words/s 17548 ; accuracy train : 61.87310791015625\n",
      "217536 ; loss 0.9 ; sentence/s 334 ; words/s 17560 ; accuracy train : 61.88924789428711\n",
      "223936 ; loss 0.89 ; sentence/s 340 ; words/s 17310 ; accuracy train : 61.90401840209961\n",
      "230336 ; loss 0.88 ; sentence/s 336 ; words/s 17284 ; accuracy train : 61.94314193725586\n",
      "236736 ; loss 0.88 ; sentence/s 338 ; words/s 17055 ; accuracy train : 61.96875\n",
      "243136 ; loss 0.88 ; sentence/s 335 ; words/s 17490 ; accuracy train : 61.97697448730469\n",
      "249536 ; loss 0.87 ; sentence/s 331 ; words/s 17672 ; accuracy train : 62.02163314819336\n",
      "255936 ; loss 0.87 ; sentence/s 332 ; words/s 17731 ; accuracy train : 62.04375076293945\n",
      "262336 ; loss 0.87 ; sentence/s 336 ; words/s 17351 ; accuracy train : 62.078887939453125\n",
      "268736 ; loss 0.87 ; sentence/s 333 ; words/s 17467 ; accuracy train : 62.09821319580078\n",
      "275136 ; loss 0.87 ; sentence/s 331 ; words/s 17717 ; accuracy train : 62.12863540649414\n",
      "281536 ; loss 0.86 ; sentence/s 334 ; words/s 17436 ; accuracy train : 62.1640625\n",
      "287936 ; loss 0.85 ; sentence/s 333 ; words/s 17393 ; accuracy train : 62.20833206176758\n",
      "294336 ; loss 0.85 ; sentence/s 328 ; words/s 17661 ; accuracy train : 62.24592208862305\n",
      "300736 ; loss 0.85 ; sentence/s 331 ; words/s 17623 ; accuracy train : 62.29720687866211\n",
      "307136 ; loss 0.85 ; sentence/s 334 ; words/s 17457 ; accuracy train : 62.3212890625\n",
      "313536 ; loss 0.85 ; sentence/s 334 ; words/s 17286 ; accuracy train : 62.36320114135742\n",
      "319936 ; loss 0.85 ; sentence/s 331 ; words/s 17577 ; accuracy train : 62.38874816894531\n",
      "326336 ; loss 0.84 ; sentence/s 333 ; words/s 17248 ; accuracy train : 62.42831039428711\n",
      "332736 ; loss 0.83 ; sentence/s 332 ; words/s 17736 ; accuracy train : 62.464542388916016\n",
      "339136 ; loss 0.82 ; sentence/s 333 ; words/s 17738 ; accuracy train : 62.52623748779297\n",
      "345536 ; loss 0.83 ; sentence/s 339 ; words/s 17359 ; accuracy train : 62.56423568725586\n",
      "351936 ; loss 0.82 ; sentence/s 332 ; words/s 17601 ; accuracy train : 62.61477279663086\n",
      "358336 ; loss 0.82 ; sentence/s 338 ; words/s 17321 ; accuracy train : 62.658203125\n",
      "364736 ; loss 0.83 ; sentence/s 338 ; words/s 17431 ; accuracy train : 62.68256759643555\n",
      "371136 ; loss 0.83 ; sentence/s 339 ; words/s 17292 ; accuracy train : 62.706626892089844\n",
      "377536 ; loss 0.82 ; sentence/s 341 ; words/s 17424 ; accuracy train : 62.75132369995117\n",
      "383936 ; loss 0.82 ; sentence/s 338 ; words/s 17703 ; accuracy train : 62.778907775878906\n",
      "390336 ; loss 0.81 ; sentence/s 332 ; words/s 17847 ; accuracy train : 62.83171081542969\n",
      "396736 ; loss 0.81 ; sentence/s 336 ; words/s 17609 ; accuracy train : 62.88508224487305\n",
      "403136 ; loss 0.8 ; sentence/s 336 ; words/s 17548 ; accuracy train : 62.9382438659668\n",
      "409536 ; loss 0.82 ; sentence/s 334 ; words/s 17645 ; accuracy train : 62.957763671875\n",
      "415936 ; loss 0.81 ; sentence/s 336 ; words/s 17630 ; accuracy train : 63.004085540771484\n",
      "422336 ; loss 0.8 ; sentence/s 332 ; words/s 18026 ; accuracy train : 63.05018997192383\n",
      "428736 ; loss 0.81 ; sentence/s 337 ; words/s 17674 ; accuracy train : 63.08442306518555\n",
      "435136 ; loss 0.8 ; sentence/s 338 ; words/s 17557 ; accuracy train : 63.12890625\n",
      "441536 ; loss 0.8 ; sentence/s 338 ; words/s 17523 ; accuracy train : 63.175498962402344\n",
      "447936 ; loss 0.79 ; sentence/s 335 ; words/s 17548 ; accuracy train : 63.224998474121094\n",
      "454336 ; loss 0.8 ; sentence/s 334 ; words/s 17534 ; accuracy train : 63.26474380493164\n",
      "460736 ; loss 0.79 ; sentence/s 332 ; words/s 17785 ; accuracy train : 63.30360412597656\n",
      "467136 ; loss 0.79 ; sentence/s 335 ; words/s 17718 ; accuracy train : 63.341609954833984\n",
      "473536 ; loss 0.8 ; sentence/s 337 ; words/s 17580 ; accuracy train : 63.3735237121582\n",
      "479936 ; loss 0.78 ; sentence/s 335 ; words/s 17560 ; accuracy train : 63.414791107177734\n",
      "486336 ; loss 0.78 ; sentence/s 335 ; words/s 17748 ; accuracy train : 63.470394134521484\n",
      "492736 ; loss 0.78 ; sentence/s 340 ; words/s 17526 ; accuracy train : 63.51826477050781\n",
      "499136 ; loss 0.79 ; sentence/s 336 ; words/s 17667 ; accuracy train : 63.557891845703125\n",
      "505536 ; loss 0.79 ; sentence/s 335 ; words/s 17598 ; accuracy train : 63.600276947021484\n",
      "511936 ; loss 0.77 ; sentence/s 336 ; words/s 17673 ; accuracy train : 63.63984298706055\n",
      "518336 ; loss 0.78 ; sentence/s 334 ; words/s 17687 ; accuracy train : 63.68885040283203\n",
      "524736 ; loss 0.78 ; sentence/s 341 ; words/s 17175 ; accuracy train : 63.72541809082031\n",
      "531136 ; loss 0.76 ; sentence/s 333 ; words/s 17785 ; accuracy train : 63.77353286743164\n",
      "537536 ; loss 0.75 ; sentence/s 338 ; words/s 17427 ; accuracy train : 63.830169677734375\n",
      "543936 ; loss 0.77 ; sentence/s 336 ; words/s 17805 ; accuracy train : 63.86801528930664\n",
      "results : epoch 2 ; mean accuracy train : 63.90372848510742\n",
      "\n",
      "VALIDATION : Epoch 2\n",
      "togrep : results : epoch 2 ; mean accuracy valid :              67.71997833251953\n",
      "saving model at epoch 2\n",
      "\n",
      "TRAINING : Epoch 3\n",
      "Learning rate : 0.09801\n",
      "6336 ; loss 0.77 ; sentence/s 332 ; words/s 17890 ; accuracy train : 67.375\n",
      "12736 ; loss 0.76 ; sentence/s 338 ; words/s 17358 ; accuracy train : 67.625\n",
      "19136 ; loss 0.77 ; sentence/s 335 ; words/s 17555 ; accuracy train : 67.69271087646484\n",
      "25536 ; loss 0.76 ; sentence/s 336 ; words/s 17642 ; accuracy train : 67.5546875\n",
      "31936 ; loss 0.76 ; sentence/s 336 ; words/s 17536 ; accuracy train : 67.59062194824219\n",
      "38336 ; loss 0.77 ; sentence/s 336 ; words/s 17344 ; accuracy train : 67.5\n",
      "44736 ; loss 0.75 ; sentence/s 338 ; words/s 17507 ; accuracy train : 67.55357360839844\n",
      "51136 ; loss 0.76 ; sentence/s 335 ; words/s 17790 ; accuracy train : 67.5234375\n",
      "57536 ; loss 0.76 ; sentence/s 333 ; words/s 17870 ; accuracy train : 67.44965362548828\n",
      "63936 ; loss 0.75 ; sentence/s 336 ; words/s 17623 ; accuracy train : 67.5328140258789\n",
      "70336 ; loss 0.75 ; sentence/s 336 ; words/s 17523 ; accuracy train : 67.5625\n",
      "76736 ; loss 0.75 ; sentence/s 337 ; words/s 17432 ; accuracy train : 67.58724212646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83136 ; loss 0.75 ; sentence/s 339 ; words/s 17557 ; accuracy train : 67.60816955566406\n",
      "89536 ; loss 0.75 ; sentence/s 334 ; words/s 17670 ; accuracy train : 67.6953125\n",
      "95936 ; loss 0.75 ; sentence/s 336 ; words/s 17538 ; accuracy train : 67.73229217529297\n",
      "102336 ; loss 0.75 ; sentence/s 334 ; words/s 17612 ; accuracy train : 67.759765625\n",
      "108736 ; loss 0.75 ; sentence/s 333 ; words/s 17679 ; accuracy train : 67.81433868408203\n",
      "115136 ; loss 0.75 ; sentence/s 332 ; words/s 17553 ; accuracy train : 67.85243225097656\n",
      "121536 ; loss 0.73 ; sentence/s 332 ; words/s 17621 ; accuracy train : 67.9555892944336\n",
      "127936 ; loss 0.73 ; sentence/s 336 ; words/s 17322 ; accuracy train : 67.98359680175781\n",
      "134336 ; loss 0.74 ; sentence/s 338 ; words/s 17532 ; accuracy train : 67.98809814453125\n",
      "140736 ; loss 0.74 ; sentence/s 334 ; words/s 17457 ; accuracy train : 68.0234375\n",
      "147136 ; loss 0.74 ; sentence/s 337 ; words/s 17400 ; accuracy train : 68.06521606445312\n",
      "153536 ; loss 0.74 ; sentence/s 334 ; words/s 17433 ; accuracy train : 68.09439849853516\n",
      "159936 ; loss 0.73 ; sentence/s 334 ; words/s 17705 ; accuracy train : 68.12875366210938\n",
      "166336 ; loss 0.74 ; sentence/s 333 ; words/s 17628 ; accuracy train : 68.15023803710938\n",
      "172736 ; loss 0.74 ; sentence/s 338 ; words/s 17436 ; accuracy train : 68.14178466796875\n",
      "179136 ; loss 0.73 ; sentence/s 333 ; words/s 17698 ; accuracy train : 68.17131805419922\n",
      "185536 ; loss 0.74 ; sentence/s 334 ; words/s 17583 ; accuracy train : 68.203125\n",
      "191936 ; loss 0.72 ; sentence/s 339 ; words/s 17351 ; accuracy train : 68.22551727294922\n",
      "198336 ; loss 0.72 ; sentence/s 331 ; words/s 17926 ; accuracy train : 68.30594635009766\n",
      "204736 ; loss 0.72 ; sentence/s 340 ; words/s 17396 ; accuracy train : 68.37353515625\n",
      "211136 ; loss 0.73 ; sentence/s 333 ; words/s 17732 ; accuracy train : 68.37547302246094\n",
      "217536 ; loss 0.72 ; sentence/s 337 ; words/s 17429 ; accuracy train : 68.4296875\n",
      "223936 ; loss 0.71 ; sentence/s 333 ; words/s 17797 ; accuracy train : 68.4763412475586\n",
      "230336 ; loss 0.73 ; sentence/s 339 ; words/s 17413 ; accuracy train : 68.50564575195312\n",
      "236736 ; loss 0.72 ; sentence/s 342 ; words/s 17488 ; accuracy train : 68.53166961669922\n",
      "243136 ; loss 0.72 ; sentence/s 335 ; words/s 17376 ; accuracy train : 68.56866455078125\n",
      "249536 ; loss 0.72 ; sentence/s 336 ; words/s 17445 ; accuracy train : 68.59254455566406\n",
      "255936 ; loss 0.72 ; sentence/s 332 ; words/s 17598 ; accuracy train : 68.61210632324219\n",
      "262336 ; loss 0.72 ; sentence/s 334 ; words/s 17679 ; accuracy train : 68.63909912109375\n",
      "268736 ; loss 0.71 ; sentence/s 333 ; words/s 17918 ; accuracy train : 68.69158935546875\n",
      "275136 ; loss 0.71 ; sentence/s 334 ; words/s 17712 ; accuracy train : 68.72056579589844\n",
      "281536 ; loss 0.71 ; sentence/s 339 ; words/s 17564 ; accuracy train : 68.7546157836914\n",
      "287936 ; loss 0.71 ; sentence/s 331 ; words/s 17704 ; accuracy train : 68.79826354980469\n",
      "294336 ; loss 0.71 ; sentence/s 331 ; words/s 17725 ; accuracy train : 68.81929016113281\n",
      "300736 ; loss 0.71 ; sentence/s 331 ; words/s 17785 ; accuracy train : 68.8603744506836\n",
      "307136 ; loss 0.7 ; sentence/s 336 ; words/s 17728 ; accuracy train : 68.89322662353516\n",
      "313536 ; loss 0.7 ; sentence/s 333 ; words/s 17585 ; accuracy train : 68.9422836303711\n",
      "319936 ; loss 0.7 ; sentence/s 336 ; words/s 17568 ; accuracy train : 68.97562408447266\n",
      "326336 ; loss 0.71 ; sentence/s 335 ; words/s 17503 ; accuracy train : 68.99478912353516\n",
      "332736 ; loss 0.7 ; sentence/s 336 ; words/s 17435 ; accuracy train : 69.03636169433594\n",
      "339136 ; loss 0.69 ; sentence/s 335 ; words/s 17691 ; accuracy train : 69.07694244384766\n",
      "345536 ; loss 0.69 ; sentence/s 336 ; words/s 17504 ; accuracy train : 69.11458587646484\n",
      "351936 ; loss 0.69 ; sentence/s 334 ; words/s 17378 ; accuracy train : 69.15198516845703\n",
      "358336 ; loss 0.7 ; sentence/s 330 ; words/s 17891 ; accuracy train : 69.17131805419922\n",
      "364736 ; loss 0.69 ; sentence/s 339 ; words/s 17487 ; accuracy train : 69.20504760742188\n",
      "371136 ; loss 0.7 ; sentence/s 339 ; words/s 17307 ; accuracy train : 69.23033142089844\n",
      "377536 ; loss 0.7 ; sentence/s 335 ; words/s 17725 ; accuracy train : 69.25370788574219\n",
      "383936 ; loss 0.7 ; sentence/s 335 ; words/s 17822 ; accuracy train : 69.27734375\n",
      "390336 ; loss 0.7 ; sentence/s 336 ; words/s 17622 ; accuracy train : 69.29764556884766\n",
      "396736 ; loss 0.69 ; sentence/s 332 ; words/s 17786 ; accuracy train : 69.33064270019531\n",
      "403136 ; loss 0.69 ; sentence/s 333 ; words/s 17841 ; accuracy train : 69.36508178710938\n",
      "409536 ; loss 0.69 ; sentence/s 335 ; words/s 17717 ; accuracy train : 69.397705078125\n",
      "415936 ; loss 0.69 ; sentence/s 336 ; words/s 17439 ; accuracy train : 69.41466522216797\n",
      "422336 ; loss 0.69 ; sentence/s 338 ; words/s 17726 ; accuracy train : 69.4382095336914\n",
      "428736 ; loss 0.7 ; sentence/s 337 ; words/s 17493 ; accuracy train : 69.45965576171875\n",
      "435136 ; loss 0.68 ; sentence/s 335 ; words/s 17577 ; accuracy train : 69.4977035522461\n",
      "441536 ; loss 0.69 ; sentence/s 336 ; words/s 17603 ; accuracy train : 69.5190200805664\n",
      "447936 ; loss 0.69 ; sentence/s 335 ; words/s 17516 ; accuracy train : 69.53571319580078\n",
      "454336 ; loss 0.68 ; sentence/s 336 ; words/s 17718 ; accuracy train : 69.564697265625\n",
      "460736 ; loss 0.68 ; sentence/s 337 ; words/s 17768 ; accuracy train : 69.59049224853516\n",
      "467136 ; loss 0.69 ; sentence/s 336 ; words/s 17567 ; accuracy train : 69.61622619628906\n",
      "473536 ; loss 0.67 ; sentence/s 336 ; words/s 17891 ; accuracy train : 69.6423110961914\n",
      "479936 ; loss 0.68 ; sentence/s 335 ; words/s 17662 ; accuracy train : 69.66979217529297\n",
      "486336 ; loss 0.68 ; sentence/s 337 ; words/s 17376 ; accuracy train : 69.69058227539062\n",
      "492736 ; loss 0.67 ; sentence/s 333 ; words/s 17591 ; accuracy train : 69.72930145263672\n",
      "499136 ; loss 0.67 ; sentence/s 335 ; words/s 17573 ; accuracy train : 69.75941467285156\n",
      "505536 ; loss 0.68 ; sentence/s 335 ; words/s 17476 ; accuracy train : 69.78204345703125\n",
      "511936 ; loss 0.67 ; sentence/s 334 ; words/s 17675 ; accuracy train : 69.80741882324219\n",
      "518336 ; loss 0.66 ; sentence/s 337 ; words/s 17651 ; accuracy train : 69.8514633178711\n",
      "524736 ; loss 0.68 ; sentence/s 336 ; words/s 17833 ; accuracy train : 69.87137603759766\n",
      "531136 ; loss 0.67 ; sentence/s 337 ; words/s 17492 ; accuracy train : 69.89570617675781\n",
      "537536 ; loss 0.68 ; sentence/s 338 ; words/s 17394 ; accuracy train : 69.91722106933594\n",
      "543936 ; loss 0.68 ; sentence/s 334 ; words/s 17824 ; accuracy train : 69.93418884277344\n",
      "results : epoch 3 ; mean accuracy train : 69.9574203491211\n",
      "\n",
      "VALIDATION : Epoch 3\n",
      "togrep : results : epoch 3 ; mean accuracy valid :              72.02804565429688\n",
      "saving model at epoch 3\n",
      "\n",
      "TRAINING : Epoch 4\n",
      "Learning rate : 0.0970299\n",
      "6336 ; loss 0.65 ; sentence/s 336 ; words/s 17577 ; accuracy train : 73.0625\n",
      "12736 ; loss 0.68 ; sentence/s 334 ; words/s 17799 ; accuracy train : 72.59375\n",
      "19136 ; loss 0.67 ; sentence/s 333 ; words/s 17598 ; accuracy train : 72.640625\n",
      "25536 ; loss 0.67 ; sentence/s 339 ; words/s 17419 ; accuracy train : 72.45703125\n",
      "31936 ; loss 0.67 ; sentence/s 334 ; words/s 17607 ; accuracy train : 72.4593734741211\n",
      "38336 ; loss 0.66 ; sentence/s 337 ; words/s 17623 ; accuracy train : 72.49739837646484\n",
      "44736 ; loss 0.67 ; sentence/s 330 ; words/s 17844 ; accuracy train : 72.47991180419922\n",
      "51136 ; loss 0.66 ; sentence/s 336 ; words/s 17501 ; accuracy train : 72.39453125\n",
      "57536 ; loss 0.66 ; sentence/s 337 ; words/s 17507 ; accuracy train : 72.41666412353516\n",
      "63936 ; loss 0.65 ; sentence/s 336 ; words/s 17554 ; accuracy train : 72.50312805175781\n",
      "70336 ; loss 0.65 ; sentence/s 338 ; words/s 17477 ; accuracy train : 72.51278686523438\n",
      "76736 ; loss 0.66 ; sentence/s 335 ; words/s 17710 ; accuracy train : 72.51692962646484\n",
      "83136 ; loss 0.66 ; sentence/s 333 ; words/s 17818 ; accuracy train : 72.50720977783203\n",
      "89536 ; loss 0.66 ; sentence/s 335 ; words/s 17696 ; accuracy train : 72.51786041259766\n",
      "95936 ; loss 0.66 ; sentence/s 333 ; words/s 17669 ; accuracy train : 72.53020477294922\n",
      "102336 ; loss 0.65 ; sentence/s 338 ; words/s 17450 ; accuracy train : 72.5634765625\n",
      "108736 ; loss 0.65 ; sentence/s 336 ; words/s 17557 ; accuracy train : 72.63694763183594\n",
      "115136 ; loss 0.65 ; sentence/s 335 ; words/s 17801 ; accuracy train : 72.67534637451172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121536 ; loss 0.67 ; sentence/s 332 ; words/s 17724 ; accuracy train : 72.66447448730469\n",
      "127936 ; loss 0.66 ; sentence/s 336 ; words/s 17456 ; accuracy train : 72.6539077758789\n",
      "134336 ; loss 0.66 ; sentence/s 334 ; words/s 17922 ; accuracy train : 72.64657592773438\n",
      "140736 ; loss 0.66 ; sentence/s 335 ; words/s 17442 ; accuracy train : 72.6555404663086\n",
      "147136 ; loss 0.66 ; sentence/s 340 ; words/s 17459 ; accuracy train : 72.6596450805664\n",
      "153536 ; loss 0.65 ; sentence/s 336 ; words/s 17512 ; accuracy train : 72.68880462646484\n",
      "159936 ; loss 0.66 ; sentence/s 335 ; words/s 17596 ; accuracy train : 72.70437622070312\n",
      "166336 ; loss 0.65 ; sentence/s 336 ; words/s 17479 ; accuracy train : 72.72535705566406\n",
      "172736 ; loss 0.64 ; sentence/s 341 ; words/s 17262 ; accuracy train : 72.76388549804688\n",
      "179136 ; loss 0.63 ; sentence/s 333 ; words/s 17800 ; accuracy train : 72.81082916259766\n",
      "185536 ; loss 0.65 ; sentence/s 336 ; words/s 17608 ; accuracy train : 72.8356704711914\n",
      "191936 ; loss 0.65 ; sentence/s 338 ; words/s 17518 ; accuracy train : 72.83333587646484\n",
      "198336 ; loss 0.63 ; sentence/s 333 ; words/s 17752 ; accuracy train : 72.86289978027344\n",
      "204736 ; loss 0.64 ; sentence/s 332 ; words/s 17801 ; accuracy train : 72.89794921875\n",
      "211136 ; loss 0.64 ; sentence/s 334 ; words/s 17856 ; accuracy train : 72.92045593261719\n",
      "217536 ; loss 0.65 ; sentence/s 331 ; words/s 17802 ; accuracy train : 72.9397964477539\n",
      "223936 ; loss 0.64 ; sentence/s 335 ; words/s 17682 ; accuracy train : 72.95625305175781\n",
      "230336 ; loss 0.65 ; sentence/s 339 ; words/s 17399 ; accuracy train : 72.96744537353516\n",
      "236736 ; loss 0.63 ; sentence/s 336 ; words/s 17750 ; accuracy train : 72.9945068359375\n",
      "243136 ; loss 0.63 ; sentence/s 336 ; words/s 17555 ; accuracy train : 73.0234375\n",
      "249536 ; loss 0.64 ; sentence/s 332 ; words/s 17708 ; accuracy train : 73.0272445678711\n",
      "255936 ; loss 0.65 ; sentence/s 334 ; words/s 17510 ; accuracy train : 73.0374984741211\n",
      "262336 ; loss 0.62 ; sentence/s 334 ; words/s 17584 ; accuracy train : 73.07926940917969\n",
      "268736 ; loss 0.62 ; sentence/s 332 ; words/s 17624 ; accuracy train : 73.12239837646484\n",
      "275136 ; loss 0.63 ; sentence/s 333 ; words/s 17706 ; accuracy train : 73.14971160888672\n",
      "281536 ; loss 0.63 ; sentence/s 336 ; words/s 17544 ; accuracy train : 73.16832733154297\n",
      "287936 ; loss 0.64 ; sentence/s 339 ; words/s 17098 ; accuracy train : 73.17361450195312\n",
      "294336 ; loss 0.64 ; sentence/s 331 ; words/s 17980 ; accuracy train : 73.18682098388672\n",
      "300736 ; loss 0.63 ; sentence/s 339 ; words/s 17590 ; accuracy train : 73.19947052001953\n",
      "307136 ; loss 0.62 ; sentence/s 337 ; words/s 17591 ; accuracy train : 73.21907806396484\n",
      "313536 ; loss 0.64 ; sentence/s 334 ; words/s 17610 ; accuracy train : 73.22831726074219\n",
      "319936 ; loss 0.64 ; sentence/s 339 ; words/s 17552 ; accuracy train : 73.23562622070312\n",
      "326336 ; loss 0.64 ; sentence/s 336 ; words/s 17430 ; accuracy train : 73.23590850830078\n",
      "332736 ; loss 0.61 ; sentence/s 336 ; words/s 17567 ; accuracy train : 73.26351928710938\n",
      "339136 ; loss 0.63 ; sentence/s 338 ; words/s 17434 ; accuracy train : 73.28743743896484\n",
      "345536 ; loss 0.62 ; sentence/s 336 ; words/s 17539 ; accuracy train : 73.30352783203125\n",
      "351936 ; loss 0.62 ; sentence/s 336 ; words/s 17750 ; accuracy train : 73.32386016845703\n",
      "358336 ; loss 0.61 ; sentence/s 334 ; words/s 17703 ; accuracy train : 73.35240173339844\n",
      "364736 ; loss 0.62 ; sentence/s 333 ; words/s 17742 ; accuracy train : 73.37281036376953\n",
      "371136 ; loss 0.63 ; sentence/s 338 ; words/s 17525 ; accuracy train : 73.3752670288086\n",
      "377536 ; loss 0.61 ; sentence/s 333 ; words/s 17682 ; accuracy train : 73.39936065673828\n",
      "383936 ; loss 0.63 ; sentence/s 340 ; words/s 17555 ; accuracy train : 73.40103912353516\n",
      "390336 ; loss 0.62 ; sentence/s 337 ; words/s 17492 ; accuracy train : 73.42264556884766\n",
      "396736 ; loss 0.62 ; sentence/s 339 ; words/s 17431 ; accuracy train : 73.43775177001953\n",
      "403136 ; loss 0.62 ; sentence/s 335 ; words/s 17586 ; accuracy train : 73.45014953613281\n",
      "409536 ; loss 0.62 ; sentence/s 340 ; words/s 17425 ; accuracy train : 73.4697265625\n",
      "415936 ; loss 0.62 ; sentence/s 337 ; words/s 17363 ; accuracy train : 73.4920654296875\n",
      "422336 ; loss 0.63 ; sentence/s 335 ; words/s 17581 ; accuracy train : 73.4992904663086\n",
      "428736 ; loss 0.61 ; sentence/s 333 ; words/s 17769 ; accuracy train : 73.52705383300781\n",
      "435136 ; loss 0.62 ; sentence/s 334 ; words/s 17656 ; accuracy train : 73.5374526977539\n",
      "441536 ; loss 0.62 ; sentence/s 335 ; words/s 17517 ; accuracy train : 73.55457305908203\n",
      "447936 ; loss 0.61 ; sentence/s 335 ; words/s 17656 ; accuracy train : 73.56495666503906\n",
      "454336 ; loss 0.61 ; sentence/s 338 ; words/s 17537 ; accuracy train : 73.58934783935547\n",
      "460736 ; loss 0.61 ; sentence/s 333 ; words/s 17676 ; accuracy train : 73.607421875\n",
      "467136 ; loss 0.63 ; sentence/s 330 ; words/s 17701 ; accuracy train : 73.6147232055664\n",
      "473536 ; loss 0.6 ; sentence/s 342 ; words/s 17333 ; accuracy train : 73.64167785644531\n",
      "479936 ; loss 0.62 ; sentence/s 338 ; words/s 17577 ; accuracy train : 73.6554183959961\n",
      "486336 ; loss 0.6 ; sentence/s 336 ; words/s 17546 ; accuracy train : 73.68688201904297\n",
      "492736 ; loss 0.62 ; sentence/s 336 ; words/s 17589 ; accuracy train : 73.69094848632812\n",
      "499136 ; loss 0.61 ; sentence/s 335 ; words/s 17510 ; accuracy train : 73.70893096923828\n",
      "505536 ; loss 0.62 ; sentence/s 331 ; words/s 18180 ; accuracy train : 73.71954345703125\n",
      "511936 ; loss 0.61 ; sentence/s 336 ; words/s 17557 ; accuracy train : 73.73359680175781\n",
      "518336 ; loss 0.62 ; sentence/s 331 ; words/s 17898 ; accuracy train : 73.74324798583984\n",
      "524736 ; loss 0.61 ; sentence/s 335 ; words/s 17712 ; accuracy train : 73.75590515136719\n",
      "531136 ; loss 0.61 ; sentence/s 334 ; words/s 17818 ; accuracy train : 73.76957702636719\n",
      "537536 ; loss 0.62 ; sentence/s 336 ; words/s 17582 ; accuracy train : 73.77418518066406\n",
      "543936 ; loss 0.6 ; sentence/s 334 ; words/s 17561 ; accuracy train : 73.78474426269531\n",
      "results : epoch 4 ; mean accuracy train : 73.79329681396484\n",
      "\n",
      "VALIDATION : Epoch 4\n",
      "togrep : results : epoch 4 ; mean accuracy valid :              74.7104263305664\n",
      "saving model at epoch 4\n",
      "\n",
      "TRAINING : Epoch 5\n",
      "Learning rate : 0.096059601\n",
      "6336 ; loss 0.6 ; sentence/s 336 ; words/s 17430 ; accuracy train : 75.59375\n",
      "12736 ; loss 0.6 ; sentence/s 337 ; words/s 17652 ; accuracy train : 75.421875\n",
      "19136 ; loss 0.59 ; sentence/s 337 ; words/s 17607 ; accuracy train : 75.78125\n",
      "25536 ; loss 0.59 ; sentence/s 333 ; words/s 17551 ; accuracy train : 75.984375\n",
      "31936 ; loss 0.6 ; sentence/s 336 ; words/s 17463 ; accuracy train : 75.92500305175781\n",
      "38336 ; loss 0.61 ; sentence/s 336 ; words/s 17560 ; accuracy train : 75.78125\n",
      "44736 ; loss 0.61 ; sentence/s 340 ; words/s 17253 ; accuracy train : 75.703125\n",
      "51136 ; loss 0.61 ; sentence/s 332 ; words/s 17596 ; accuracy train : 75.5859375\n",
      "57536 ; loss 0.61 ; sentence/s 335 ; words/s 17827 ; accuracy train : 75.43228912353516\n",
      "63936 ; loss 0.61 ; sentence/s 340 ; words/s 17497 ; accuracy train : 75.41719055175781\n",
      "70336 ; loss 0.6 ; sentence/s 335 ; words/s 17555 ; accuracy train : 75.47016906738281\n",
      "76736 ; loss 0.6 ; sentence/s 337 ; words/s 17834 ; accuracy train : 75.46875\n",
      "83136 ; loss 0.6 ; sentence/s 335 ; words/s 17541 ; accuracy train : 75.45913696289062\n",
      "89536 ; loss 0.59 ; sentence/s 333 ; words/s 17743 ; accuracy train : 75.48883819580078\n",
      "95936 ; loss 0.61 ; sentence/s 335 ; words/s 17735 ; accuracy train : 75.43958282470703\n",
      "102336 ; loss 0.61 ; sentence/s 336 ; words/s 17679 ; accuracy train : 75.3994140625\n",
      "108736 ; loss 0.59 ; sentence/s 334 ; words/s 17597 ; accuracy train : 75.48345947265625\n",
      "115136 ; loss 0.6 ; sentence/s 341 ; words/s 17529 ; accuracy train : 75.48871612548828\n",
      "121536 ; loss 0.59 ; sentence/s 334 ; words/s 17611 ; accuracy train : 75.50493621826172\n",
      "127936 ; loss 0.6 ; sentence/s 332 ; words/s 17639 ; accuracy train : 75.51953125\n",
      "134336 ; loss 0.61 ; sentence/s 336 ; words/s 17510 ; accuracy train : 75.48735046386719\n",
      "140736 ; loss 0.6 ; sentence/s 338 ; words/s 17642 ; accuracy train : 75.49645233154297\n",
      "147136 ; loss 0.59 ; sentence/s 336 ; words/s 17675 ; accuracy train : 75.49048614501953\n",
      "153536 ; loss 0.61 ; sentence/s 337 ; words/s 17732 ; accuracy train : 75.46875\n",
      "159936 ; loss 0.6 ; sentence/s 337 ; words/s 17691 ; accuracy train : 75.46812438964844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166336 ; loss 0.6 ; sentence/s 338 ; words/s 17669 ; accuracy train : 75.46935272216797\n",
      "172736 ; loss 0.6 ; sentence/s 336 ; words/s 17534 ; accuracy train : 75.47164154052734\n",
      "179136 ; loss 0.6 ; sentence/s 335 ; words/s 17793 ; accuracy train : 75.48213958740234\n",
      "185536 ; loss 0.58 ; sentence/s 333 ; words/s 17733 ; accuracy train : 75.4989242553711\n",
      "191936 ; loss 0.58 ; sentence/s 334 ; words/s 17696 ; accuracy train : 75.5218734741211\n",
      "198336 ; loss 0.6 ; sentence/s 332 ; words/s 17967 ; accuracy train : 75.5241928100586\n",
      "204736 ; loss 0.59 ; sentence/s 332 ; words/s 17775 ; accuracy train : 75.54248046875\n",
      "211136 ; loss 0.59 ; sentence/s 334 ; words/s 17624 ; accuracy train : 75.5516128540039\n",
      "217536 ; loss 0.59 ; sentence/s 334 ; words/s 17611 ; accuracy train : 75.56204223632812\n",
      "223936 ; loss 0.58 ; sentence/s 336 ; words/s 17623 ; accuracy train : 75.5999984741211\n",
      "230336 ; loss 0.59 ; sentence/s 335 ; words/s 17527 ; accuracy train : 75.58853912353516\n",
      "236736 ; loss 0.57 ; sentence/s 335 ; words/s 17494 ; accuracy train : 75.62626647949219\n",
      "243136 ; loss 0.58 ; sentence/s 337 ; words/s 17519 ; accuracy train : 75.63938903808594\n",
      "249536 ; loss 0.59 ; sentence/s 336 ; words/s 17733 ; accuracy train : 75.63982391357422\n",
      "255936 ; loss 0.59 ; sentence/s 333 ; words/s 17986 ; accuracy train : 75.65116882324219\n",
      "262336 ; loss 0.6 ; sentence/s 338 ; words/s 17509 ; accuracy train : 75.64557647705078\n",
      "268736 ; loss 0.57 ; sentence/s 336 ; words/s 17650 ; accuracy train : 75.67112731933594\n",
      "275136 ; loss 0.57 ; sentence/s 336 ; words/s 17611 ; accuracy train : 75.6998519897461\n",
      "281536 ; loss 0.59 ; sentence/s 331 ; words/s 17784 ; accuracy train : 75.71342468261719\n",
      "287936 ; loss 0.58 ; sentence/s 337 ; words/s 17634 ; accuracy train : 75.74097442626953\n",
      "294336 ; loss 0.6 ; sentence/s 335 ; words/s 17654 ; accuracy train : 75.74048614501953\n",
      "300736 ; loss 0.59 ; sentence/s 334 ; words/s 17968 ; accuracy train : 75.74568176269531\n",
      "307136 ; loss 0.58 ; sentence/s 337 ; words/s 17513 ; accuracy train : 75.755859375\n",
      "313536 ; loss 0.59 ; sentence/s 335 ; words/s 17866 ; accuracy train : 75.76307678222656\n",
      "319936 ; loss 0.59 ; sentence/s 334 ; words/s 17855 ; accuracy train : 75.77093505859375\n",
      "326336 ; loss 0.58 ; sentence/s 339 ; words/s 17470 ; accuracy train : 75.77849578857422\n",
      "332736 ; loss 0.59 ; sentence/s 335 ; words/s 17695 ; accuracy train : 75.78064727783203\n",
      "339136 ; loss 0.6 ; sentence/s 338 ; words/s 17334 ; accuracy train : 75.77800750732422\n",
      "345536 ; loss 0.59 ; sentence/s 335 ; words/s 17591 ; accuracy train : 75.77632904052734\n",
      "351936 ; loss 0.59 ; sentence/s 336 ; words/s 17434 ; accuracy train : 75.76704406738281\n",
      "358336 ; loss 0.57 ; sentence/s 332 ; words/s 17869 ; accuracy train : 75.78348541259766\n",
      "364736 ; loss 0.58 ; sentence/s 342 ; words/s 17546 ; accuracy train : 75.78426361083984\n",
      "371136 ; loss 0.59 ; sentence/s 337 ; words/s 17418 ; accuracy train : 75.77989959716797\n",
      "377536 ; loss 0.58 ; sentence/s 337 ; words/s 17271 ; accuracy train : 75.78839874267578\n",
      "383936 ; loss 0.59 ; sentence/s 336 ; words/s 17710 ; accuracy train : 75.7914047241211\n",
      "390336 ; loss 0.56 ; sentence/s 333 ; words/s 17804 ; accuracy train : 75.82351684570312\n",
      "396736 ; loss 0.57 ; sentence/s 336 ; words/s 17476 ; accuracy train : 75.83870697021484\n",
      "403136 ; loss 0.57 ; sentence/s 339 ; words/s 17381 ; accuracy train : 75.8449935913086\n",
      "409536 ; loss 0.58 ; sentence/s 332 ; words/s 17815 ; accuracy train : 75.848876953125\n",
      "415936 ; loss 0.58 ; sentence/s 337 ; words/s 17523 ; accuracy train : 75.85047912597656\n",
      "422336 ; loss 0.58 ; sentence/s 334 ; words/s 17478 ; accuracy train : 75.85179901123047\n",
      "428736 ; loss 0.6 ; sentence/s 333 ; words/s 17657 ; accuracy train : 75.84911346435547\n",
      "435136 ; loss 0.58 ; sentence/s 335 ; words/s 17554 ; accuracy train : 75.84857177734375\n",
      "441536 ; loss 0.57 ; sentence/s 334 ; words/s 17655 ; accuracy train : 75.86141204833984\n",
      "447936 ; loss 0.57 ; sentence/s 337 ; words/s 17495 ; accuracy train : 75.87879180908203\n",
      "454336 ; loss 0.57 ; sentence/s 333 ; words/s 17791 ; accuracy train : 75.89480590820312\n",
      "460736 ; loss 0.58 ; sentence/s 339 ; words/s 17390 ; accuracy train : 75.90234375\n",
      "467136 ; loss 0.57 ; sentence/s 333 ; words/s 17722 ; accuracy train : 75.9165267944336\n",
      "473536 ; loss 0.57 ; sentence/s 334 ; words/s 17708 ; accuracy train : 75.92588806152344\n",
      "479936 ; loss 0.57 ; sentence/s 337 ; words/s 17751 ; accuracy train : 75.93708038330078\n",
      "486336 ; loss 0.58 ; sentence/s 333 ; words/s 17586 ; accuracy train : 75.94428253173828\n",
      "492736 ; loss 0.58 ; sentence/s 338 ; words/s 17510 ; accuracy train : 75.94013977050781\n",
      "499136 ; loss 0.58 ; sentence/s 334 ; words/s 17617 ; accuracy train : 75.9493179321289\n",
      "505536 ; loss 0.58 ; sentence/s 337 ; words/s 17784 ; accuracy train : 75.94501495361328\n",
      "511936 ; loss 0.57 ; sentence/s 335 ; words/s 17543 ; accuracy train : 75.9544906616211\n",
      "518336 ; loss 0.57 ; sentence/s 332 ; words/s 17880 ; accuracy train : 75.9668197631836\n",
      "524736 ; loss 0.57 ; sentence/s 330 ; words/s 17841 ; accuracy train : 75.98170471191406\n",
      "531136 ; loss 0.57 ; sentence/s 336 ; words/s 17588 ; accuracy train : 75.99529266357422\n",
      "537536 ; loss 0.56 ; sentence/s 338 ; words/s 17352 ; accuracy train : 76.00725555419922\n",
      "543936 ; loss 0.57 ; sentence/s 334 ; words/s 17654 ; accuracy train : 76.015625\n",
      "results : epoch 5 ; mean accuracy train : 76.0189437866211\n",
      "\n",
      "VALIDATION : Epoch 5\n",
      "togrep : results : epoch 5 ; mean accuracy valid :              76.2243423461914\n",
      "saving model at epoch 5\n",
      "\n",
      "TRAINING : Epoch 6\n",
      "Learning rate : 0.09509900499\n",
      "6336 ; loss 0.56 ; sentence/s 335 ; words/s 17481 ; accuracy train : 77.5\n",
      "12736 ; loss 0.58 ; sentence/s 337 ; words/s 17555 ; accuracy train : 76.6796875\n",
      "19136 ; loss 0.58 ; sentence/s 339 ; words/s 17429 ; accuracy train : 76.71875\n",
      "25536 ; loss 0.55 ; sentence/s 333 ; words/s 17849 ; accuracy train : 77.015625\n",
      "31936 ; loss 0.57 ; sentence/s 335 ; words/s 17753 ; accuracy train : 77.0093765258789\n",
      "38336 ; loss 0.57 ; sentence/s 334 ; words/s 17809 ; accuracy train : 76.92447662353516\n",
      "44736 ; loss 0.57 ; sentence/s 334 ; words/s 17587 ; accuracy train : 76.84821319580078\n",
      "51136 ; loss 0.56 ; sentence/s 335 ; words/s 17536 ; accuracy train : 76.861328125\n",
      "57536 ; loss 0.56 ; sentence/s 335 ; words/s 17773 ; accuracy train : 76.94097137451172\n",
      "63936 ; loss 0.56 ; sentence/s 331 ; words/s 17823 ; accuracy train : 76.9625015258789\n",
      "70336 ; loss 0.56 ; sentence/s 341 ; words/s 17517 ; accuracy train : 76.98295593261719\n",
      "76736 ; loss 0.56 ; sentence/s 337 ; words/s 17536 ; accuracy train : 76.96875\n",
      "83136 ; loss 0.57 ; sentence/s 340 ; words/s 17458 ; accuracy train : 76.94711303710938\n",
      "89536 ; loss 0.56 ; sentence/s 334 ; words/s 17704 ; accuracy train : 76.98102569580078\n",
      "95936 ; loss 0.57 ; sentence/s 336 ; words/s 17587 ; accuracy train : 76.953125\n",
      "102336 ; loss 0.57 ; sentence/s 338 ; words/s 17419 ; accuracy train : 76.9365234375\n",
      "108736 ; loss 0.56 ; sentence/s 337 ; words/s 17576 ; accuracy train : 76.95128631591797\n",
      "115136 ; loss 0.56 ; sentence/s 340 ; words/s 17463 ; accuracy train : 76.95572662353516\n",
      "121536 ; loss 0.56 ; sentence/s 336 ; words/s 17585 ; accuracy train : 77.0\n",
      "127936 ; loss 0.57 ; sentence/s 336 ; words/s 17852 ; accuracy train : 76.97265625\n",
      "134336 ; loss 0.57 ; sentence/s 337 ; words/s 17464 ; accuracy train : 76.97321319580078\n",
      "140736 ; loss 0.55 ; sentence/s 334 ; words/s 17493 ; accuracy train : 77.01136016845703\n",
      "147136 ; loss 0.58 ; sentence/s 330 ; words/s 17865 ; accuracy train : 77.01155090332031\n",
      "153536 ; loss 0.55 ; sentence/s 334 ; words/s 17711 ; accuracy train : 77.046875\n",
      "159936 ; loss 0.56 ; sentence/s 338 ; words/s 17416 ; accuracy train : 77.0512466430664\n",
      "166336 ; loss 0.54 ; sentence/s 337 ; words/s 17682 ; accuracy train : 77.10216522216797\n",
      "172736 ; loss 0.58 ; sentence/s 332 ; words/s 17628 ; accuracy train : 77.06944274902344\n",
      "179136 ; loss 0.57 ; sentence/s 336 ; words/s 17452 ; accuracy train : 77.03850555419922\n",
      "185536 ; loss 0.57 ; sentence/s 334 ; words/s 17685 ; accuracy train : 77.0167007446289\n",
      "191936 ; loss 0.57 ; sentence/s 336 ; words/s 17736 ; accuracy train : 77.01822662353516\n",
      "198336 ; loss 0.56 ; sentence/s 331 ; words/s 17876 ; accuracy train : 77.02570343017578\n",
      "204736 ; loss 0.56 ; sentence/s 337 ; words/s 17511 ; accuracy train : 77.0263671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211136 ; loss 0.55 ; sentence/s 333 ; words/s 17591 ; accuracy train : 77.05255889892578\n",
      "217536 ; loss 0.56 ; sentence/s 339 ; words/s 17471 ; accuracy train : 77.05928039550781\n",
      "223936 ; loss 0.56 ; sentence/s 338 ; words/s 17437 ; accuracy train : 77.06160736083984\n",
      "230336 ; loss 0.56 ; sentence/s 340 ; words/s 17497 ; accuracy train : 77.06163024902344\n",
      "236736 ; loss 0.55 ; sentence/s 331 ; words/s 17807 ; accuracy train : 77.07474517822266\n",
      "243136 ; loss 0.56 ; sentence/s 337 ; words/s 17624 ; accuracy train : 77.08264923095703\n",
      "249536 ; loss 0.55 ; sentence/s 335 ; words/s 17464 ; accuracy train : 77.09214782714844\n",
      "255936 ; loss 0.56 ; sentence/s 338 ; words/s 17534 ; accuracy train : 77.08203125\n",
      "262336 ; loss 0.55 ; sentence/s 341 ; words/s 17328 ; accuracy train : 77.09565734863281\n",
      "268736 ; loss 0.56 ; sentence/s 334 ; words/s 17649 ; accuracy train : 77.09970092773438\n",
      "275136 ; loss 0.55 ; sentence/s 334 ; words/s 17941 ; accuracy train : 77.109375\n",
      "281536 ; loss 0.56 ; sentence/s 335 ; words/s 17721 ; accuracy train : 77.11186218261719\n",
      "287936 ; loss 0.55 ; sentence/s 332 ; words/s 17881 ; accuracy train : 77.12326049804688\n",
      "294336 ; loss 0.56 ; sentence/s 335 ; words/s 17516 ; accuracy train : 77.1328125\n",
      "300736 ; loss 0.56 ; sentence/s 333 ; words/s 17618 ; accuracy train : 77.13929748535156\n",
      "307136 ; loss 0.55 ; sentence/s 338 ; words/s 17730 ; accuracy train : 77.15755462646484\n",
      "313536 ; loss 0.57 ; sentence/s 339 ; words/s 17457 ; accuracy train : 77.15752410888672\n",
      "319936 ; loss 0.56 ; sentence/s 336 ; words/s 17747 ; accuracy train : 77.15312194824219\n",
      "326336 ; loss 0.55 ; sentence/s 334 ; words/s 17714 ; accuracy train : 77.1617660522461\n",
      "332736 ; loss 0.55 ; sentence/s 335 ; words/s 17589 ; accuracy train : 77.16917419433594\n",
      "339136 ; loss 0.55 ; sentence/s 332 ; words/s 17721 ; accuracy train : 77.17894744873047\n",
      "345536 ; loss 0.56 ; sentence/s 335 ; words/s 17579 ; accuracy train : 77.17997741699219\n",
      "351936 ; loss 0.55 ; sentence/s 337 ; words/s 17564 ; accuracy train : 77.19715881347656\n",
      "358336 ; loss 0.57 ; sentence/s 338 ; words/s 17399 ; accuracy train : 77.18861389160156\n",
      "364736 ; loss 0.56 ; sentence/s 336 ; words/s 17626 ; accuracy train : 77.18887329101562\n",
      "371136 ; loss 0.55 ; sentence/s 337 ; words/s 17403 ; accuracy train : 77.19073486328125\n",
      "377536 ; loss 0.55 ; sentence/s 335 ; words/s 17490 ; accuracy train : 77.19518280029297\n",
      "383936 ; loss 0.54 ; sentence/s 333 ; words/s 17732 ; accuracy train : 77.21823120117188\n",
      "390336 ; loss 0.55 ; sentence/s 335 ; words/s 17630 ; accuracy train : 77.22463989257812\n",
      "396736 ; loss 0.56 ; sentence/s 337 ; words/s 17461 ; accuracy train : 77.23135375976562\n",
      "403136 ; loss 0.52 ; sentence/s 339 ; words/s 17454 ; accuracy train : 77.25991821289062\n",
      "409536 ; loss 0.54 ; sentence/s 335 ; words/s 17702 ; accuracy train : 77.26416015625\n",
      "415936 ; loss 0.55 ; sentence/s 338 ; words/s 17620 ; accuracy train : 77.27307891845703\n",
      "422336 ; loss 0.54 ; sentence/s 334 ; words/s 17599 ; accuracy train : 77.28337860107422\n",
      "428736 ; loss 0.55 ; sentence/s 334 ; words/s 17765 ; accuracy train : 77.29524230957031\n",
      "435136 ; loss 0.54 ; sentence/s 334 ; words/s 17665 ; accuracy train : 77.30928039550781\n",
      "441536 ; loss 0.55 ; sentence/s 340 ; words/s 17206 ; accuracy train : 77.31635284423828\n",
      "447936 ; loss 0.55 ; sentence/s 333 ; words/s 17558 ; accuracy train : 77.3294677734375\n",
      "454336 ; loss 0.54 ; sentence/s 336 ; words/s 17460 ; accuracy train : 77.3461685180664\n",
      "460736 ; loss 0.55 ; sentence/s 336 ; words/s 17622 ; accuracy train : 77.34375\n",
      "467136 ; loss 0.55 ; sentence/s 336 ; words/s 17513 ; accuracy train : 77.34781646728516\n",
      "473536 ; loss 0.55 ; sentence/s 336 ; words/s 17737 ; accuracy train : 77.34712982177734\n",
      "479936 ; loss 0.55 ; sentence/s 336 ; words/s 17526 ; accuracy train : 77.35562133789062\n",
      "486336 ; loss 0.53 ; sentence/s 337 ; words/s 17425 ; accuracy train : 77.38178253173828\n",
      "492736 ; loss 0.55 ; sentence/s 332 ; words/s 17693 ; accuracy train : 77.38758087158203\n",
      "499136 ; loss 0.55 ; sentence/s 334 ; words/s 17788 ; accuracy train : 77.38902282714844\n",
      "505536 ; loss 0.54 ; sentence/s 336 ; words/s 17664 ; accuracy train : 77.39913177490234\n",
      "511936 ; loss 0.54 ; sentence/s 333 ; words/s 17791 ; accuracy train : 77.4039077758789\n",
      "518336 ; loss 0.55 ; sentence/s 333 ; words/s 17670 ; accuracy train : 77.40470886230469\n",
      "524736 ; loss 0.55 ; sentence/s 337 ; words/s 17494 ; accuracy train : 77.41006469726562\n",
      "531136 ; loss 0.55 ; sentence/s 335 ; words/s 17586 ; accuracy train : 77.41453552246094\n",
      "537536 ; loss 0.55 ; sentence/s 337 ; words/s 17494 ; accuracy train : 77.41778564453125\n",
      "543936 ; loss 0.54 ; sentence/s 333 ; words/s 17578 ; accuracy train : 77.41930389404297\n",
      "results : epoch 6 ; mean accuracy train : 77.42693328857422\n",
      "\n",
      "VALIDATION : Epoch 6\n",
      "togrep : results : epoch 6 ; mean accuracy valid :              76.31578826904297\n",
      "saving model at epoch 6\n",
      "\n",
      "TRAINING : Epoch 7\n",
      "Learning rate : 0.0941480149401\n",
      "6336 ; loss 0.53 ; sentence/s 334 ; words/s 17635 ; accuracy train : 78.609375\n",
      "12736 ; loss 0.54 ; sentence/s 334 ; words/s 17655 ; accuracy train : 78.375\n",
      "19136 ; loss 0.54 ; sentence/s 338 ; words/s 17452 ; accuracy train : 78.49478912353516\n",
      "25536 ; loss 0.56 ; sentence/s 333 ; words/s 17817 ; accuracy train : 78.22265625\n",
      "31936 ; loss 0.54 ; sentence/s 336 ; words/s 17416 ; accuracy train : 78.19062805175781\n",
      "38336 ; loss 0.53 ; sentence/s 339 ; words/s 17545 ; accuracy train : 78.26041412353516\n",
      "44736 ; loss 0.53 ; sentence/s 335 ; words/s 17760 ; accuracy train : 78.24107360839844\n",
      "51136 ; loss 0.53 ; sentence/s 334 ; words/s 17681 ; accuracy train : 78.23828125\n",
      "57536 ; loss 0.55 ; sentence/s 336 ; words/s 17671 ; accuracy train : 78.21006774902344\n",
      "63936 ; loss 0.54 ; sentence/s 338 ; words/s 17526 ; accuracy train : 78.2328109741211\n",
      "70336 ; loss 0.55 ; sentence/s 333 ; words/s 17744 ; accuracy train : 78.19318389892578\n",
      "76736 ; loss 0.55 ; sentence/s 334 ; words/s 17702 ; accuracy train : 78.16275787353516\n",
      "83136 ; loss 0.54 ; sentence/s 336 ; words/s 17664 ; accuracy train : 78.15264129638672\n",
      "89536 ; loss 0.54 ; sentence/s 337 ; words/s 17574 ; accuracy train : 78.13616180419922\n",
      "95936 ; loss 0.53 ; sentence/s 335 ; words/s 17654 ; accuracy train : 78.15833282470703\n",
      "102336 ; loss 0.53 ; sentence/s 340 ; words/s 17410 ; accuracy train : 78.140625\n",
      "108736 ; loss 0.53 ; sentence/s 339 ; words/s 17492 ; accuracy train : 78.19944763183594\n",
      "115136 ; loss 0.54 ; sentence/s 336 ; words/s 17644 ; accuracy train : 78.18142700195312\n",
      "121536 ; loss 0.55 ; sentence/s 336 ; words/s 17775 ; accuracy train : 78.14391326904297\n",
      "127936 ; loss 0.55 ; sentence/s 336 ; words/s 17732 ; accuracy train : 78.1015625\n",
      "134336 ; loss 0.55 ; sentence/s 330 ; words/s 17899 ; accuracy train : 78.06473541259766\n",
      "140736 ; loss 0.53 ; sentence/s 338 ; words/s 17320 ; accuracy train : 78.09233093261719\n",
      "147136 ; loss 0.53 ; sentence/s 336 ; words/s 17462 ; accuracy train : 78.11548614501953\n",
      "153536 ; loss 0.55 ; sentence/s 341 ; words/s 17332 ; accuracy train : 78.09896087646484\n",
      "159936 ; loss 0.53 ; sentence/s 328 ; words/s 17896 ; accuracy train : 78.12750244140625\n",
      "166336 ; loss 0.53 ; sentence/s 336 ; words/s 17532 ; accuracy train : 78.14423370361328\n",
      "172736 ; loss 0.55 ; sentence/s 333 ; words/s 17703 ; accuracy train : 78.13136291503906\n",
      "179136 ; loss 0.53 ; sentence/s 335 ; words/s 17703 ; accuracy train : 78.15234375\n",
      "185536 ; loss 0.54 ; sentence/s 334 ; words/s 17736 ; accuracy train : 78.15894317626953\n",
      "191936 ; loss 0.54 ; sentence/s 334 ; words/s 17888 ; accuracy train : 78.15312194824219\n",
      "198336 ; loss 0.54 ; sentence/s 333 ; words/s 17741 ; accuracy train : 78.15019989013672\n",
      "204736 ; loss 0.53 ; sentence/s 333 ; words/s 17732 ; accuracy train : 78.16357421875\n",
      "211136 ; loss 0.53 ; sentence/s 340 ; words/s 17453 ; accuracy train : 78.17755889892578\n",
      "217536 ; loss 0.52 ; sentence/s 338 ; words/s 17554 ; accuracy train : 78.19025421142578\n",
      "223936 ; loss 0.54 ; sentence/s 333 ; words/s 17741 ; accuracy train : 78.17544555664062\n",
      "230336 ; loss 0.55 ; sentence/s 335 ; words/s 17760 ; accuracy train : 78.17013549804688\n",
      "236736 ; loss 0.53 ; sentence/s 336 ; words/s 17654 ; accuracy train : 78.18791961669922\n",
      "243136 ; loss 0.54 ; sentence/s 335 ; words/s 17550 ; accuracy train : 78.18914794921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249536 ; loss 0.54 ; sentence/s 338 ; words/s 17616 ; accuracy train : 78.171875\n",
      "255936 ; loss 0.53 ; sentence/s 338 ; words/s 17533 ; accuracy train : 78.1878890991211\n",
      "262336 ; loss 0.54 ; sentence/s 337 ; words/s 17591 ; accuracy train : 78.18978881835938\n",
      "268736 ; loss 0.55 ; sentence/s 335 ; words/s 17777 ; accuracy train : 78.17298889160156\n",
      "275136 ; loss 0.54 ; sentence/s 336 ; words/s 17631 ; accuracy train : 78.17805480957031\n",
      "281536 ; loss 0.53 ; sentence/s 336 ; words/s 17542 ; accuracy train : 78.20134735107422\n",
      "287936 ; loss 0.54 ; sentence/s 334 ; words/s 17728 ; accuracy train : 78.1982650756836\n",
      "294336 ; loss 0.53 ; sentence/s 336 ; words/s 17629 ; accuracy train : 78.21399688720703\n",
      "300736 ; loss 0.54 ; sentence/s 334 ; words/s 17669 ; accuracy train : 78.21210479736328\n",
      "307136 ; loss 0.54 ; sentence/s 337 ; words/s 17420 ; accuracy train : 78.21614837646484\n",
      "313536 ; loss 0.55 ; sentence/s 333 ; words/s 17711 ; accuracy train : 78.20695495605469\n",
      "319936 ; loss 0.54 ; sentence/s 335 ; words/s 17418 ; accuracy train : 78.20406341552734\n",
      "326336 ; loss 0.53 ; sentence/s 334 ; words/s 17507 ; accuracy train : 78.22119903564453\n",
      "332736 ; loss 0.53 ; sentence/s 337 ; words/s 17346 ; accuracy train : 78.22986602783203\n",
      "339136 ; loss 0.53 ; sentence/s 334 ; words/s 17665 ; accuracy train : 78.2308349609375\n",
      "345536 ; loss 0.53 ; sentence/s 340 ; words/s 17240 ; accuracy train : 78.22858428955078\n",
      "351936 ; loss 0.52 ; sentence/s 333 ; words/s 17543 ; accuracy train : 78.2423324584961\n",
      "358336 ; loss 0.53 ; sentence/s 336 ; words/s 17656 ; accuracy train : 78.25557708740234\n",
      "364736 ; loss 0.54 ; sentence/s 336 ; words/s 17605 ; accuracy train : 78.24917602539062\n",
      "371136 ; loss 0.53 ; sentence/s 339 ; words/s 17540 ; accuracy train : 78.2575454711914\n",
      "377536 ; loss 0.54 ; sentence/s 336 ; words/s 17556 ; accuracy train : 78.25926971435547\n",
      "383936 ; loss 0.53 ; sentence/s 336 ; words/s 17681 ; accuracy train : 78.26249694824219\n",
      "390336 ; loss 0.52 ; sentence/s 336 ; words/s 17750 ; accuracy train : 78.2781753540039\n",
      "396736 ; loss 0.53 ; sentence/s 334 ; words/s 17716 ; accuracy train : 78.29208374023438\n",
      "403136 ; loss 0.54 ; sentence/s 333 ; words/s 17901 ; accuracy train : 78.28174591064453\n",
      "409536 ; loss 0.51 ; sentence/s 339 ; words/s 17550 ; accuracy train : 78.300048828125\n",
      "415936 ; loss 0.53 ; sentence/s 338 ; words/s 17571 ; accuracy train : 78.30216217041016\n",
      "422336 ; loss 0.53 ; sentence/s 336 ; words/s 17853 ; accuracy train : 78.3172378540039\n",
      "428736 ; loss 0.53 ; sentence/s 336 ; words/s 17466 ; accuracy train : 78.32485961914062\n",
      "435136 ; loss 0.53 ; sentence/s 335 ; words/s 17635 ; accuracy train : 78.33455657958984\n",
      "441536 ; loss 0.53 ; sentence/s 336 ; words/s 17694 ; accuracy train : 78.33967590332031\n",
      "447936 ; loss 0.54 ; sentence/s 336 ; words/s 17884 ; accuracy train : 78.34107208251953\n",
      "454336 ; loss 0.54 ; sentence/s 331 ; words/s 17981 ; accuracy train : 78.34308624267578\n",
      "460736 ; loss 0.51 ; sentence/s 338 ; words/s 17442 ; accuracy train : 78.36458587646484\n",
      "467136 ; loss 0.53 ; sentence/s 334 ; words/s 17692 ; accuracy train : 78.35916137695312\n",
      "473536 ; loss 0.54 ; sentence/s 334 ; words/s 17601 ; accuracy train : 78.35747528076172\n",
      "479936 ; loss 0.52 ; sentence/s 340 ; words/s 17262 ; accuracy train : 78.3722915649414\n",
      "486336 ; loss 0.53 ; sentence/s 337 ; words/s 17569 ; accuracy train : 78.38116455078125\n",
      "492736 ; loss 0.53 ; sentence/s 340 ; words/s 17516 ; accuracy train : 78.38189697265625\n",
      "499136 ; loss 0.54 ; sentence/s 339 ; words/s 17609 ; accuracy train : 78.37680053710938\n",
      "505536 ; loss 0.52 ; sentence/s 336 ; words/s 17815 ; accuracy train : 78.37638092041016\n",
      "511936 ; loss 0.53 ; sentence/s 341 ; words/s 17615 ; accuracy train : 78.37519836425781\n",
      "518336 ; loss 0.52 ; sentence/s 339 ; words/s 17712 ; accuracy train : 78.37847137451172\n",
      "524736 ; loss 0.53 ; sentence/s 337 ; words/s 17678 ; accuracy train : 78.38224029541016\n",
      "531136 ; loss 0.51 ; sentence/s 336 ; words/s 17901 ; accuracy train : 78.39307403564453\n",
      "537536 ; loss 0.52 ; sentence/s 338 ; words/s 17731 ; accuracy train : 78.39750671386719\n",
      "543936 ; loss 0.53 ; sentence/s 337 ; words/s 17639 ; accuracy train : 78.3961410522461\n",
      "results : epoch 7 ; mean accuracy train : 78.40733337402344\n",
      "\n",
      "VALIDATION : Epoch 7\n",
      "togrep : results : epoch 7 ; mean accuracy valid :              77.15911102294922\n",
      "saving model at epoch 7\n",
      "\n",
      "TRAINING : Epoch 8\n",
      "Learning rate : 0.093206534790699\n",
      "6336 ; loss 0.52 ; sentence/s 335 ; words/s 17540 ; accuracy train : 78.765625\n",
      "12736 ; loss 0.51 ; sentence/s 338 ; words/s 17535 ; accuracy train : 79.1328125\n",
      "19136 ; loss 0.52 ; sentence/s 336 ; words/s 17766 ; accuracy train : 79.11978912353516\n",
      "25536 ; loss 0.54 ; sentence/s 338 ; words/s 17535 ; accuracy train : 78.81640625\n",
      "31936 ; loss 0.52 ; sentence/s 337 ; words/s 17616 ; accuracy train : 78.9124984741211\n",
      "38336 ; loss 0.52 ; sentence/s 334 ; words/s 17855 ; accuracy train : 79.09635162353516\n",
      "44736 ; loss 0.52 ; sentence/s 337 ; words/s 17792 ; accuracy train : 79.078125\n",
      "51136 ; loss 0.52 ; sentence/s 337 ; words/s 17535 ; accuracy train : 79.060546875\n",
      "57536 ; loss 0.53 ; sentence/s 337 ; words/s 17514 ; accuracy train : 78.99131774902344\n",
      "63936 ; loss 0.52 ; sentence/s 343 ; words/s 17485 ; accuracy train : 79.0234375\n",
      "70336 ; loss 0.54 ; sentence/s 338 ; words/s 17731 ; accuracy train : 78.99858093261719\n",
      "76736 ; loss 0.53 ; sentence/s 334 ; words/s 17858 ; accuracy train : 78.97525787353516\n",
      "83136 ; loss 0.53 ; sentence/s 335 ; words/s 17886 ; accuracy train : 78.96154022216797\n",
      "89536 ; loss 0.51 ; sentence/s 335 ; words/s 17828 ; accuracy train : 79.00111389160156\n",
      "95936 ; loss 0.52 ; sentence/s 333 ; words/s 17824 ; accuracy train : 79.03333282470703\n",
      "102336 ; loss 0.52 ; sentence/s 339 ; words/s 17424 ; accuracy train : 79.0263671875\n",
      "108736 ; loss 0.51 ; sentence/s 337 ; words/s 17604 ; accuracy train : 79.06617736816406\n",
      "115136 ; loss 0.51 ; sentence/s 335 ; words/s 17846 ; accuracy train : 79.12152862548828\n",
      "121536 ; loss 0.52 ; sentence/s 332 ; words/s 17964 ; accuracy train : 79.12582397460938\n",
      "127936 ; loss 0.52 ; sentence/s 335 ; words/s 17577 ; accuracy train : 79.14921569824219\n",
      "134336 ; loss 0.51 ; sentence/s 336 ; words/s 17721 ; accuracy train : 79.15178680419922\n",
      "140736 ; loss 0.52 ; sentence/s 336 ; words/s 17549 ; accuracy train : 79.1555404663086\n",
      "147136 ; loss 0.53 ; sentence/s 338 ; words/s 17609 ; accuracy train : 79.1440200805664\n",
      "153536 ; loss 0.53 ; sentence/s 335 ; words/s 17625 ; accuracy train : 79.11588287353516\n",
      "159936 ; loss 0.51 ; sentence/s 335 ; words/s 17628 ; accuracy train : 79.1362533569336\n",
      "166336 ; loss 0.52 ; sentence/s 339 ; words/s 17565 ; accuracy train : 79.15504455566406\n",
      "172736 ; loss 0.51 ; sentence/s 335 ; words/s 17962 ; accuracy train : 79.17245483398438\n",
      "179136 ; loss 0.52 ; sentence/s 337 ; words/s 17974 ; accuracy train : 79.17076110839844\n",
      "185536 ; loss 0.53 ; sentence/s 338 ; words/s 17657 ; accuracy train : 79.16110229492188\n",
      "191936 ; loss 0.52 ; sentence/s 341 ; words/s 17479 ; accuracy train : 79.16041564941406\n",
      "198336 ; loss 0.5 ; sentence/s 338 ; words/s 17713 ; accuracy train : 79.17338562011719\n",
      "204736 ; loss 0.51 ; sentence/s 338 ; words/s 17766 ; accuracy train : 79.20068359375\n",
      "211136 ; loss 0.51 ; sentence/s 341 ; words/s 17493 ; accuracy train : 79.2102279663086\n",
      "217536 ; loss 0.53 ; sentence/s 336 ; words/s 17731 ; accuracy train : 79.2054214477539\n",
      "223936 ; loss 0.52 ; sentence/s 341 ; words/s 17561 ; accuracy train : 79.21160888671875\n",
      "230336 ; loss 0.52 ; sentence/s 339 ; words/s 17469 ; accuracy train : 79.19314575195312\n",
      "236736 ; loss 0.52 ; sentence/s 342 ; words/s 17401 ; accuracy train : 79.18834686279297\n",
      "243136 ; loss 0.52 ; sentence/s 337 ; words/s 17534 ; accuracy train : 79.17886352539062\n",
      "249536 ; loss 0.53 ; sentence/s 337 ; words/s 17637 ; accuracy train : 79.17227935791016\n",
      "255936 ; loss 0.51 ; sentence/s 338 ; words/s 17637 ; accuracy train : 79.1832046508789\n",
      "262336 ; loss 0.51 ; sentence/s 337 ; words/s 17599 ; accuracy train : 79.17606353759766\n",
      "268736 ; loss 0.51 ; sentence/s 333 ; words/s 17843 ; accuracy train : 79.1889877319336\n",
      "275136 ; loss 0.5 ; sentence/s 334 ; words/s 17735 ; accuracy train : 79.20784759521484\n",
      "281536 ; loss 0.51 ; sentence/s 336 ; words/s 17764 ; accuracy train : 79.20454406738281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287936 ; loss 0.52 ; sentence/s 338 ; words/s 17695 ; accuracy train : 79.20972442626953\n",
      "294336 ; loss 0.51 ; sentence/s 333 ; words/s 17932 ; accuracy train : 79.20686340332031\n",
      "300736 ; loss 0.53 ; sentence/s 340 ; words/s 17618 ; accuracy train : 79.19248962402344\n",
      "307136 ; loss 0.52 ; sentence/s 336 ; words/s 17792 ; accuracy train : 79.18326568603516\n",
      "313536 ; loss 0.53 ; sentence/s 339 ; words/s 17559 ; accuracy train : 79.1769790649414\n",
      "319936 ; loss 0.53 ; sentence/s 332 ; words/s 17883 ; accuracy train : 79.15937805175781\n",
      "326336 ; loss 0.53 ; sentence/s 332 ; words/s 17712 ; accuracy train : 79.14920043945312\n",
      "332736 ; loss 0.52 ; sentence/s 331 ; words/s 18383 ; accuracy train : 79.15084075927734\n",
      "339136 ; loss 0.53 ; sentence/s 335 ; words/s 17689 ; accuracy train : 79.14533996582031\n",
      "345536 ; loss 0.53 ; sentence/s 335 ; words/s 17549 ; accuracy train : 79.13194274902344\n",
      "351936 ; loss 0.53 ; sentence/s 337 ; words/s 17604 ; accuracy train : 79.12216186523438\n",
      "358336 ; loss 0.51 ; sentence/s 334 ; words/s 17842 ; accuracy train : 79.12918853759766\n",
      "364736 ; loss 0.51 ; sentence/s 335 ; words/s 17545 ; accuracy train : 79.12637329101562\n",
      "371136 ; loss 0.5 ; sentence/s 339 ; words/s 17420 ; accuracy train : 79.13200378417969\n",
      "377536 ; loss 0.52 ; sentence/s 333 ; words/s 17846 ; accuracy train : 79.13612365722656\n",
      "383936 ; loss 0.51 ; sentence/s 336 ; words/s 17652 ; accuracy train : 79.14036560058594\n",
      "390336 ; loss 0.51 ; sentence/s 339 ; words/s 17275 ; accuracy train : 79.14830780029297\n",
      "396736 ; loss 0.53 ; sentence/s 340 ; words/s 17585 ; accuracy train : 79.13130187988281\n",
      "403136 ; loss 0.52 ; sentence/s 339 ; words/s 17658 ; accuracy train : 79.12773132324219\n",
      "409536 ; loss 0.51 ; sentence/s 335 ; words/s 17677 ; accuracy train : 79.13720703125\n",
      "415936 ; loss 0.51 ; sentence/s 334 ; words/s 17730 ; accuracy train : 79.14398956298828\n",
      "422336 ; loss 0.5 ; sentence/s 337 ; words/s 17529 ; accuracy train : 79.15459442138672\n",
      "428736 ; loss 0.52 ; sentence/s 338 ; words/s 17403 ; accuracy train : 79.15088653564453\n",
      "435136 ; loss 0.52 ; sentence/s 337 ; words/s 17592 ; accuracy train : 79.1507339477539\n",
      "441536 ; loss 0.51 ; sentence/s 332 ; words/s 17858 ; accuracy train : 79.1544418334961\n",
      "447936 ; loss 0.52 ; sentence/s 335 ; words/s 17666 ; accuracy train : 79.15156555175781\n",
      "454336 ; loss 0.5 ; sentence/s 338 ; words/s 17719 ; accuracy train : 79.16219329833984\n",
      "460736 ; loss 0.52 ; sentence/s 336 ; words/s 17595 ; accuracy train : 79.16189575195312\n",
      "467136 ; loss 0.53 ; sentence/s 340 ; words/s 17561 ; accuracy train : 79.15111541748047\n",
      "473536 ; loss 0.52 ; sentence/s 340 ; words/s 17518 ; accuracy train : 79.14759063720703\n",
      "479936 ; loss 0.51 ; sentence/s 336 ; words/s 17665 ; accuracy train : 79.15374755859375\n",
      "486336 ; loss 0.52 ; sentence/s 337 ; words/s 17788 ; accuracy train : 79.14535522460938\n",
      "492736 ; loss 0.52 ; sentence/s 341 ; words/s 17540 ; accuracy train : 79.1465072631836\n",
      "499136 ; loss 0.51 ; sentence/s 341 ; words/s 17698 ; accuracy train : 79.1484375\n",
      "505536 ; loss 0.51 ; sentence/s 338 ; words/s 17533 ; accuracy train : 79.15229797363281\n",
      "511936 ; loss 0.52 ; sentence/s 337 ; words/s 17559 ; accuracy train : 79.1527328491211\n",
      "518336 ; loss 0.5 ; sentence/s 335 ; words/s 17678 ; accuracy train : 79.16107177734375\n",
      "524736 ; loss 0.52 ; sentence/s 337 ; words/s 17697 ; accuracy train : 79.16596984863281\n",
      "531136 ; loss 0.51 ; sentence/s 336 ; words/s 17681 ; accuracy train : 79.17018127441406\n",
      "537536 ; loss 0.5 ; sentence/s 338 ; words/s 17641 ; accuracy train : 79.17652893066406\n",
      "543936 ; loss 0.52 ; sentence/s 339 ; words/s 17584 ; accuracy train : 79.16967010498047\n",
      "results : epoch 8 ; mean accuracy train : 79.16511535644531\n",
      "\n",
      "VALIDATION : Epoch 8\n",
      "togrep : results : epoch 8 ; mean accuracy valid :              76.72221374511719\n",
      "Shrinking lr by : 5. New lr = 0.0186413069581398\n",
      "\n",
      "TRAINING : Epoch 9\n",
      "Learning rate : 0.0184548938885584\n",
      "6336 ; loss 0.49 ; sentence/s 333 ; words/s 17644 ; accuracy train : 79.703125\n",
      "12736 ; loss 0.5 ; sentence/s 338 ; words/s 17475 ; accuracy train : 79.5625\n",
      "19136 ; loss 0.5 ; sentence/s 337 ; words/s 17792 ; accuracy train : 79.76041412353516\n",
      "25536 ; loss 0.5 ; sentence/s 336 ; words/s 17664 ; accuracy train : 79.78515625\n",
      "31936 ; loss 0.51 ; sentence/s 338 ; words/s 17683 ; accuracy train : 79.8343734741211\n",
      "38336 ; loss 0.51 ; sentence/s 338 ; words/s 17779 ; accuracy train : 79.75\n",
      "44736 ; loss 0.5 ; sentence/s 335 ; words/s 17765 ; accuracy train : 79.84375\n",
      "51136 ; loss 0.5 ; sentence/s 340 ; words/s 17639 ; accuracy train : 79.837890625\n",
      "57536 ; loss 0.51 ; sentence/s 338 ; words/s 17472 ; accuracy train : 79.83680725097656\n",
      "63936 ; loss 0.5 ; sentence/s 336 ; words/s 17805 ; accuracy train : 79.88594055175781\n",
      "70336 ; loss 0.5 ; sentence/s 338 ; words/s 17695 ; accuracy train : 79.97301483154297\n",
      "76736 ; loss 0.51 ; sentence/s 339 ; words/s 17698 ; accuracy train : 79.9609375\n",
      "83136 ; loss 0.51 ; sentence/s 335 ; words/s 17800 ; accuracy train : 79.94711303710938\n",
      "89536 ; loss 0.5 ; sentence/s 333 ; words/s 18184 ; accuracy train : 79.91963958740234\n",
      "95936 ; loss 0.51 ; sentence/s 336 ; words/s 17776 ; accuracy train : 79.89895629882812\n",
      "102336 ; loss 0.49 ; sentence/s 338 ; words/s 17721 ; accuracy train : 79.93359375\n",
      "108736 ; loss 0.51 ; sentence/s 339 ; words/s 17615 ; accuracy train : 79.92646789550781\n",
      "115136 ; loss 0.51 ; sentence/s 342 ; words/s 17447 ; accuracy train : 79.88802337646484\n",
      "121536 ; loss 0.5 ; sentence/s 338 ; words/s 17743 ; accuracy train : 79.9004898071289\n",
      "127936 ; loss 0.52 ; sentence/s 338 ; words/s 17582 ; accuracy train : 79.86640930175781\n",
      "134336 ; loss 0.51 ; sentence/s 340 ; words/s 17516 ; accuracy train : 79.83110046386719\n",
      "140736 ; loss 0.49 ; sentence/s 333 ; words/s 17828 ; accuracy train : 79.86363983154297\n",
      "147136 ; loss 0.51 ; sentence/s 339 ; words/s 17405 ; accuracy train : 79.85801696777344\n",
      "153536 ; loss 0.49 ; sentence/s 337 ; words/s 17578 ; accuracy train : 79.87955474853516\n",
      "159936 ; loss 0.49 ; sentence/s 333 ; words/s 17612 ; accuracy train : 79.88749694824219\n",
      "166336 ; loss 0.51 ; sentence/s 335 ; words/s 17960 ; accuracy train : 79.86658477783203\n",
      "172736 ; loss 0.49 ; sentence/s 338 ; words/s 17601 ; accuracy train : 79.89236450195312\n",
      "179136 ; loss 0.5 ; sentence/s 335 ; words/s 17746 ; accuracy train : 79.88616180419922\n",
      "185536 ; loss 0.5 ; sentence/s 339 ; words/s 17454 ; accuracy train : 79.89439392089844\n",
      "191936 ; loss 0.51 ; sentence/s 337 ; words/s 17857 ; accuracy train : 79.8765640258789\n",
      "198336 ; loss 0.52 ; sentence/s 335 ; words/s 17751 ; accuracy train : 79.84375\n",
      "204736 ; loss 0.5 ; sentence/s 338 ; words/s 17529 ; accuracy train : 79.849609375\n",
      "211136 ; loss 0.5 ; sentence/s 340 ; words/s 17566 ; accuracy train : 79.84896087646484\n",
      "217536 ; loss 0.52 ; sentence/s 340 ; words/s 17432 ; accuracy train : 79.81755828857422\n",
      "223936 ; loss 0.51 ; sentence/s 338 ; words/s 17963 ; accuracy train : 79.81160736083984\n",
      "230336 ; loss 0.49 ; sentence/s 335 ; words/s 17817 ; accuracy train : 79.82508850097656\n",
      "236736 ; loss 0.51 ; sentence/s 336 ; words/s 17842 ; accuracy train : 79.82601165771484\n",
      "243136 ; loss 0.5 ; sentence/s 334 ; words/s 17743 ; accuracy train : 79.8046875\n",
      "249536 ; loss 0.51 ; sentence/s 335 ; words/s 17669 ; accuracy train : 79.80648803710938\n",
      "255936 ; loss 0.51 ; sentence/s 336 ; words/s 17514 ; accuracy train : 79.8031234741211\n",
      "262336 ; loss 0.51 ; sentence/s 334 ; words/s 17559 ; accuracy train : 79.80525970458984\n",
      "268736 ; loss 0.51 ; sentence/s 338 ; words/s 17522 ; accuracy train : 79.80692291259766\n",
      "275136 ; loss 0.5 ; sentence/s 338 ; words/s 17663 ; accuracy train : 79.796875\n",
      "281536 ; loss 0.5 ; sentence/s 336 ; words/s 17536 ; accuracy train : 79.80859375\n",
      "287936 ; loss 0.49 ; sentence/s 336 ; words/s 17665 ; accuracy train : 79.84131622314453\n",
      "294336 ; loss 0.51 ; sentence/s 338 ; words/s 17559 ; accuracy train : 79.83423614501953\n",
      "300736 ; loss 0.51 ; sentence/s 336 ; words/s 17751 ; accuracy train : 79.8291244506836\n",
      "307136 ; loss 0.5 ; sentence/s 339 ; words/s 17462 ; accuracy train : 79.81380462646484\n",
      "313536 ; loss 0.5 ; sentence/s 340 ; words/s 17592 ; accuracy train : 79.8182373046875\n",
      "319936 ; loss 0.5 ; sentence/s 339 ; words/s 17643 ; accuracy train : 79.81500244140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326336 ; loss 0.51 ; sentence/s 337 ; words/s 17378 ; accuracy train : 79.79901885986328\n",
      "332736 ; loss 0.5 ; sentence/s 335 ; words/s 17839 ; accuracy train : 79.80198669433594\n",
      "339136 ; loss 0.51 ; sentence/s 336 ; words/s 17795 ; accuracy train : 79.796875\n",
      "345536 ; loss 0.5 ; sentence/s 335 ; words/s 17846 ; accuracy train : 79.80034637451172\n",
      "351936 ; loss 0.5 ; sentence/s 337 ; words/s 17656 ; accuracy train : 79.81108093261719\n",
      "358336 ; loss 0.52 ; sentence/s 335 ; words/s 17847 ; accuracy train : 79.80078125\n",
      "364736 ; loss 0.51 ; sentence/s 341 ; words/s 17568 ; accuracy train : 79.79988861083984\n",
      "371136 ; loss 0.5 ; sentence/s 340 ; words/s 17557 ; accuracy train : 79.80899810791016\n",
      "377536 ; loss 0.51 ; sentence/s 335 ; words/s 17846 ; accuracy train : 79.7971420288086\n",
      "383936 ; loss 0.49 ; sentence/s 335 ; words/s 17936 ; accuracy train : 79.8031234741211\n",
      "390336 ; loss 0.51 ; sentence/s 337 ; words/s 17379 ; accuracy train : 79.79585266113281\n",
      "396736 ; loss 0.51 ; sentence/s 337 ; words/s 17601 ; accuracy train : 79.79637145996094\n",
      "403136 ; loss 0.49 ; sentence/s 335 ; words/s 17807 ; accuracy train : 79.81523132324219\n",
      "409536 ; loss 0.5 ; sentence/s 333 ; words/s 17763 ; accuracy train : 79.81494140625\n",
      "415936 ; loss 0.51 ; sentence/s 338 ; words/s 17698 ; accuracy train : 79.80648803710938\n",
      "422336 ; loss 0.5 ; sentence/s 337 ; words/s 17393 ; accuracy train : 79.79711151123047\n",
      "428736 ; loss 0.5 ; sentence/s 336 ; words/s 17391 ; accuracy train : 79.802001953125\n",
      "435136 ; loss 0.51 ; sentence/s 338 ; words/s 17400 ; accuracy train : 79.80261993408203\n",
      "441536 ; loss 0.51 ; sentence/s 331 ; words/s 17666 ; accuracy train : 79.80366516113281\n",
      "447936 ; loss 0.52 ; sentence/s 334 ; words/s 17543 ; accuracy train : 79.80000305175781\n",
      "454336 ; loss 0.51 ; sentence/s 336 ; words/s 17848 ; accuracy train : 79.80523681640625\n",
      "460736 ; loss 0.5 ; sentence/s 343 ; words/s 17327 ; accuracy train : 79.80924224853516\n",
      "467136 ; loss 0.51 ; sentence/s 333 ; words/s 18100 ; accuracy train : 79.80928802490234\n",
      "473536 ; loss 0.5 ; sentence/s 334 ; words/s 17924 ; accuracy train : 79.8108139038086\n",
      "479936 ; loss 0.51 ; sentence/s 334 ; words/s 17709 ; accuracy train : 79.81124877929688\n",
      "486336 ; loss 0.5 ; sentence/s 337 ; words/s 17845 ; accuracy train : 79.81661224365234\n",
      "492736 ; loss 0.5 ; sentence/s 338 ; words/s 17526 ; accuracy train : 79.8125\n",
      "499136 ; loss 0.5 ; sentence/s 336 ; words/s 17611 ; accuracy train : 79.81430053710938\n",
      "505536 ; loss 0.51 ; sentence/s 338 ; words/s 17778 ; accuracy train : 79.81230163574219\n",
      "511936 ; loss 0.49 ; sentence/s 339 ; words/s 17669 ; accuracy train : 79.82148742675781\n",
      "518336 ; loss 0.5 ; sentence/s 333 ; words/s 17824 ; accuracy train : 79.82889556884766\n",
      "524736 ; loss 0.51 ; sentence/s 338 ; words/s 17548 ; accuracy train : 79.81993103027344\n",
      "531136 ; loss 0.49 ; sentence/s 338 ; words/s 17588 ; accuracy train : 79.8273696899414\n",
      "537536 ; loss 0.53 ; sentence/s 333 ; words/s 17691 ; accuracy train : 79.8125\n",
      "543936 ; loss 0.49 ; sentence/s 340 ; words/s 17524 ; accuracy train : 79.82022094726562\n",
      "results : epoch 9 ; mean accuracy train : 79.81749725341797\n",
      "\n",
      "VALIDATION : Epoch 9\n",
      "togrep : results : epoch 9 ; mean accuracy valid :              77.6671371459961\n",
      "saving model at epoch 9\n",
      "\n",
      "TRAINING : Epoch 10\n",
      "Learning rate : 0.018270344949672817\n",
      "6336 ; loss 0.49 ; sentence/s 337 ; words/s 17447 ; accuracy train : 80.28125\n",
      "12736 ; loss 0.5 ; sentence/s 337 ; words/s 17609 ; accuracy train : 80.203125\n",
      "19136 ; loss 0.5 ; sentence/s 339 ; words/s 17572 ; accuracy train : 80.03125\n",
      "25536 ; loss 0.51 ; sentence/s 339 ; words/s 17763 ; accuracy train : 79.83984375\n",
      "31936 ; loss 0.5 ; sentence/s 341 ; words/s 17510 ; accuracy train : 79.84062194824219\n",
      "38336 ; loss 0.51 ; sentence/s 336 ; words/s 17737 ; accuracy train : 79.87760162353516\n",
      "44736 ; loss 0.49 ; sentence/s 336 ; words/s 17626 ; accuracy train : 79.93526458740234\n",
      "51136 ; loss 0.5 ; sentence/s 341 ; words/s 17541 ; accuracy train : 79.947265625\n",
      "57536 ; loss 0.5 ; sentence/s 340 ; words/s 17626 ; accuracy train : 80.0\n",
      "63936 ; loss 0.49 ; sentence/s 337 ; words/s 17577 ; accuracy train : 80.0\n",
      "70336 ; loss 0.5 ; sentence/s 334 ; words/s 17813 ; accuracy train : 79.9616470336914\n",
      "76736 ; loss 0.49 ; sentence/s 335 ; words/s 17898 ; accuracy train : 80.00390625\n",
      "83136 ; loss 0.5 ; sentence/s 335 ; words/s 17852 ; accuracy train : 80.05408477783203\n",
      "89536 ; loss 0.49 ; sentence/s 336 ; words/s 17681 ; accuracy train : 80.10044860839844\n",
      "95936 ; loss 0.49 ; sentence/s 340 ; words/s 17595 ; accuracy train : 80.10520935058594\n",
      "102336 ; loss 0.51 ; sentence/s 336 ; words/s 17670 ; accuracy train : 80.0927734375\n",
      "108736 ; loss 0.5 ; sentence/s 340 ; words/s 17614 ; accuracy train : 80.08547973632812\n",
      "115136 ; loss 0.51 ; sentence/s 330 ; words/s 18123 ; accuracy train : 80.03385162353516\n",
      "121536 ; loss 0.52 ; sentence/s 336 ; words/s 17715 ; accuracy train : 79.9901351928711\n",
      "127936 ; loss 0.5 ; sentence/s 336 ; words/s 17602 ; accuracy train : 79.98046875\n",
      "134336 ; loss 0.52 ; sentence/s 337 ; words/s 17628 ; accuracy train : 79.94047546386719\n",
      "140736 ; loss 0.5 ; sentence/s 343 ; words/s 17619 ; accuracy train : 79.9446029663086\n",
      "147136 ; loss 0.49 ; sentence/s 338 ; words/s 17638 ; accuracy train : 79.99320983886719\n",
      "153536 ; loss 0.5 ; sentence/s 336 ; words/s 17721 ; accuracy train : 79.98697662353516\n",
      "159936 ; loss 0.5 ; sentence/s 334 ; words/s 17878 ; accuracy train : 79.98999786376953\n",
      "166336 ; loss 0.49 ; sentence/s 338 ; words/s 17533 ; accuracy train : 80.0\n",
      "172736 ; loss 0.5 ; sentence/s 336 ; words/s 17659 ; accuracy train : 79.98900604248047\n",
      "179136 ; loss 0.5 ; sentence/s 336 ; words/s 17748 ; accuracy train : 79.96038055419922\n",
      "185536 ; loss 0.52 ; sentence/s 334 ; words/s 17763 ; accuracy train : 79.9299545288086\n",
      "191936 ; loss 0.51 ; sentence/s 338 ; words/s 17477 ; accuracy train : 79.93073272705078\n",
      "198336 ; loss 0.49 ; sentence/s 338 ; words/s 17559 ; accuracy train : 79.93800354003906\n",
      "204736 ; loss 0.51 ; sentence/s 339 ; words/s 17268 ; accuracy train : 79.92578125\n",
      "211136 ; loss 0.51 ; sentence/s 337 ; words/s 17399 ; accuracy train : 79.88304901123047\n",
      "217536 ; loss 0.5 ; sentence/s 337 ; words/s 17579 ; accuracy train : 79.88740539550781\n",
      "223936 ; loss 0.49 ; sentence/s 337 ; words/s 17625 ; accuracy train : 79.90089416503906\n",
      "230336 ; loss 0.51 ; sentence/s 333 ; words/s 17950 ; accuracy train : 79.89236450195312\n",
      "236736 ; loss 0.51 ; sentence/s 337 ; words/s 17418 ; accuracy train : 79.88851165771484\n",
      "243136 ; loss 0.5 ; sentence/s 338 ; words/s 17672 ; accuracy train : 79.8770523071289\n",
      "249536 ; loss 0.51 ; sentence/s 335 ; words/s 17714 ; accuracy train : 79.87459564208984\n",
      "255936 ; loss 0.48 ; sentence/s 336 ; words/s 17742 ; accuracy train : 79.90351867675781\n",
      "262336 ; loss 0.5 ; sentence/s 332 ; words/s 17778 ; accuracy train : 79.8974838256836\n",
      "268736 ; loss 0.49 ; sentence/s 337 ; words/s 17526 ; accuracy train : 79.92150115966797\n",
      "275136 ; loss 0.5 ; sentence/s 337 ; words/s 17700 ; accuracy train : 79.91824340820312\n",
      "281536 ; loss 0.51 ; sentence/s 336 ; words/s 17766 ; accuracy train : 79.91513061523438\n",
      "287936 ; loss 0.5 ; sentence/s 335 ; words/s 17720 ; accuracy train : 79.921875\n",
      "294336 ; loss 0.49 ; sentence/s 333 ; words/s 17962 ; accuracy train : 79.94055938720703\n",
      "300736 ; loss 0.52 ; sentence/s 332 ; words/s 17585 ; accuracy train : 79.9278564453125\n",
      "307136 ; loss 0.49 ; sentence/s 336 ; words/s 17686 ; accuracy train : 79.93294525146484\n",
      "313536 ; loss 0.51 ; sentence/s 333 ; words/s 17729 ; accuracy train : 79.9170913696289\n",
      "319936 ; loss 0.5 ; sentence/s 336 ; words/s 17632 ; accuracy train : 79.92843627929688\n",
      "326336 ; loss 0.51 ; sentence/s 338 ; words/s 17435 ; accuracy train : 79.91880798339844\n",
      "332736 ; loss 0.49 ; sentence/s 339 ; words/s 17582 ; accuracy train : 79.93389129638672\n",
      "339136 ; loss 0.47 ; sentence/s 332 ; words/s 17863 ; accuracy train : 79.95695495605469\n",
      "345536 ; loss 0.5 ; sentence/s 341 ; words/s 17448 ; accuracy train : 79.96353912353516\n",
      "351936 ; loss 0.51 ; sentence/s 340 ; words/s 17483 ; accuracy train : 79.9468765258789\n",
      "358336 ; loss 0.51 ; sentence/s 337 ; words/s 17808 ; accuracy train : 79.94642639160156\n",
      "364736 ; loss 0.5 ; sentence/s 338 ; words/s 17651 ; accuracy train : 79.95613861083984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371136 ; loss 0.51 ; sentence/s 334 ; words/s 17785 ; accuracy train : 79.94773864746094\n",
      "377536 ; loss 0.5 ; sentence/s 335 ; words/s 17696 ; accuracy train : 79.94808959960938\n",
      "383936 ; loss 0.49 ; sentence/s 337 ; words/s 17565 ; accuracy train : 79.96562194824219\n",
      "390336 ; loss 0.49 ; sentence/s 335 ; words/s 17759 ; accuracy train : 79.97976684570312\n",
      "396736 ; loss 0.49 ; sentence/s 341 ; words/s 17332 ; accuracy train : 79.98815155029297\n",
      "403136 ; loss 0.5 ; sentence/s 340 ; words/s 17524 ; accuracy train : 79.97866821289062\n",
      "409536 ; loss 0.51 ; sentence/s 344 ; words/s 17355 ; accuracy train : 79.977294921875\n",
      "415936 ; loss 0.51 ; sentence/s 335 ; words/s 17910 ; accuracy train : 79.97595977783203\n",
      "422336 ; loss 0.49 ; sentence/s 337 ; words/s 17695 ; accuracy train : 79.98413848876953\n",
      "428736 ; loss 0.5 ; sentence/s 335 ; words/s 17957 ; accuracy train : 79.97457885742188\n",
      "435136 ; loss 0.5 ; sentence/s 334 ; words/s 17730 ; accuracy train : 79.97702026367188\n",
      "441536 ; loss 0.52 ; sentence/s 339 ; words/s 17741 ; accuracy train : 79.9671630859375\n",
      "447936 ; loss 0.49 ; sentence/s 335 ; words/s 17831 ; accuracy train : 79.97767639160156\n",
      "454336 ; loss 0.49 ; sentence/s 336 ; words/s 17784 ; accuracy train : 79.99515533447266\n",
      "460736 ; loss 0.51 ; sentence/s 340 ; words/s 17393 ; accuracy train : 79.99305725097656\n",
      "467136 ; loss 0.49 ; sentence/s 339 ; words/s 17607 ; accuracy train : 79.99272155761719\n",
      "473536 ; loss 0.5 ; sentence/s 340 ; words/s 17434 ; accuracy train : 79.99851989746094\n",
      "479936 ; loss 0.5 ; sentence/s 335 ; words/s 17681 ; accuracy train : 79.99833679199219\n",
      "486336 ; loss 0.5 ; sentence/s 336 ; words/s 17909 ; accuracy train : 79.99383544921875\n",
      "492736 ; loss 0.53 ; sentence/s 338 ; words/s 17416 ; accuracy train : 79.97605895996094\n",
      "499136 ; loss 0.5 ; sentence/s 334 ; words/s 17902 ; accuracy train : 79.97476196289062\n",
      "505536 ; loss 0.5 ; sentence/s 338 ; words/s 17452 ; accuracy train : 79.97032928466797\n",
      "511936 ; loss 0.52 ; sentence/s 337 ; words/s 17768 ; accuracy train : 79.9632797241211\n",
      "518336 ; loss 0.5 ; sentence/s 335 ; words/s 17959 ; accuracy train : 79.96739959716797\n",
      "524736 ; loss 0.51 ; sentence/s 336 ; words/s 17765 ; accuracy train : 79.96055603027344\n",
      "531136 ; loss 0.51 ; sentence/s 336 ; words/s 17623 ; accuracy train : 79.9563217163086\n",
      "537536 ; loss 0.5 ; sentence/s 332 ; words/s 17952 ; accuracy train : 79.96075439453125\n",
      "543936 ; loss 0.5 ; sentence/s 336 ; words/s 17583 ; accuracy train : 79.96268463134766\n",
      "results : epoch 10 ; mean accuracy train : 79.96202850341797\n",
      "\n",
      "VALIDATION : Epoch 10\n",
      "togrep : results : epoch 10 ; mean accuracy valid :              77.68746185302734\n",
      "saving model at epoch 10\n",
      "\n",
      "TRAINING : Epoch 11\n",
      "Learning rate : 0.01808764150017609\n",
      "6336 ; loss 0.5 ; sentence/s 336 ; words/s 17584 ; accuracy train : 79.9375\n",
      "12736 ; loss 0.48 ; sentence/s 335 ; words/s 17703 ; accuracy train : 80.3046875\n",
      "19136 ; loss 0.5 ; sentence/s 336 ; words/s 17830 ; accuracy train : 80.19791412353516\n",
      "25536 ; loss 0.49 ; sentence/s 336 ; words/s 17830 ; accuracy train : 80.203125\n",
      "31936 ; loss 0.5 ; sentence/s 339 ; words/s 17445 ; accuracy train : 80.2249984741211\n",
      "38336 ; loss 0.5 ; sentence/s 339 ; words/s 17546 ; accuracy train : 80.19791412353516\n",
      "44736 ; loss 0.51 ; sentence/s 338 ; words/s 17436 ; accuracy train : 80.15848541259766\n",
      "51136 ; loss 0.5 ; sentence/s 334 ; words/s 17996 ; accuracy train : 80.146484375\n",
      "57536 ; loss 0.49 ; sentence/s 334 ; words/s 17748 ; accuracy train : 80.16840362548828\n",
      "63936 ; loss 0.5 ; sentence/s 339 ; words/s 17454 ; accuracy train : 80.1953125\n",
      "70336 ; loss 0.5 ; sentence/s 335 ; words/s 17510 ; accuracy train : 80.2258529663086\n",
      "76736 ; loss 0.5 ; sentence/s 336 ; words/s 17639 ; accuracy train : 80.19271087646484\n",
      "83136 ; loss 0.49 ; sentence/s 335 ; words/s 17618 ; accuracy train : 80.23316955566406\n",
      "89536 ; loss 0.5 ; sentence/s 336 ; words/s 17607 ; accuracy train : 80.203125\n",
      "95936 ; loss 0.5 ; sentence/s 334 ; words/s 17917 ; accuracy train : 80.19999694824219\n",
      "102336 ; loss 0.51 ; sentence/s 338 ; words/s 17595 ; accuracy train : 80.2041015625\n",
      "108736 ; loss 0.51 ; sentence/s 334 ; words/s 17686 ; accuracy train : 80.13419342041016\n",
      "115136 ; loss 0.51 ; sentence/s 334 ; words/s 17597 ; accuracy train : 80.11371612548828\n",
      "121536 ; loss 0.52 ; sentence/s 338 ; words/s 17494 ; accuracy train : 80.05756378173828\n",
      "127936 ; loss 0.48 ; sentence/s 335 ; words/s 17783 ; accuracy train : 80.07499694824219\n",
      "134336 ; loss 0.5 ; sentence/s 340 ; words/s 17521 ; accuracy train : 80.06993865966797\n",
      "140736 ; loss 0.51 ; sentence/s 336 ; words/s 17697 ; accuracy train : 80.04119110107422\n",
      "147136 ; loss 0.49 ; sentence/s 337 ; words/s 17569 ; accuracy train : 80.0699691772461\n",
      "153536 ; loss 0.5 ; sentence/s 335 ; words/s 17553 ; accuracy train : 80.07877349853516\n",
      "159936 ; loss 0.5 ; sentence/s 336 ; words/s 17725 ; accuracy train : 80.05437469482422\n",
      "166336 ; loss 0.49 ; sentence/s 333 ; words/s 17831 ; accuracy train : 80.08714294433594\n",
      "172736 ; loss 0.49 ; sentence/s 338 ; words/s 17747 ; accuracy train : 80.10243225097656\n",
      "179136 ; loss 0.5 ; sentence/s 340 ; words/s 17455 ; accuracy train : 80.09319305419922\n",
      "185536 ; loss 0.5 ; sentence/s 339 ; words/s 17620 ; accuracy train : 80.10991668701172\n",
      "191936 ; loss 0.48 ; sentence/s 334 ; words/s 17918 ; accuracy train : 80.12708282470703\n",
      "198336 ; loss 0.49 ; sentence/s 334 ; words/s 17802 ; accuracy train : 80.1335678100586\n",
      "204736 ; loss 0.51 ; sentence/s 337 ; words/s 17604 ; accuracy train : 80.12646484375\n",
      "211136 ; loss 0.5 ; sentence/s 337 ; words/s 17552 ; accuracy train : 80.11931610107422\n",
      "217536 ; loss 0.5 ; sentence/s 339 ; words/s 17472 ; accuracy train : 80.11902618408203\n",
      "223936 ; loss 0.5 ; sentence/s 335 ; words/s 17894 ; accuracy train : 80.10758972167969\n",
      "230336 ; loss 0.49 ; sentence/s 333 ; words/s 18094 ; accuracy train : 80.09809112548828\n",
      "236736 ; loss 0.5 ; sentence/s 334 ; words/s 17519 ; accuracy train : 80.08615112304688\n",
      "243136 ; loss 0.49 ; sentence/s 333 ; words/s 17793 ; accuracy train : 80.08511352539062\n",
      "249536 ; loss 0.49 ; sentence/s 337 ; words/s 17419 ; accuracy train : 80.09535217285156\n",
      "255936 ; loss 0.5 ; sentence/s 337 ; words/s 17666 ; accuracy train : 80.091796875\n",
      "262336 ; loss 0.49 ; sentence/s 333 ; words/s 17559 ; accuracy train : 80.10099029541016\n",
      "268736 ; loss 0.5 ; sentence/s 339 ; words/s 17458 ; accuracy train : 80.09896087646484\n",
      "275136 ; loss 0.51 ; sentence/s 336 ; words/s 17530 ; accuracy train : 80.10319519042969\n",
      "281536 ; loss 0.5 ; sentence/s 336 ; words/s 17609 ; accuracy train : 80.11611938476562\n",
      "287936 ; loss 0.49 ; sentence/s 334 ; words/s 17723 ; accuracy train : 80.13298797607422\n",
      "294336 ; loss 0.51 ; sentence/s 338 ; words/s 17593 ; accuracy train : 80.12670135498047\n",
      "300736 ; loss 0.5 ; sentence/s 342 ; words/s 17273 ; accuracy train : 80.11436462402344\n",
      "307136 ; loss 0.51 ; sentence/s 337 ; words/s 17574 ; accuracy train : 80.10482025146484\n",
      "313536 ; loss 0.5 ; sentence/s 333 ; words/s 17689 ; accuracy train : 80.11128997802734\n",
      "319936 ; loss 0.5 ; sentence/s 333 ; words/s 17868 ; accuracy train : 80.1031265258789\n",
      "326336 ; loss 0.51 ; sentence/s 337 ; words/s 17678 ; accuracy train : 80.09650421142578\n",
      "332736 ; loss 0.51 ; sentence/s 336 ; words/s 17548 ; accuracy train : 80.09044647216797\n",
      "339136 ; loss 0.5 ; sentence/s 337 ; words/s 17525 ; accuracy train : 80.08815002441406\n",
      "345536 ; loss 0.51 ; sentence/s 333 ; words/s 17684 ; accuracy train : 80.07667541503906\n",
      "351936 ; loss 0.5 ; sentence/s 336 ; words/s 17748 ; accuracy train : 80.0835189819336\n",
      "358336 ; loss 0.5 ; sentence/s 339 ; words/s 17309 ; accuracy train : 80.09514617919922\n",
      "364736 ; loss 0.49 ; sentence/s 336 ; words/s 17588 ; accuracy train : 80.1036148071289\n",
      "371136 ; loss 0.49 ; sentence/s 336 ; words/s 17658 ; accuracy train : 80.11180114746094\n",
      "377536 ; loss 0.51 ; sentence/s 339 ; words/s 17602 ; accuracy train : 80.08792114257812\n",
      "383936 ; loss 0.51 ; sentence/s 341 ; words/s 17275 ; accuracy train : 80.07786560058594\n",
      "390336 ; loss 0.49 ; sentence/s 335 ; words/s 17588 ; accuracy train : 80.09093475341797\n",
      "396736 ; loss 0.49 ; sentence/s 338 ; words/s 17580 ; accuracy train : 80.09425354003906\n",
      "403136 ; loss 0.49 ; sentence/s 337 ; words/s 17758 ; accuracy train : 80.0999526977539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409536 ; loss 0.5 ; sentence/s 341 ; words/s 17493 ; accuracy train : 80.1064453125\n",
      "415936 ; loss 0.49 ; sentence/s 335 ; words/s 17669 ; accuracy train : 80.11129760742188\n",
      "422336 ; loss 0.5 ; sentence/s 338 ; words/s 17547 ; accuracy train : 80.109375\n",
      "428736 ; loss 0.5 ; sentence/s 339 ; words/s 17570 ; accuracy train : 80.11566925048828\n",
      "435136 ; loss 0.5 ; sentence/s 338 ; words/s 17511 ; accuracy train : 80.11764526367188\n",
      "441536 ; loss 0.5 ; sentence/s 339 ; words/s 17295 ; accuracy train : 80.11164093017578\n",
      "447936 ; loss 0.5 ; sentence/s 336 ; words/s 17761 ; accuracy train : 80.11473083496094\n",
      "454336 ; loss 0.49 ; sentence/s 336 ; words/s 17764 ; accuracy train : 80.12698364257812\n",
      "460736 ; loss 0.49 ; sentence/s 339 ; words/s 17652 ; accuracy train : 80.12890625\n",
      "467136 ; loss 0.51 ; sentence/s 336 ; words/s 17554 ; accuracy train : 80.13291931152344\n",
      "473536 ; loss 0.51 ; sentence/s 341 ; words/s 17388 ; accuracy train : 80.1233139038086\n",
      "479936 ; loss 0.49 ; sentence/s 332 ; words/s 17791 ; accuracy train : 80.1258316040039\n",
      "486336 ; loss 0.51 ; sentence/s 336 ; words/s 17788 ; accuracy train : 80.11944580078125\n",
      "492736 ; loss 0.51 ; sentence/s 336 ; words/s 17760 ; accuracy train : 80.11546325683594\n",
      "499136 ; loss 0.49 ; sentence/s 337 ; words/s 17755 ; accuracy train : 80.11298370361328\n",
      "505536 ; loss 0.52 ; sentence/s 336 ; words/s 17868 ; accuracy train : 80.09909057617188\n",
      "511936 ; loss 0.51 ; sentence/s 338 ; words/s 17808 ; accuracy train : 80.0894546508789\n",
      "518336 ; loss 0.49 ; sentence/s 334 ; words/s 17935 ; accuracy train : 80.0941390991211\n",
      "524736 ; loss 0.52 ; sentence/s 337 ; words/s 17754 ; accuracy train : 80.08269500732422\n",
      "531136 ; loss 0.49 ; sentence/s 337 ; words/s 17457 ; accuracy train : 80.087158203125\n",
      "537536 ; loss 0.49 ; sentence/s 332 ; words/s 17795 ; accuracy train : 80.09244537353516\n",
      "543936 ; loss 0.49 ; sentence/s 332 ; words/s 17740 ; accuracy train : 80.08897399902344\n",
      "results : epoch 11 ; mean accuracy train : 80.0914535522461\n",
      "\n",
      "VALIDATION : Epoch 11\n",
      "togrep : results : epoch 11 ; mean accuracy valid :              77.75858306884766\n",
      "saving model at epoch 11\n",
      "\n",
      "TRAINING : Epoch 12\n",
      "Learning rate : 0.017906765085174327\n",
      "6336 ; loss 0.49 ; sentence/s 336 ; words/s 17484 ; accuracy train : 80.484375\n",
      "12736 ; loss 0.48 ; sentence/s 340 ; words/s 17534 ; accuracy train : 80.609375\n",
      "19136 ; loss 0.49 ; sentence/s 337 ; words/s 17748 ; accuracy train : 80.50521087646484\n",
      "25536 ; loss 0.51 ; sentence/s 334 ; words/s 17984 ; accuracy train : 80.19140625\n",
      "31936 ; loss 0.5 ; sentence/s 337 ; words/s 17610 ; accuracy train : 80.0718765258789\n",
      "38336 ; loss 0.51 ; sentence/s 338 ; words/s 17563 ; accuracy train : 79.97135162353516\n",
      "44736 ; loss 0.48 ; sentence/s 336 ; words/s 17916 ; accuracy train : 79.99776458740234\n",
      "51136 ; loss 0.5 ; sentence/s 340 ; words/s 17447 ; accuracy train : 80.044921875\n",
      "57536 ; loss 0.48 ; sentence/s 337 ; words/s 17676 ; accuracy train : 80.15103912353516\n",
      "63936 ; loss 0.49 ; sentence/s 334 ; words/s 17866 ; accuracy train : 80.2328109741211\n",
      "70336 ; loss 0.47 ; sentence/s 341 ; words/s 17488 ; accuracy train : 80.29119110107422\n",
      "76736 ; loss 0.5 ; sentence/s 338 ; words/s 17526 ; accuracy train : 80.25130462646484\n",
      "83136 ; loss 0.49 ; sentence/s 337 ; words/s 17676 ; accuracy train : 80.26802825927734\n",
      "89536 ; loss 0.49 ; sentence/s 340 ; words/s 17633 ; accuracy train : 80.234375\n",
      "95936 ; loss 0.49 ; sentence/s 336 ; words/s 17578 ; accuracy train : 80.21770477294922\n",
      "102336 ; loss 0.52 ; sentence/s 337 ; words/s 17651 ; accuracy train : 80.150390625\n",
      "108736 ; loss 0.49 ; sentence/s 333 ; words/s 17791 ; accuracy train : 80.14981842041016\n",
      "115136 ; loss 0.5 ; sentence/s 336 ; words/s 17741 ; accuracy train : 80.16059112548828\n",
      "121536 ; loss 0.52 ; sentence/s 334 ; words/s 17744 ; accuracy train : 80.11431121826172\n",
      "127936 ; loss 0.49 ; sentence/s 333 ; words/s 17955 ; accuracy train : 80.1429672241211\n",
      "134336 ; loss 0.49 ; sentence/s 336 ; words/s 17760 ; accuracy train : 80.15178680419922\n",
      "140736 ; loss 0.5 ; sentence/s 336 ; words/s 17595 ; accuracy train : 80.11931610107422\n",
      "147136 ; loss 0.49 ; sentence/s 338 ; words/s 17583 ; accuracy train : 80.1324691772461\n",
      "153536 ; loss 0.49 ; sentence/s 332 ; words/s 17940 ; accuracy train : 80.15494537353516\n",
      "159936 ; loss 0.5 ; sentence/s 337 ; words/s 17605 ; accuracy train : 80.13874816894531\n",
      "166336 ; loss 0.5 ; sentence/s 333 ; words/s 17532 ; accuracy train : 80.12560272216797\n",
      "172736 ; loss 0.51 ; sentence/s 342 ; words/s 17328 ; accuracy train : 80.1209487915039\n",
      "179136 ; loss 0.49 ; sentence/s 334 ; words/s 17482 ; accuracy train : 80.12165069580078\n",
      "185536 ; loss 0.49 ; sentence/s 338 ; words/s 17572 ; accuracy train : 80.12985229492188\n",
      "191936 ; loss 0.49 ; sentence/s 336 ; words/s 17706 ; accuracy train : 80.15676879882812\n",
      "198336 ; loss 0.51 ; sentence/s 335 ; words/s 17849 ; accuracy train : 80.14213562011719\n",
      "204736 ; loss 0.49 ; sentence/s 333 ; words/s 17918 ; accuracy train : 80.15673828125\n",
      "211136 ; loss 0.49 ; sentence/s 335 ; words/s 17854 ; accuracy train : 80.15672302246094\n",
      "217536 ; loss 0.49 ; sentence/s 338 ; words/s 17652 ; accuracy train : 80.1695785522461\n",
      "223936 ; loss 0.5 ; sentence/s 337 ; words/s 17562 ; accuracy train : 80.17098236083984\n",
      "230336 ; loss 0.52 ; sentence/s 337 ; words/s 17571 ; accuracy train : 80.14236450195312\n",
      "236736 ; loss 0.5 ; sentence/s 337 ; words/s 17692 ; accuracy train : 80.140625\n",
      "243136 ; loss 0.5 ; sentence/s 337 ; words/s 17592 ; accuracy train : 80.13157653808594\n",
      "249536 ; loss 0.49 ; sentence/s 333 ; words/s 17903 ; accuracy train : 80.14583587646484\n",
      "255936 ; loss 0.51 ; sentence/s 337 ; words/s 17625 ; accuracy train : 80.1171875\n",
      "262336 ; loss 0.49 ; sentence/s 336 ; words/s 17654 ; accuracy train : 80.12309265136719\n",
      "268736 ; loss 0.48 ; sentence/s 338 ; words/s 17721 ; accuracy train : 80.13876342773438\n",
      "275136 ; loss 0.48 ; sentence/s 334 ; words/s 17712 ; accuracy train : 80.16751098632812\n",
      "281536 ; loss 0.49 ; sentence/s 334 ; words/s 17666 ; accuracy train : 80.16974639892578\n",
      "287936 ; loss 0.48 ; sentence/s 333 ; words/s 17673 ; accuracy train : 80.18541717529297\n",
      "294336 ; loss 0.49 ; sentence/s 334 ; words/s 17711 ; accuracy train : 80.17900848388672\n",
      "300736 ; loss 0.48 ; sentence/s 338 ; words/s 17475 ; accuracy train : 80.17819213867188\n",
      "307136 ; loss 0.5 ; sentence/s 340 ; words/s 17391 ; accuracy train : 80.1826171875\n",
      "313536 ; loss 0.49 ; sentence/s 333 ; words/s 17563 ; accuracy train : 80.18973541259766\n",
      "319936 ; loss 0.51 ; sentence/s 335 ; words/s 17848 ; accuracy train : 80.1762466430664\n",
      "326336 ; loss 0.49 ; sentence/s 336 ; words/s 17583 ; accuracy train : 80.1767807006836\n",
      "332736 ; loss 0.5 ; sentence/s 335 ; words/s 17519 ; accuracy train : 80.17998504638672\n",
      "339136 ; loss 0.5 ; sentence/s 334 ; words/s 17905 ; accuracy train : 80.17393493652344\n",
      "345536 ; loss 0.51 ; sentence/s 339 ; words/s 17512 ; accuracy train : 80.16580200195312\n",
      "351936 ; loss 0.5 ; sentence/s 335 ; words/s 17654 ; accuracy train : 80.1571044921875\n",
      "358336 ; loss 0.49 ; sentence/s 337 ; words/s 17841 ; accuracy train : 80.158203125\n",
      "364736 ; loss 0.49 ; sentence/s 338 ; words/s 17622 ; accuracy train : 80.1672134399414\n",
      "371136 ; loss 0.5 ; sentence/s 338 ; words/s 17461 ; accuracy train : 80.17322540283203\n",
      "377536 ; loss 0.49 ; sentence/s 338 ; words/s 17590 ; accuracy train : 80.18193817138672\n",
      "383936 ; loss 0.5 ; sentence/s 337 ; words/s 17765 ; accuracy train : 80.17786407470703\n",
      "390336 ; loss 0.5 ; sentence/s 336 ; words/s 17717 ; accuracy train : 80.18596649169922\n",
      "396736 ; loss 0.5 ; sentence/s 336 ; words/s 17658 ; accuracy train : 80.18220520019531\n",
      "403136 ; loss 0.49 ; sentence/s 338 ; words/s 17695 ; accuracy train : 80.19345092773438\n",
      "409536 ; loss 0.49 ; sentence/s 342 ; words/s 17429 ; accuracy train : 80.195556640625\n",
      "415936 ; loss 0.51 ; sentence/s 340 ; words/s 17447 ; accuracy train : 80.18029022216797\n",
      "422336 ; loss 0.5 ; sentence/s 335 ; words/s 17746 ; accuracy train : 80.18228912353516\n",
      "428736 ; loss 0.49 ; sentence/s 339 ; words/s 17679 ; accuracy train : 80.18353271484375\n",
      "435136 ; loss 0.51 ; sentence/s 335 ; words/s 17657 ; accuracy train : 80.17532348632812\n",
      "441536 ; loss 0.49 ; sentence/s 331 ; words/s 17946 ; accuracy train : 80.17821502685547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447936 ; loss 0.51 ; sentence/s 332 ; words/s 18134 ; accuracy train : 80.17388153076172\n",
      "454336 ; loss 0.51 ; sentence/s 338 ; words/s 17648 ; accuracy train : 80.16527557373047\n",
      "460736 ; loss 0.5 ; sentence/s 336 ; words/s 17859 ; accuracy train : 80.15950775146484\n",
      "467136 ; loss 0.49 ; sentence/s 336 ; words/s 17737 ; accuracy train : 80.16288757324219\n",
      "473536 ; loss 0.5 ; sentence/s 337 ; words/s 17755 ; accuracy train : 80.16216278076172\n",
      "479936 ; loss 0.5 ; sentence/s 338 ; words/s 17863 ; accuracy train : 80.16666412353516\n",
      "486336 ; loss 0.48 ; sentence/s 337 ; words/s 17521 ; accuracy train : 80.1772232055664\n",
      "492736 ; loss 0.5 ; sentence/s 340 ; words/s 17427 ; accuracy train : 80.17005157470703\n",
      "499136 ; loss 0.49 ; sentence/s 339 ; words/s 17411 ; accuracy train : 80.1728744506836\n",
      "505536 ; loss 0.49 ; sentence/s 340 ; words/s 17507 ; accuracy train : 80.17701721191406\n",
      "511936 ; loss 0.48 ; sentence/s 336 ; words/s 17860 ; accuracy train : 80.1861343383789\n",
      "518336 ; loss 0.51 ; sentence/s 344 ; words/s 17381 ; accuracy train : 80.18074798583984\n",
      "524736 ; loss 0.49 ; sentence/s 338 ; words/s 17815 ; accuracy train : 80.18463897705078\n",
      "531136 ; loss 0.5 ; sentence/s 337 ; words/s 17522 ; accuracy train : 80.1882553100586\n",
      "537536 ; loss 0.49 ; sentence/s 339 ; words/s 17497 ; accuracy train : 80.18824768066406\n",
      "543936 ; loss 0.49 ; sentence/s 334 ; words/s 17888 ; accuracy train : 80.19485473632812\n",
      "results : epoch 12 ; mean accuracy train : 80.19193267822266\n",
      "\n",
      "VALIDATION : Epoch 12\n",
      "togrep : results : epoch 12 ; mean accuracy valid :              77.7789077758789\n",
      "saving model at epoch 12\n",
      "\n",
      "TRAINING : Epoch 13\n",
      "Learning rate : 0.017727697434322585\n",
      "6336 ; loss 0.49 ; sentence/s 336 ; words/s 17698 ; accuracy train : 80.703125\n",
      "12736 ; loss 0.51 ; sentence/s 336 ; words/s 17607 ; accuracy train : 80.1171875\n",
      "19136 ; loss 0.49 ; sentence/s 332 ; words/s 17908 ; accuracy train : 80.140625\n",
      "25536 ; loss 0.49 ; sentence/s 334 ; words/s 17635 ; accuracy train : 80.0859375\n",
      "31936 ; loss 0.49 ; sentence/s 334 ; words/s 17780 ; accuracy train : 80.21562194824219\n",
      "38336 ; loss 0.48 ; sentence/s 338 ; words/s 17629 ; accuracy train : 80.42708587646484\n",
      "44736 ; loss 0.5 ; sentence/s 336 ; words/s 17671 ; accuracy train : 80.38616180419922\n",
      "51136 ; loss 0.48 ; sentence/s 338 ; words/s 17420 ; accuracy train : 80.435546875\n",
      "57536 ; loss 0.48 ; sentence/s 335 ; words/s 17692 ; accuracy train : 80.42534637451172\n",
      "63936 ; loss 0.48 ; sentence/s 337 ; words/s 17885 ; accuracy train : 80.4000015258789\n",
      "70336 ; loss 0.49 ; sentence/s 338 ; words/s 17390 ; accuracy train : 80.44033813476562\n",
      "76736 ; loss 0.5 ; sentence/s 336 ; words/s 17704 ; accuracy train : 80.39322662353516\n",
      "83136 ; loss 0.49 ; sentence/s 339 ; words/s 17424 ; accuracy train : 80.40264129638672\n",
      "89536 ; loss 0.49 ; sentence/s 334 ; words/s 17648 ; accuracy train : 80.38169860839844\n",
      "95936 ; loss 0.51 ; sentence/s 340 ; words/s 17592 ; accuracy train : 80.35729217529297\n",
      "102336 ; loss 0.48 ; sentence/s 335 ; words/s 17680 ; accuracy train : 80.3837890625\n",
      "108736 ; loss 0.49 ; sentence/s 337 ; words/s 17547 ; accuracy train : 80.36121368408203\n",
      "115136 ; loss 0.5 ; sentence/s 338 ; words/s 17460 ; accuracy train : 80.31684112548828\n",
      "121536 ; loss 0.49 ; sentence/s 337 ; words/s 17668 ; accuracy train : 80.33470153808594\n",
      "127936 ; loss 0.49 ; sentence/s 334 ; words/s 17667 ; accuracy train : 80.3179702758789\n",
      "134336 ; loss 0.49 ; sentence/s 337 ; words/s 17821 ; accuracy train : 80.35416412353516\n",
      "140736 ; loss 0.5 ; sentence/s 334 ; words/s 17697 ; accuracy train : 80.30966186523438\n",
      "147136 ; loss 0.48 ; sentence/s 334 ; words/s 17849 ; accuracy train : 80.33831787109375\n",
      "153536 ; loss 0.5 ; sentence/s 332 ; words/s 17767 ; accuracy train : 80.31705474853516\n",
      "159936 ; loss 0.5 ; sentence/s 334 ; words/s 17841 ; accuracy train : 80.3012466430664\n",
      "166336 ; loss 0.48 ; sentence/s 337 ; words/s 17745 ; accuracy train : 80.34014129638672\n",
      "172736 ; loss 0.49 ; sentence/s 334 ; words/s 17706 ; accuracy train : 80.35127258300781\n",
      "179136 ; loss 0.5 ; sentence/s 333 ; words/s 17906 ; accuracy train : 80.33984375\n",
      "185536 ; loss 0.5 ; sentence/s 332 ; words/s 17786 ; accuracy train : 80.35614013671875\n",
      "191936 ; loss 0.5 ; sentence/s 336 ; words/s 17767 ; accuracy train : 80.34687805175781\n",
      "198336 ; loss 0.49 ; sentence/s 339 ; words/s 17549 ; accuracy train : 80.34526062011719\n",
      "204736 ; loss 0.5 ; sentence/s 333 ; words/s 17864 ; accuracy train : 80.318359375\n",
      "211136 ; loss 0.49 ; sentence/s 339 ; words/s 17588 ; accuracy train : 80.3352279663086\n",
      "217536 ; loss 0.48 ; sentence/s 335 ; words/s 17616 ; accuracy train : 80.34834289550781\n",
      "223936 ; loss 0.49 ; sentence/s 337 ; words/s 17582 ; accuracy train : 80.3513412475586\n",
      "230336 ; loss 0.51 ; sentence/s 337 ; words/s 17537 ; accuracy train : 80.32552337646484\n",
      "236736 ; loss 0.5 ; sentence/s 338 ; words/s 17452 ; accuracy train : 80.3310775756836\n",
      "243136 ; loss 0.48 ; sentence/s 337 ; words/s 17577 ; accuracy train : 80.3334732055664\n",
      "249536 ; loss 0.49 ; sentence/s 340 ; words/s 17673 ; accuracy train : 80.34535217285156\n",
      "255936 ; loss 0.48 ; sentence/s 341 ; words/s 17532 ; accuracy train : 80.36601257324219\n",
      "262336 ; loss 0.5 ; sentence/s 336 ; words/s 17688 ; accuracy train : 80.36051940917969\n",
      "268736 ; loss 0.51 ; sentence/s 338 ; words/s 17574 ; accuracy train : 80.35379791259766\n",
      "275136 ; loss 0.48 ; sentence/s 339 ; words/s 17588 ; accuracy train : 80.36155700683594\n",
      "281536 ; loss 0.49 ; sentence/s 338 ; words/s 17869 ; accuracy train : 80.35653686523438\n",
      "287936 ; loss 0.5 ; sentence/s 333 ; words/s 18006 ; accuracy train : 80.35173797607422\n",
      "294336 ; loss 0.5 ; sentence/s 337 ; words/s 17679 ; accuracy train : 80.35427856445312\n",
      "300736 ; loss 0.49 ; sentence/s 341 ; words/s 17524 ; accuracy train : 80.3567123413086\n",
      "307136 ; loss 0.5 ; sentence/s 339 ; words/s 17524 ; accuracy train : 80.3505859375\n",
      "313536 ; loss 0.48 ; sentence/s 336 ; words/s 17485 ; accuracy train : 80.3494873046875\n",
      "319936 ; loss 0.49 ; sentence/s 338 ; words/s 17693 ; accuracy train : 80.3471908569336\n",
      "326336 ; loss 0.5 ; sentence/s 337 ; words/s 17777 ; accuracy train : 80.35018157958984\n",
      "332736 ; loss 0.5 ; sentence/s 337 ; words/s 17722 ; accuracy train : 80.34705352783203\n",
      "339136 ; loss 0.48 ; sentence/s 335 ; words/s 17817 ; accuracy train : 80.35643005371094\n",
      "345536 ; loss 0.5 ; sentence/s 334 ; words/s 17851 ; accuracy train : 80.34143829345703\n",
      "351936 ; loss 0.49 ; sentence/s 339 ; words/s 17522 ; accuracy train : 80.34346771240234\n",
      "358336 ; loss 0.48 ; sentence/s 339 ; words/s 17522 ; accuracy train : 80.35323333740234\n",
      "364736 ; loss 0.49 ; sentence/s 338 ; words/s 17558 ; accuracy train : 80.35882568359375\n",
      "371136 ; loss 0.48 ; sentence/s 338 ; words/s 17778 ; accuracy train : 80.37123107910156\n",
      "377536 ; loss 0.5 ; sentence/s 338 ; words/s 17719 ; accuracy train : 80.35804748535156\n",
      "383936 ; loss 0.49 ; sentence/s 337 ; words/s 17629 ; accuracy train : 80.35234069824219\n",
      "390336 ; loss 0.5 ; sentence/s 333 ; words/s 17932 ; accuracy train : 80.34016418457031\n",
      "396736 ; loss 0.49 ; sentence/s 340 ; words/s 17275 ; accuracy train : 80.33946228027344\n",
      "403136 ; loss 0.5 ; sentence/s 337 ; words/s 17700 ; accuracy train : 80.34151458740234\n",
      "409536 ; loss 0.51 ; sentence/s 343 ; words/s 17451 ; accuracy train : 80.320068359375\n",
      "415936 ; loss 0.49 ; sentence/s 337 ; words/s 17319 ; accuracy train : 80.31634521484375\n",
      "422336 ; loss 0.48 ; sentence/s 339 ; words/s 17506 ; accuracy train : 80.32717895507812\n",
      "428736 ; loss 0.49 ; sentence/s 335 ; words/s 17612 ; accuracy train : 80.328125\n",
      "435136 ; loss 0.5 ; sentence/s 336 ; words/s 17662 ; accuracy train : 80.31732177734375\n",
      "441536 ; loss 0.5 ; sentence/s 335 ; words/s 17713 ; accuracy train : 80.32110595703125\n",
      "447936 ; loss 0.5 ; sentence/s 336 ; words/s 17741 ; accuracy train : 80.31539916992188\n",
      "454336 ; loss 0.49 ; sentence/s 334 ; words/s 17665 ; accuracy train : 80.31690216064453\n",
      "460736 ; loss 0.51 ; sentence/s 336 ; words/s 17750 ; accuracy train : 80.31488800048828\n",
      "467136 ; loss 0.5 ; sentence/s 337 ; words/s 17801 ; accuracy train : 80.31057739257812\n",
      "473536 ; loss 0.51 ; sentence/s 339 ; words/s 17422 ; accuracy train : 80.29772186279297\n",
      "479936 ; loss 0.47 ; sentence/s 337 ; words/s 17672 ; accuracy train : 80.31541442871094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486336 ; loss 0.49 ; sentence/s 337 ; words/s 17642 ; accuracy train : 80.32154846191406\n",
      "492736 ; loss 0.5 ; sentence/s 341 ; words/s 17487 ; accuracy train : 80.31919860839844\n",
      "499136 ; loss 0.5 ; sentence/s 336 ; words/s 17910 ; accuracy train : 80.30689239501953\n",
      "505536 ; loss 0.49 ; sentence/s 334 ; words/s 17714 ; accuracy train : 80.31092071533203\n",
      "511936 ; loss 0.49 ; sentence/s 340 ; words/s 17485 ; accuracy train : 80.3115234375\n",
      "518336 ; loss 0.49 ; sentence/s 333 ; words/s 17740 ; accuracy train : 80.31192016601562\n",
      "524736 ; loss 0.49 ; sentence/s 339 ; words/s 17663 ; accuracy train : 80.31288146972656\n",
      "531136 ; loss 0.48 ; sentence/s 334 ; words/s 17716 ; accuracy train : 80.32003021240234\n",
      "537536 ; loss 0.5 ; sentence/s 338 ; words/s 17493 ; accuracy train : 80.32328796386719\n",
      "543936 ; loss 0.49 ; sentence/s 338 ; words/s 17701 ; accuracy train : 80.3218765258789\n",
      "results : epoch 13 ; mean accuracy train : 80.3182601928711\n",
      "\n",
      "VALIDATION : Epoch 13\n",
      "togrep : results : epoch 13 ; mean accuracy valid :              77.656982421875\n",
      "Shrinking lr by : 5. New lr = 0.003545539486864517\n",
      "\n",
      "TRAINING : Epoch 14\n",
      "Learning rate : 0.0035100840919958715\n",
      "6336 ; loss 0.49 ; sentence/s 337 ; words/s 17464 ; accuracy train : 80.375\n",
      "12736 ; loss 0.5 ; sentence/s 337 ; words/s 17534 ; accuracy train : 80.203125\n",
      "19136 ; loss 0.48 ; sentence/s 337 ; words/s 17570 ; accuracy train : 80.53125\n",
      "25536 ; loss 0.48 ; sentence/s 338 ; words/s 17560 ; accuracy train : 80.59765625\n",
      "31936 ; loss 0.5 ; sentence/s 334 ; words/s 17779 ; accuracy train : 80.53437805175781\n",
      "38336 ; loss 0.51 ; sentence/s 336 ; words/s 17906 ; accuracy train : 80.453125\n",
      "44736 ; loss 0.49 ; sentence/s 334 ; words/s 17729 ; accuracy train : 80.39508819580078\n",
      "51136 ; loss 0.49 ; sentence/s 333 ; words/s 17821 ; accuracy train : 80.333984375\n",
      "57536 ; loss 0.49 ; sentence/s 338 ; words/s 17684 ; accuracy train : 80.38715362548828\n",
      "63936 ; loss 0.49 ; sentence/s 337 ; words/s 17656 ; accuracy train : 80.40156555175781\n",
      "70336 ; loss 0.49 ; sentence/s 338 ; words/s 17563 ; accuracy train : 80.48011016845703\n",
      "76736 ; loss 0.5 ; sentence/s 337 ; words/s 17704 ; accuracy train : 80.43359375\n",
      "83136 ; loss 0.5 ; sentence/s 335 ; words/s 17841 ; accuracy train : 80.40504455566406\n",
      "89536 ; loss 0.49 ; sentence/s 334 ; words/s 17750 ; accuracy train : 80.41963958740234\n",
      "95936 ; loss 0.49 ; sentence/s 336 ; words/s 17471 ; accuracy train : 80.4000015258789\n",
      "102336 ; loss 0.48 ; sentence/s 338 ; words/s 17514 ; accuracy train : 80.4306640625\n",
      "108736 ; loss 0.47 ; sentence/s 333 ; words/s 17698 ; accuracy train : 80.4944839477539\n",
      "115136 ; loss 0.49 ; sentence/s 336 ; words/s 17853 ; accuracy train : 80.49826049804688\n",
      "121536 ; loss 0.49 ; sentence/s 338 ; words/s 17662 ; accuracy train : 80.5254898071289\n",
      "127936 ; loss 0.49 ; sentence/s 338 ; words/s 17650 ; accuracy train : 80.51094055175781\n",
      "134336 ; loss 0.49 ; sentence/s 336 ; words/s 17683 ; accuracy train : 80.5171127319336\n",
      "140736 ; loss 0.51 ; sentence/s 340 ; words/s 17632 ; accuracy train : 80.46377563476562\n",
      "147136 ; loss 0.48 ; sentence/s 333 ; words/s 17701 ; accuracy train : 80.4877700805664\n",
      "153536 ; loss 0.49 ; sentence/s 335 ; words/s 17677 ; accuracy train : 80.49153900146484\n",
      "159936 ; loss 0.5 ; sentence/s 337 ; words/s 17844 ; accuracy train : 80.47250366210938\n",
      "166336 ; loss 0.5 ; sentence/s 335 ; words/s 17622 ; accuracy train : 80.45673370361328\n",
      "172736 ; loss 0.48 ; sentence/s 339 ; words/s 17624 ; accuracy train : 80.47396087646484\n",
      "179136 ; loss 0.49 ; sentence/s 336 ; words/s 17529 ; accuracy train : 80.48270416259766\n",
      "185536 ; loss 0.47 ; sentence/s 337 ; words/s 17688 ; accuracy train : 80.50862121582031\n",
      "191936 ; loss 0.49 ; sentence/s 336 ; words/s 17551 ; accuracy train : 80.52864837646484\n",
      "198336 ; loss 0.48 ; sentence/s 337 ; words/s 17317 ; accuracy train : 80.54788208007812\n",
      "204736 ; loss 0.48 ; sentence/s 337 ; words/s 17743 ; accuracy train : 80.541015625\n",
      "211136 ; loss 0.5 ; sentence/s 340 ; words/s 17380 ; accuracy train : 80.52462005615234\n",
      "217536 ; loss 0.5 ; sentence/s 341 ; words/s 17531 ; accuracy train : 80.50827026367188\n",
      "223936 ; loss 0.51 ; sentence/s 340 ; words/s 17460 ; accuracy train : 80.48124694824219\n",
      "230336 ; loss 0.49 ; sentence/s 336 ; words/s 17690 ; accuracy train : 80.47569274902344\n",
      "236736 ; loss 0.48 ; sentence/s 339 ; words/s 17643 ; accuracy train : 80.4826889038086\n",
      "243136 ; loss 0.5 ; sentence/s 338 ; words/s 17709 ; accuracy train : 80.4712142944336\n",
      "249536 ; loss 0.48 ; sentence/s 333 ; words/s 17932 ; accuracy train : 80.46875\n",
      "255936 ; loss 0.48 ; sentence/s 337 ; words/s 17668 ; accuracy train : 80.484375\n",
      "262336 ; loss 0.49 ; sentence/s 339 ; words/s 17465 ; accuracy train : 80.48323059082031\n",
      "268736 ; loss 0.49 ; sentence/s 337 ; words/s 17485 ; accuracy train : 80.47358703613281\n",
      "275136 ; loss 0.5 ; sentence/s 334 ; words/s 17609 ; accuracy train : 80.46511840820312\n",
      "281536 ; loss 0.49 ; sentence/s 336 ; words/s 17590 ; accuracy train : 80.46235656738281\n",
      "287936 ; loss 0.49 ; sentence/s 338 ; words/s 17493 ; accuracy train : 80.46597290039062\n",
      "294336 ; loss 0.49 ; sentence/s 339 ; words/s 17481 ; accuracy train : 80.47010803222656\n",
      "300736 ; loss 0.5 ; sentence/s 334 ; words/s 17835 ; accuracy train : 80.46443176269531\n",
      "307136 ; loss 0.49 ; sentence/s 334 ; words/s 17805 ; accuracy train : 80.46126556396484\n",
      "313536 ; loss 0.5 ; sentence/s 337 ; words/s 17445 ; accuracy train : 80.45121002197266\n",
      "319936 ; loss 0.48 ; sentence/s 335 ; words/s 17603 ; accuracy train : 80.45375061035156\n",
      "326336 ; loss 0.5 ; sentence/s 335 ; words/s 17851 ; accuracy train : 80.45067596435547\n",
      "332736 ; loss 0.49 ; sentence/s 339 ; words/s 17543 ; accuracy train : 80.44410705566406\n",
      "339136 ; loss 0.5 ; sentence/s 334 ; words/s 18003 ; accuracy train : 80.44221496582031\n",
      "345536 ; loss 0.5 ; sentence/s 336 ; words/s 17829 ; accuracy train : 80.4337387084961\n",
      "351936 ; loss 0.48 ; sentence/s 333 ; words/s 17865 ; accuracy train : 80.43636322021484\n",
      "358336 ; loss 0.49 ; sentence/s 334 ; words/s 17756 ; accuracy train : 80.44001007080078\n",
      "364736 ; loss 0.49 ; sentence/s 339 ; words/s 17524 ; accuracy train : 80.4410629272461\n",
      "371136 ; loss 0.49 ; sentence/s 339 ; words/s 17586 ; accuracy train : 80.4375\n",
      "377536 ; loss 0.5 ; sentence/s 334 ; words/s 17977 ; accuracy train : 80.42558288574219\n",
      "383936 ; loss 0.49 ; sentence/s 334 ; words/s 17761 ; accuracy train : 80.4320297241211\n",
      "390336 ; loss 0.49 ; sentence/s 336 ; words/s 17692 ; accuracy train : 80.4349365234375\n",
      "396736 ; loss 0.49 ; sentence/s 337 ; words/s 17792 ; accuracy train : 80.43724822998047\n",
      "403136 ; loss 0.5 ; sentence/s 341 ; words/s 17450 ; accuracy train : 80.43700408935547\n",
      "409536 ; loss 0.49 ; sentence/s 334 ; words/s 17684 ; accuracy train : 80.435302734375\n",
      "415936 ; loss 0.49 ; sentence/s 334 ; words/s 17704 ; accuracy train : 80.437255859375\n",
      "422336 ; loss 0.49 ; sentence/s 341 ; words/s 17248 ; accuracy train : 80.43252563476562\n",
      "428736 ; loss 0.48 ; sentence/s 336 ; words/s 17609 ; accuracy train : 80.43656921386719\n",
      "435136 ; loss 0.5 ; sentence/s 330 ; words/s 17989 ; accuracy train : 80.42371368408203\n",
      "441536 ; loss 0.49 ; sentence/s 339 ; words/s 17656 ; accuracy train : 80.42164611816406\n",
      "447936 ; loss 0.49 ; sentence/s 338 ; words/s 17461 ; accuracy train : 80.4203109741211\n",
      "454336 ; loss 0.49 ; sentence/s 339 ; words/s 17407 ; accuracy train : 80.42363739013672\n",
      "460736 ; loss 0.49 ; sentence/s 338 ; words/s 17438 ; accuracy train : 80.42166137695312\n",
      "467136 ; loss 0.5 ; sentence/s 334 ; words/s 17722 ; accuracy train : 80.4165267944336\n",
      "473536 ; loss 0.48 ; sentence/s 338 ; words/s 17485 ; accuracy train : 80.41680908203125\n",
      "479936 ; loss 0.5 ; sentence/s 337 ; words/s 17414 ; accuracy train : 80.40937805175781\n",
      "486336 ; loss 0.48 ; sentence/s 337 ; words/s 17829 ; accuracy train : 80.41426849365234\n",
      "492736 ; loss 0.49 ; sentence/s 336 ; words/s 17871 ; accuracy train : 80.41497802734375\n",
      "499136 ; loss 0.5 ; sentence/s 335 ; words/s 17591 ; accuracy train : 80.40785217285156\n",
      "505536 ; loss 0.49 ; sentence/s 338 ; words/s 17631 ; accuracy train : 80.41455841064453\n",
      "511936 ; loss 0.49 ; sentence/s 336 ; words/s 17880 ; accuracy train : 80.41679382324219\n",
      "518336 ; loss 0.48 ; sentence/s 338 ; words/s 17590 ; accuracy train : 80.42168426513672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524736 ; loss 0.49 ; sentence/s 334 ; words/s 17754 ; accuracy train : 80.4209213256836\n",
      "531136 ; loss 0.48 ; sentence/s 335 ; words/s 17623 ; accuracy train : 80.41679382324219\n",
      "537536 ; loss 0.48 ; sentence/s 335 ; words/s 17700 ; accuracy train : 80.42764282226562\n",
      "543936 ; loss 0.5 ; sentence/s 331 ; words/s 18132 ; accuracy train : 80.42518615722656\n",
      "results : epoch 14 ; mean accuracy train : 80.42947387695312\n",
      "\n",
      "VALIDATION : Epoch 14\n",
      "togrep : results : epoch 14 ; mean accuracy valid :              77.92115783691406\n",
      "saving model at epoch 14\n",
      "\n",
      "TRAINING : Epoch 15\n",
      "Learning rate : 0.0034749832510759127\n",
      "6336 ; loss 0.49 ; sentence/s 334 ; words/s 17568 ; accuracy train : 80.234375\n",
      "12736 ; loss 0.51 ; sentence/s 334 ; words/s 17635 ; accuracy train : 79.703125\n",
      "19136 ; loss 0.49 ; sentence/s 336 ; words/s 17667 ; accuracy train : 80.08333587646484\n",
      "25536 ; loss 0.5 ; sentence/s 338 ; words/s 17562 ; accuracy train : 80.08203125\n",
      "31936 ; loss 0.5 ; sentence/s 332 ; words/s 17815 ; accuracy train : 80.078125\n",
      "38336 ; loss 0.49 ; sentence/s 342 ; words/s 17335 ; accuracy train : 80.1875\n",
      "44736 ; loss 0.49 ; sentence/s 335 ; words/s 17826 ; accuracy train : 80.22544860839844\n",
      "51136 ; loss 0.48 ; sentence/s 337 ; words/s 17653 ; accuracy train : 80.31640625\n",
      "57536 ; loss 0.47 ; sentence/s 340 ; words/s 17406 ; accuracy train : 80.47396087646484\n",
      "63936 ; loss 0.5 ; sentence/s 334 ; words/s 17822 ; accuracy train : 80.42655944824219\n",
      "70336 ; loss 0.49 ; sentence/s 336 ; words/s 17698 ; accuracy train : 80.45454406738281\n",
      "76736 ; loss 0.49 ; sentence/s 339 ; words/s 17367 ; accuracy train : 80.38411712646484\n",
      "83136 ; loss 0.49 ; sentence/s 338 ; words/s 17509 ; accuracy train : 80.42668151855469\n",
      "89536 ; loss 0.47 ; sentence/s 336 ; words/s 17822 ; accuracy train : 80.46651458740234\n",
      "95936 ; loss 0.5 ; sentence/s 337 ; words/s 17721 ; accuracy train : 80.41666412353516\n",
      "102336 ; loss 0.49 ; sentence/s 336 ; words/s 17597 ; accuracy train : 80.4130859375\n",
      "108736 ; loss 0.49 ; sentence/s 334 ; words/s 17949 ; accuracy train : 80.44393157958984\n",
      "115136 ; loss 0.48 ; sentence/s 336 ; words/s 17831 ; accuracy train : 80.47222137451172\n",
      "121536 ; loss 0.48 ; sentence/s 334 ; words/s 17769 ; accuracy train : 80.47532653808594\n",
      "127936 ; loss 0.5 ; sentence/s 338 ; words/s 17572 ; accuracy train : 80.46171569824219\n",
      "134336 ; loss 0.49 ; sentence/s 334 ; words/s 17515 ; accuracy train : 80.43973541259766\n",
      "140736 ; loss 0.48 ; sentence/s 335 ; words/s 17464 ; accuracy train : 80.45596313476562\n",
      "147136 ; loss 0.49 ; sentence/s 332 ; words/s 17837 ; accuracy train : 80.44905090332031\n",
      "153536 ; loss 0.49 ; sentence/s 337 ; words/s 17518 ; accuracy train : 80.4140625\n",
      "159936 ; loss 0.5 ; sentence/s 344 ; words/s 17218 ; accuracy train : 80.4106216430664\n",
      "166336 ; loss 0.48 ; sentence/s 340 ; words/s 17598 ; accuracy train : 80.42548370361328\n",
      "172736 ; loss 0.49 ; sentence/s 342 ; words/s 17635 ; accuracy train : 80.43228912353516\n",
      "179136 ; loss 0.49 ; sentence/s 335 ; words/s 17921 ; accuracy train : 80.43582916259766\n",
      "185536 ; loss 0.47 ; sentence/s 337 ; words/s 17966 ; accuracy train : 80.46012878417969\n",
      "191936 ; loss 0.5 ; sentence/s 336 ; words/s 17797 ; accuracy train : 80.45520782470703\n",
      "198336 ; loss 0.5 ; sentence/s 339 ; words/s 17470 ; accuracy train : 80.43800354003906\n",
      "204736 ; loss 0.52 ; sentence/s 334 ; words/s 17890 ; accuracy train : 80.4130859375\n",
      "211136 ; loss 0.5 ; sentence/s 336 ; words/s 17727 ; accuracy train : 80.39393615722656\n",
      "217536 ; loss 0.49 ; sentence/s 336 ; words/s 17630 ; accuracy train : 80.38235473632812\n",
      "223936 ; loss 0.49 ; sentence/s 336 ; words/s 17647 ; accuracy train : 80.3919677734375\n",
      "230336 ; loss 0.49 ; sentence/s 338 ; words/s 17713 ; accuracy train : 80.38541412353516\n",
      "236736 ; loss 0.48 ; sentence/s 336 ; words/s 17560 ; accuracy train : 80.39949035644531\n",
      "243136 ; loss 0.5 ; sentence/s 335 ; words/s 17717 ; accuracy train : 80.38938903808594\n",
      "249536 ; loss 0.48 ; sentence/s 334 ; words/s 17778 ; accuracy train : 80.39183044433594\n",
      "255936 ; loss 0.5 ; sentence/s 338 ; words/s 17454 ; accuracy train : 80.39375305175781\n",
      "262336 ; loss 0.51 ; sentence/s 332 ; words/s 18035 ; accuracy train : 80.37118530273438\n",
      "268736 ; loss 0.48 ; sentence/s 338 ; words/s 17512 ; accuracy train : 80.38727569580078\n",
      "275136 ; loss 0.49 ; sentence/s 333 ; words/s 17791 ; accuracy train : 80.3877182006836\n",
      "281536 ; loss 0.5 ; sentence/s 340 ; words/s 17567 ; accuracy train : 80.3874282836914\n",
      "287936 ; loss 0.49 ; sentence/s 337 ; words/s 17495 ; accuracy train : 80.390625\n",
      "294336 ; loss 0.48 ; sentence/s 339 ; words/s 17628 ; accuracy train : 80.39164733886719\n",
      "300736 ; loss 0.49 ; sentence/s 338 ; words/s 17562 ; accuracy train : 80.38697052001953\n",
      "307136 ; loss 0.48 ; sentence/s 337 ; words/s 17706 ; accuracy train : 80.41178131103516\n",
      "313536 ; loss 0.48 ; sentence/s 336 ; words/s 17585 ; accuracy train : 80.4269790649414\n",
      "319936 ; loss 0.48 ; sentence/s 333 ; words/s 17990 ; accuracy train : 80.4437484741211\n",
      "326336 ; loss 0.48 ; sentence/s 334 ; words/s 17676 ; accuracy train : 80.45220947265625\n",
      "332736 ; loss 0.49 ; sentence/s 335 ; words/s 17645 ; accuracy train : 80.44651794433594\n",
      "339136 ; loss 0.49 ; sentence/s 334 ; words/s 17795 ; accuracy train : 80.44486999511719\n",
      "345536 ; loss 0.49 ; sentence/s 335 ; words/s 17630 ; accuracy train : 80.45052337646484\n",
      "351936 ; loss 0.5 ; sentence/s 337 ; words/s 17587 ; accuracy train : 80.44829559326172\n",
      "358336 ; loss 0.49 ; sentence/s 327 ; words/s 18121 ; accuracy train : 80.43610382080078\n",
      "364736 ; loss 0.49 ; sentence/s 339 ; words/s 17395 ; accuracy train : 80.43448638916016\n",
      "371136 ; loss 0.49 ; sentence/s 334 ; words/s 17981 ; accuracy train : 80.44207763671875\n",
      "377536 ; loss 0.49 ; sentence/s 339 ; words/s 17499 ; accuracy train : 80.44332885742188\n",
      "383936 ; loss 0.49 ; sentence/s 342 ; words/s 17736 ; accuracy train : 80.44609069824219\n",
      "390336 ; loss 0.48 ; sentence/s 335 ; words/s 17874 ; accuracy train : 80.4505615234375\n",
      "396736 ; loss 0.48 ; sentence/s 339 ; words/s 17465 ; accuracy train : 80.45917510986328\n",
      "403136 ; loss 0.48 ; sentence/s 340 ; words/s 17668 ; accuracy train : 80.4623031616211\n",
      "409536 ; loss 0.48 ; sentence/s 340 ; words/s 17443 ; accuracy train : 80.477294921875\n",
      "415936 ; loss 0.5 ; sentence/s 337 ; words/s 17782 ; accuracy train : 80.46562194824219\n",
      "422336 ; loss 0.5 ; sentence/s 340 ; words/s 17589 ; accuracy train : 80.45572662353516\n",
      "428736 ; loss 0.51 ; sentence/s 335 ; words/s 17866 ; accuracy train : 80.44612884521484\n",
      "435136 ; loss 0.5 ; sentence/s 335 ; words/s 17630 ; accuracy train : 80.44186401367188\n",
      "441536 ; loss 0.5 ; sentence/s 335 ; words/s 17644 ; accuracy train : 80.43614196777344\n",
      "447936 ; loss 0.49 ; sentence/s 337 ; words/s 17533 ; accuracy train : 80.43682861328125\n",
      "454336 ; loss 0.5 ; sentence/s 334 ; words/s 17494 ; accuracy train : 80.4375\n",
      "460736 ; loss 0.47 ; sentence/s 334 ; words/s 17703 ; accuracy train : 80.447265625\n",
      "467136 ; loss 0.5 ; sentence/s 331 ; words/s 17883 ; accuracy train : 80.4419937133789\n",
      "473536 ; loss 0.48 ; sentence/s 337 ; words/s 17671 ; accuracy train : 80.45439147949219\n",
      "479936 ; loss 0.49 ; sentence/s 338 ; words/s 17418 ; accuracy train : 80.46083068847656\n",
      "486336 ; loss 0.48 ; sentence/s 336 ; words/s 17552 ; accuracy train : 80.4673080444336\n",
      "492736 ; loss 0.5 ; sentence/s 338 ; words/s 17659 ; accuracy train : 80.4608383178711\n",
      "499136 ; loss 0.49 ; sentence/s 335 ; words/s 17678 ; accuracy train : 80.45953369140625\n",
      "505536 ; loss 0.49 ; sentence/s 335 ; words/s 17513 ; accuracy train : 80.45767211914062\n",
      "511936 ; loss 0.5 ; sentence/s 337 ; words/s 17696 ; accuracy train : 80.44921875\n",
      "518336 ; loss 0.5 ; sentence/s 334 ; words/s 17797 ; accuracy train : 80.44136047363281\n",
      "524736 ; loss 0.49 ; sentence/s 338 ; words/s 17481 ; accuracy train : 80.4405517578125\n",
      "531136 ; loss 0.49 ; sentence/s 336 ; words/s 17702 ; accuracy train : 80.4412612915039\n",
      "537536 ; loss 0.5 ; sentence/s 335 ; words/s 17755 ; accuracy train : 80.44047546386719\n",
      "543936 ; loss 0.5 ; sentence/s 337 ; words/s 17586 ; accuracy train : 80.43804931640625\n",
      "results : epoch 15 ; mean accuracy train : 80.4409408569336\n",
      "\n",
      "VALIDATION : Epoch 15\n",
      "togrep : results : epoch 15 ; mean accuracy valid :              77.96179962158203\n",
      "saving model at epoch 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 16\n",
      "Learning rate : 0.0034402334185651535\n",
      "6336 ; loss 0.49 ; sentence/s 334 ; words/s 17517 ; accuracy train : 80.390625\n",
      "12736 ; loss 0.49 ; sentence/s 335 ; words/s 17485 ; accuracy train : 80.609375\n",
      "19136 ; loss 0.49 ; sentence/s 337 ; words/s 17784 ; accuracy train : 80.515625\n",
      "25536 ; loss 0.5 ; sentence/s 338 ; words/s 17611 ; accuracy train : 80.46484375\n",
      "31936 ; loss 0.48 ; sentence/s 342 ; words/s 17423 ; accuracy train : 80.56562805175781\n",
      "38336 ; loss 0.49 ; sentence/s 337 ; words/s 17440 ; accuracy train : 80.57291412353516\n",
      "44736 ; loss 0.49 ; sentence/s 338 ; words/s 17672 ; accuracy train : 80.46205139160156\n",
      "51136 ; loss 0.49 ; sentence/s 336 ; words/s 17469 ; accuracy train : 80.470703125\n",
      "57536 ; loss 0.49 ; sentence/s 339 ; words/s 17521 ; accuracy train : 80.42361450195312\n",
      "63936 ; loss 0.49 ; sentence/s 338 ; words/s 17518 ; accuracy train : 80.47969055175781\n",
      "70336 ; loss 0.48 ; sentence/s 335 ; words/s 17755 ; accuracy train : 80.515625\n",
      "76736 ; loss 0.49 ; sentence/s 336 ; words/s 17836 ; accuracy train : 80.52864837646484\n",
      "83136 ; loss 0.49 ; sentence/s 337 ; words/s 17572 ; accuracy train : 80.52404022216797\n",
      "89536 ; loss 0.49 ; sentence/s 337 ; words/s 17442 ; accuracy train : 80.51451110839844\n",
      "95936 ; loss 0.48 ; sentence/s 338 ; words/s 17678 ; accuracy train : 80.56145477294922\n",
      "102336 ; loss 0.5 ; sentence/s 335 ; words/s 17998 ; accuracy train : 80.5126953125\n",
      "108736 ; loss 0.5 ; sentence/s 334 ; words/s 17801 ; accuracy train : 80.46691131591797\n",
      "115136 ; loss 0.49 ; sentence/s 342 ; words/s 17537 ; accuracy train : 80.46527862548828\n",
      "121536 ; loss 0.49 ; sentence/s 340 ; words/s 17553 ; accuracy train : 80.44983673095703\n",
      "127936 ; loss 0.49 ; sentence/s 332 ; words/s 17782 ; accuracy train : 80.453125\n",
      "134336 ; loss 0.49 ; sentence/s 337 ; words/s 17822 ; accuracy train : 80.4360122680664\n",
      "140736 ; loss 0.48 ; sentence/s 333 ; words/s 18078 ; accuracy train : 80.47869110107422\n",
      "147136 ; loss 0.49 ; sentence/s 337 ; words/s 17426 ; accuracy train : 80.49320983886719\n",
      "153536 ; loss 0.49 ; sentence/s 334 ; words/s 17734 ; accuracy train : 80.51236724853516\n",
      "159936 ; loss 0.48 ; sentence/s 338 ; words/s 17624 ; accuracy train : 80.52562713623047\n",
      "166336 ; loss 0.5 ; sentence/s 331 ; words/s 17828 ; accuracy train : 80.50360870361328\n",
      "172736 ; loss 0.5 ; sentence/s 341 ; words/s 17485 ; accuracy train : 80.48206329345703\n",
      "179136 ; loss 0.48 ; sentence/s 334 ; words/s 17951 ; accuracy train : 80.50334930419922\n",
      "185536 ; loss 0.5 ; sentence/s 337 ; words/s 17718 ; accuracy train : 80.4542007446289\n",
      "191936 ; loss 0.49 ; sentence/s 340 ; words/s 17637 ; accuracy train : 80.43854522705078\n",
      "198336 ; loss 0.49 ; sentence/s 339 ; words/s 17649 ; accuracy train : 80.43548583984375\n",
      "204736 ; loss 0.49 ; sentence/s 337 ; words/s 17643 ; accuracy train : 80.4404296875\n",
      "211136 ; loss 0.49 ; sentence/s 338 ; words/s 17534 ; accuracy train : 80.45549011230469\n",
      "217536 ; loss 0.49 ; sentence/s 340 ; words/s 17430 ; accuracy train : 80.44715118408203\n",
      "223936 ; loss 0.5 ; sentence/s 336 ; words/s 17517 ; accuracy train : 80.42232513427734\n",
      "230336 ; loss 0.49 ; sentence/s 334 ; words/s 17547 ; accuracy train : 80.42881774902344\n",
      "236736 ; loss 0.48 ; sentence/s 335 ; words/s 17731 ; accuracy train : 80.43074035644531\n",
      "243136 ; loss 0.5 ; sentence/s 339 ; words/s 17678 ; accuracy train : 80.42105102539062\n",
      "249536 ; loss 0.49 ; sentence/s 334 ; words/s 17835 ; accuracy train : 80.42988586425781\n",
      "255936 ; loss 0.49 ; sentence/s 339 ; words/s 17529 ; accuracy train : 80.4296875\n",
      "262336 ; loss 0.49 ; sentence/s 335 ; words/s 17721 ; accuracy train : 80.43331146240234\n",
      "268736 ; loss 0.5 ; sentence/s 333 ; words/s 18129 ; accuracy train : 80.41927337646484\n",
      "275136 ; loss 0.49 ; sentence/s 320 ; words/s 17239 ; accuracy train : 80.42115020751953\n",
      "281536 ; loss 0.49 ; sentence/s 329 ; words/s 17444 ; accuracy train : 80.41442108154297\n",
      "287936 ; loss 0.48 ; sentence/s 337 ; words/s 17411 ; accuracy train : 80.41909790039062\n",
      "294336 ; loss 0.49 ; sentence/s 327 ; words/s 17655 ; accuracy train : 80.41236114501953\n",
      "300736 ; loss 0.48 ; sentence/s 336 ; words/s 17394 ; accuracy train : 80.4148941040039\n",
      "307136 ; loss 0.48 ; sentence/s 333 ; words/s 17702 ; accuracy train : 80.41829681396484\n",
      "313536 ; loss 0.49 ; sentence/s 329 ; words/s 17889 ; accuracy train : 80.41422271728516\n",
      "319936 ; loss 0.5 ; sentence/s 335 ; words/s 17508 ; accuracy train : 80.40406036376953\n",
      "326336 ; loss 0.49 ; sentence/s 335 ; words/s 17478 ; accuracy train : 80.40839385986328\n",
      "332736 ; loss 0.48 ; sentence/s 333 ; words/s 17572 ; accuracy train : 80.40955352783203\n",
      "339136 ; loss 0.51 ; sentence/s 336 ; words/s 17535 ; accuracy train : 80.39769744873047\n",
      "345536 ; loss 0.51 ; sentence/s 336 ; words/s 17483 ; accuracy train : 80.39004516601562\n",
      "351936 ; loss 0.48 ; sentence/s 328 ; words/s 18040 ; accuracy train : 80.39517211914062\n",
      "358336 ; loss 0.48 ; sentence/s 332 ; words/s 17737 ; accuracy train : 80.40569305419922\n",
      "364736 ; loss 0.49 ; sentence/s 330 ; words/s 17778 ; accuracy train : 80.4243392944336\n",
      "371136 ; loss 0.48 ; sentence/s 337 ; words/s 17689 ; accuracy train : 80.43103790283203\n",
      "377536 ; loss 0.5 ; sentence/s 336 ; words/s 17366 ; accuracy train : 80.4234619140625\n",
      "383936 ; loss 0.48 ; sentence/s 334 ; words/s 17690 ; accuracy train : 80.43099212646484\n",
      "390336 ; loss 0.5 ; sentence/s 333 ; words/s 17706 ; accuracy train : 80.42289733886719\n",
      "396736 ; loss 0.49 ; sentence/s 335 ; words/s 17467 ; accuracy train : 80.43245697021484\n",
      "403136 ; loss 0.49 ; sentence/s 330 ; words/s 17623 ; accuracy train : 80.43799591064453\n",
      "409536 ; loss 0.47 ; sentence/s 337 ; words/s 17594 ; accuracy train : 80.45703125\n",
      "415936 ; loss 0.48 ; sentence/s 334 ; words/s 17473 ; accuracy train : 80.46514129638672\n",
      "422336 ; loss 0.5 ; sentence/s 339 ; words/s 17465 ; accuracy train : 80.4524154663086\n",
      "428736 ; loss 0.5 ; sentence/s 336 ; words/s 17583 ; accuracy train : 80.44705963134766\n",
      "435136 ; loss 0.49 ; sentence/s 337 ; words/s 17396 ; accuracy train : 80.44347381591797\n",
      "441536 ; loss 0.48 ; sentence/s 333 ; words/s 17594 ; accuracy train : 80.43953704833984\n",
      "447936 ; loss 0.5 ; sentence/s 332 ; words/s 18028 ; accuracy train : 80.43281555175781\n",
      "454336 ; loss 0.49 ; sentence/s 337 ; words/s 17470 ; accuracy train : 80.43265533447266\n",
      "460736 ; loss 0.49 ; sentence/s 334 ; words/s 17531 ; accuracy train : 80.42729949951172\n",
      "467136 ; loss 0.49 ; sentence/s 337 ; words/s 17745 ; accuracy train : 80.43150329589844\n",
      "473536 ; loss 0.48 ; sentence/s 337 ; words/s 17385 ; accuracy train : 80.43939971923828\n",
      "479936 ; loss 0.48 ; sentence/s 335 ; words/s 17417 ; accuracy train : 80.44833374023438\n",
      "486336 ; loss 0.48 ; sentence/s 339 ; words/s 17741 ; accuracy train : 80.45436096191406\n",
      "492736 ; loss 0.49 ; sentence/s 327 ; words/s 16778 ; accuracy train : 80.45677947998047\n",
      "499136 ; loss 0.5 ; sentence/s 333 ; words/s 17197 ; accuracy train : 80.45513153076172\n",
      "505536 ; loss 0.49 ; sentence/s 335 ; words/s 17494 ; accuracy train : 80.4608383178711\n",
      "511936 ; loss 0.5 ; sentence/s 336 ; words/s 17519 ; accuracy train : 80.4599609375\n",
      "518336 ; loss 0.48 ; sentence/s 336 ; words/s 17431 ; accuracy train : 80.46643829345703\n",
      "524736 ; loss 0.49 ; sentence/s 335 ; words/s 17521 ; accuracy train : 80.46055603027344\n",
      "531136 ; loss 0.47 ; sentence/s 336 ; words/s 17620 ; accuracy train : 80.4674301147461\n",
      "537536 ; loss 0.49 ; sentence/s 335 ; words/s 17575 ; accuracy train : 80.46559143066406\n",
      "543936 ; loss 0.49 ; sentence/s 335 ; words/s 17623 ; accuracy train : 80.46543884277344\n",
      "results : epoch 16 ; mean accuracy train : 80.4720687866211\n",
      "\n",
      "VALIDATION : Epoch 16\n",
      "togrep : results : epoch 16 ; mean accuracy valid :              77.98211669921875\n",
      "saving model at epoch 16\n",
      "\n",
      "TRAINING : Epoch 17\n",
      "Learning rate : 0.003405831084379502\n",
      "6336 ; loss 0.5 ; sentence/s 333 ; words/s 17425 ; accuracy train : 79.625\n",
      "12736 ; loss 0.5 ; sentence/s 339 ; words/s 17397 ; accuracy train : 79.8671875\n",
      "19136 ; loss 0.5 ; sentence/s 338 ; words/s 17443 ; accuracy train : 79.83853912353516\n",
      "25536 ; loss 0.48 ; sentence/s 336 ; words/s 17314 ; accuracy train : 79.98046875\n",
      "31936 ; loss 0.49 ; sentence/s 331 ; words/s 17872 ; accuracy train : 80.0718765258789\n",
      "38336 ; loss 0.49 ; sentence/s 333 ; words/s 17767 ; accuracy train : 80.12760162353516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44736 ; loss 0.47 ; sentence/s 338 ; words/s 17612 ; accuracy train : 80.31696319580078\n",
      "51136 ; loss 0.49 ; sentence/s 338 ; words/s 17386 ; accuracy train : 80.333984375\n",
      "57536 ; loss 0.5 ; sentence/s 332 ; words/s 17710 ; accuracy train : 80.22048950195312\n",
      "63936 ; loss 0.48 ; sentence/s 329 ; words/s 17756 ; accuracy train : 80.265625\n",
      "70336 ; loss 0.5 ; sentence/s 334 ; words/s 17671 ; accuracy train : 80.20454406738281\n",
      "76736 ; loss 0.48 ; sentence/s 331 ; words/s 17658 ; accuracy train : 80.27083587646484\n",
      "83136 ; loss 0.49 ; sentence/s 332 ; words/s 17660 ; accuracy train : 80.28966522216797\n",
      "89536 ; loss 0.48 ; sentence/s 336 ; words/s 17780 ; accuracy train : 80.34598541259766\n",
      "95936 ; loss 0.47 ; sentence/s 335 ; words/s 17717 ; accuracy train : 80.38541412353516\n",
      "102336 ; loss 0.48 ; sentence/s 330 ; words/s 17807 ; accuracy train : 80.404296875\n",
      "108736 ; loss 0.5 ; sentence/s 332 ; words/s 17389 ; accuracy train : 80.37224578857422\n",
      "115136 ; loss 0.49 ; sentence/s 330 ; words/s 16894 ; accuracy train : 80.40103912353516\n",
      "121536 ; loss 0.48 ; sentence/s 330 ; words/s 17326 ; accuracy train : 80.42022705078125\n",
      "127936 ; loss 0.49 ; sentence/s 332 ; words/s 17523 ; accuracy train : 80.41796875\n",
      "134336 ; loss 0.5 ; sentence/s 335 ; words/s 17849 ; accuracy train : 80.39881134033203\n",
      "140736 ; loss 0.5 ; sentence/s 332 ; words/s 17844 ; accuracy train : 80.3757095336914\n",
      "147136 ; loss 0.48 ; sentence/s 335 ; words/s 17407 ; accuracy train : 80.38926696777344\n",
      "153536 ; loss 0.49 ; sentence/s 335 ; words/s 17670 ; accuracy train : 80.39517974853516\n",
      "159936 ; loss 0.48 ; sentence/s 332 ; words/s 17810 ; accuracy train : 80.41812133789062\n",
      "166336 ; loss 0.48 ; sentence/s 339 ; words/s 17348 ; accuracy train : 80.44230651855469\n",
      "172736 ; loss 0.5 ; sentence/s 343 ; words/s 17184 ; accuracy train : 80.4375\n",
      "179136 ; loss 0.49 ; sentence/s 336 ; words/s 17704 ; accuracy train : 80.44475555419922\n",
      "185536 ; loss 0.5 ; sentence/s 334 ; words/s 17560 ; accuracy train : 80.44019317626953\n",
      "191936 ; loss 0.49 ; sentence/s 337 ; words/s 17675 ; accuracy train : 80.44895935058594\n",
      "198336 ; loss 0.48 ; sentence/s 336 ; words/s 17730 ; accuracy train : 80.4460678100586\n",
      "204736 ; loss 0.48 ; sentence/s 333 ; words/s 17789 ; accuracy train : 80.4609375\n",
      "211136 ; loss 0.49 ; sentence/s 333 ; words/s 17687 ; accuracy train : 80.45075988769531\n",
      "217536 ; loss 0.46 ; sentence/s 334 ; words/s 17499 ; accuracy train : 80.47794342041016\n",
      "223936 ; loss 0.48 ; sentence/s 336 ; words/s 17376 ; accuracy train : 80.5\n",
      "230336 ; loss 0.49 ; sentence/s 336 ; words/s 17361 ; accuracy train : 80.50824737548828\n",
      "236736 ; loss 0.49 ; sentence/s 334 ; words/s 17861 ; accuracy train : 80.52533721923828\n",
      "243136 ; loss 0.49 ; sentence/s 341 ; words/s 17497 ; accuracy train : 80.52384948730469\n",
      "249536 ; loss 0.48 ; sentence/s 336 ; words/s 17696 ; accuracy train : 80.53685760498047\n",
      "255936 ; loss 0.49 ; sentence/s 336 ; words/s 17556 ; accuracy train : 80.5171890258789\n",
      "262336 ; loss 0.5 ; sentence/s 337 ; words/s 17699 ; accuracy train : 80.48246765136719\n",
      "268736 ; loss 0.5 ; sentence/s 339 ; words/s 17627 ; accuracy train : 80.48028564453125\n",
      "275136 ; loss 0.49 ; sentence/s 334 ; words/s 17773 ; accuracy train : 80.48873901367188\n",
      "281536 ; loss 0.48 ; sentence/s 332 ; words/s 17840 ; accuracy train : 80.49502563476562\n",
      "287936 ; loss 0.49 ; sentence/s 335 ; words/s 17558 ; accuracy train : 80.49028015136719\n",
      "294336 ; loss 0.49 ; sentence/s 334 ; words/s 17507 ; accuracy train : 80.4955825805664\n",
      "300736 ; loss 0.49 ; sentence/s 335 ; words/s 17476 ; accuracy train : 80.49268341064453\n",
      "307136 ; loss 0.5 ; sentence/s 332 ; words/s 17613 ; accuracy train : 80.48177337646484\n",
      "313536 ; loss 0.48 ; sentence/s 336 ; words/s 17701 ; accuracy train : 80.50127410888672\n",
      "319936 ; loss 0.47 ; sentence/s 340 ; words/s 17619 ; accuracy train : 80.51468658447266\n",
      "326336 ; loss 0.48 ; sentence/s 333 ; words/s 17824 ; accuracy train : 80.5257339477539\n",
      "332736 ; loss 0.49 ; sentence/s 334 ; words/s 17962 ; accuracy train : 80.52283477783203\n",
      "339136 ; loss 0.5 ; sentence/s 335 ; words/s 17758 ; accuracy train : 80.51856994628906\n",
      "345536 ; loss 0.48 ; sentence/s 337 ; words/s 17759 ; accuracy train : 80.52574920654297\n",
      "351936 ; loss 0.49 ; sentence/s 337 ; words/s 17754 ; accuracy train : 80.52130889892578\n",
      "358336 ; loss 0.51 ; sentence/s 338 ; words/s 17684 ; accuracy train : 80.494140625\n",
      "364736 ; loss 0.5 ; sentence/s 335 ; words/s 17840 ; accuracy train : 80.49122619628906\n",
      "371136 ; loss 0.49 ; sentence/s 337 ; words/s 17802 ; accuracy train : 80.49137878417969\n",
      "377536 ; loss 0.49 ; sentence/s 336 ; words/s 17751 ; accuracy train : 80.48887634277344\n",
      "383936 ; loss 0.5 ; sentence/s 336 ; words/s 17678 ; accuracy train : 80.49401092529297\n",
      "390336 ; loss 0.49 ; sentence/s 338 ; words/s 17512 ; accuracy train : 80.48796081542969\n",
      "396736 ; loss 0.49 ; sentence/s 339 ; words/s 17420 ; accuracy train : 80.48185729980469\n",
      "403136 ; loss 0.49 ; sentence/s 337 ; words/s 17548 ; accuracy train : 80.48685455322266\n",
      "409536 ; loss 0.5 ; sentence/s 338 ; words/s 17710 ; accuracy train : 80.480224609375\n",
      "415936 ; loss 0.49 ; sentence/s 331 ; words/s 17836 ; accuracy train : 80.48485565185547\n",
      "422336 ; loss 0.48 ; sentence/s 338 ; words/s 17509 ; accuracy train : 80.49124145507812\n",
      "428736 ; loss 0.49 ; sentence/s 337 ; words/s 17629 ; accuracy train : 80.4890365600586\n",
      "435136 ; loss 0.5 ; sentence/s 340 ; words/s 17559 ; accuracy train : 80.4820785522461\n",
      "441536 ; loss 0.48 ; sentence/s 336 ; words/s 17435 ; accuracy train : 80.48844909667969\n",
      "447936 ; loss 0.49 ; sentence/s 335 ; words/s 18034 ; accuracy train : 80.48794555664062\n",
      "454336 ; loss 0.49 ; sentence/s 338 ; words/s 17605 ; accuracy train : 80.49361419677734\n",
      "460736 ; loss 0.48 ; sentence/s 338 ; words/s 17618 ; accuracy train : 80.49869537353516\n",
      "467136 ; loss 0.5 ; sentence/s 339 ; words/s 17550 ; accuracy train : 80.48950958251953\n",
      "473536 ; loss 0.51 ; sentence/s 339 ; words/s 17615 ; accuracy train : 80.47529602050781\n",
      "479936 ; loss 0.48 ; sentence/s 338 ; words/s 17752 ; accuracy train : 80.4745864868164\n",
      "486336 ; loss 0.49 ; sentence/s 339 ; words/s 17737 ; accuracy train : 80.47553253173828\n",
      "492736 ; loss 0.49 ; sentence/s 334 ; words/s 17716 ; accuracy train : 80.48031616210938\n",
      "499136 ; loss 0.49 ; sentence/s 337 ; words/s 17724 ; accuracy train : 80.47595977783203\n",
      "505536 ; loss 0.5 ; sentence/s 333 ; words/s 17429 ; accuracy train : 80.47606658935547\n",
      "511936 ; loss 0.5 ; sentence/s 335 ; words/s 17227 ; accuracy train : 80.47441101074219\n",
      "518336 ; loss 0.47 ; sentence/s 331 ; words/s 17410 ; accuracy train : 80.4824447631836\n",
      "524736 ; loss 0.48 ; sentence/s 319 ; words/s 16221 ; accuracy train : 80.49142456054688\n",
      "531136 ; loss 0.49 ; sentence/s 315 ; words/s 16650 ; accuracy train : 80.4954833984375\n",
      "537536 ; loss 0.49 ; sentence/s 317 ; words/s 16564 ; accuracy train : 80.48921203613281\n",
      "543936 ; loss 0.5 ; sentence/s 316 ; words/s 16129 ; accuracy train : 80.48621368408203\n",
      "results : epoch 17 ; mean accuracy train : 80.49372863769531\n",
      "\n",
      "VALIDATION : Epoch 17\n",
      "togrep : results : epoch 17 ; mean accuracy valid :              77.97195434570312\n",
      "Shrinking lr by : 5. New lr = 0.0006811662168759004\n",
      "\n",
      "TRAINING : Epoch 18\n",
      "Learning rate : 0.0006743545547071414\n",
      "6336 ; loss 0.5 ; sentence/s 317 ; words/s 16573 ; accuracy train : 79.46875\n",
      "12736 ; loss 0.5 ; sentence/s 323 ; words/s 16755 ; accuracy train : 80.15625\n",
      "19136 ; loss 0.49 ; sentence/s 332 ; words/s 17405 ; accuracy train : 80.11978912353516\n",
      "25536 ; loss 0.51 ; sentence/s 332 ; words/s 17134 ; accuracy train : 80.08984375\n",
      "31936 ; loss 0.49 ; sentence/s 333 ; words/s 17026 ; accuracy train : 80.125\n",
      "38336 ; loss 0.48 ; sentence/s 310 ; words/s 16246 ; accuracy train : 80.15364837646484\n",
      "44736 ; loss 0.48 ; sentence/s 335 ; words/s 17313 ; accuracy train : 80.23213958740234\n",
      "51136 ; loss 0.49 ; sentence/s 335 ; words/s 17545 ; accuracy train : 80.240234375\n",
      "57536 ; loss 0.5 ; sentence/s 338 ; words/s 17701 ; accuracy train : 80.19965362548828\n",
      "63936 ; loss 0.49 ; sentence/s 336 ; words/s 17462 ; accuracy train : 80.3109359741211\n",
      "70336 ; loss 0.5 ; sentence/s 338 ; words/s 17554 ; accuracy train : 80.2414779663086\n",
      "76736 ; loss 0.47 ; sentence/s 338 ; words/s 17591 ; accuracy train : 80.31380462646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83136 ; loss 0.49 ; sentence/s 337 ; words/s 17591 ; accuracy train : 80.34615325927734\n",
      "89536 ; loss 0.5 ; sentence/s 332 ; words/s 17795 ; accuracy train : 80.33258819580078\n",
      "95936 ; loss 0.49 ; sentence/s 332 ; words/s 17601 ; accuracy train : 80.35520935058594\n",
      "102336 ; loss 0.5 ; sentence/s 338 ; words/s 17566 ; accuracy train : 80.33203125\n",
      "108736 ; loss 0.49 ; sentence/s 336 ; words/s 17676 ; accuracy train : 80.33547973632812\n",
      "115136 ; loss 0.48 ; sentence/s 333 ; words/s 17434 ; accuracy train : 80.3671875\n",
      "121536 ; loss 0.49 ; sentence/s 334 ; words/s 17673 ; accuracy train : 80.37006378173828\n",
      "127936 ; loss 0.5 ; sentence/s 335 ; words/s 17704 ; accuracy train : 80.35469055175781\n",
      "134336 ; loss 0.48 ; sentence/s 339 ; words/s 17242 ; accuracy train : 80.41294860839844\n",
      "140736 ; loss 0.51 ; sentence/s 335 ; words/s 17453 ; accuracy train : 80.3671875\n",
      "147136 ; loss 0.49 ; sentence/s 339 ; words/s 17513 ; accuracy train : 80.35869598388672\n",
      "153536 ; loss 0.49 ; sentence/s 333 ; words/s 17667 ; accuracy train : 80.373046875\n",
      "159936 ; loss 0.46 ; sentence/s 339 ; words/s 17413 ; accuracy train : 80.44125366210938\n",
      "166336 ; loss 0.47 ; sentence/s 334 ; words/s 17653 ; accuracy train : 80.46935272216797\n",
      "172736 ; loss 0.5 ; sentence/s 337 ; words/s 17387 ; accuracy train : 80.4415512084961\n",
      "179136 ; loss 0.47 ; sentence/s 332 ; words/s 17392 ; accuracy train : 80.47489166259766\n",
      "185536 ; loss 0.49 ; sentence/s 337 ; words/s 17474 ; accuracy train : 80.47575378417969\n",
      "191936 ; loss 0.49 ; sentence/s 334 ; words/s 17633 ; accuracy train : 80.4828109741211\n",
      "198336 ; loss 0.49 ; sentence/s 335 ; words/s 17697 ; accuracy train : 80.48739624023438\n",
      "204736 ; loss 0.48 ; sentence/s 333 ; words/s 17640 ; accuracy train : 80.4970703125\n",
      "211136 ; loss 0.48 ; sentence/s 337 ; words/s 17434 ; accuracy train : 80.5118408203125\n",
      "217536 ; loss 0.49 ; sentence/s 333 ; words/s 17774 ; accuracy train : 80.51976013183594\n",
      "223936 ; loss 0.5 ; sentence/s 333 ; words/s 17663 ; accuracy train : 80.4924087524414\n",
      "230336 ; loss 0.49 ; sentence/s 334 ; words/s 17759 ; accuracy train : 80.49609375\n",
      "236736 ; loss 0.49 ; sentence/s 334 ; words/s 17404 ; accuracy train : 80.5101318359375\n",
      "243136 ; loss 0.5 ; sentence/s 337 ; words/s 17485 ; accuracy train : 80.49547576904297\n",
      "249536 ; loss 0.49 ; sentence/s 333 ; words/s 17846 ; accuracy train : 80.49559020996094\n",
      "255936 ; loss 0.47 ; sentence/s 337 ; words/s 17320 ; accuracy train : 80.5296859741211\n",
      "262336 ; loss 0.48 ; sentence/s 338 ; words/s 17603 ; accuracy train : 80.52781677246094\n",
      "268736 ; loss 0.48 ; sentence/s 336 ; words/s 17420 ; accuracy train : 80.53720092773438\n",
      "275136 ; loss 0.48 ; sentence/s 333 ; words/s 17631 ; accuracy train : 80.5337905883789\n",
      "281536 ; loss 0.48 ; sentence/s 341 ; words/s 17393 ; accuracy train : 80.54332733154297\n",
      "287936 ; loss 0.49 ; sentence/s 339 ; words/s 17254 ; accuracy train : 80.5406265258789\n",
      "294336 ; loss 0.5 ; sentence/s 334 ; words/s 17890 ; accuracy train : 80.5394058227539\n",
      "300736 ; loss 0.48 ; sentence/s 335 ; words/s 17676 ; accuracy train : 80.54920196533203\n",
      "307136 ; loss 0.48 ; sentence/s 335 ; words/s 17594 ; accuracy train : 80.55013275146484\n",
      "313536 ; loss 0.49 ; sentence/s 337 ; words/s 17386 ; accuracy train : 80.55070495605469\n",
      "319936 ; loss 0.49 ; sentence/s 334 ; words/s 17281 ; accuracy train : 80.55155944824219\n",
      "326336 ; loss 0.49 ; sentence/s 324 ; words/s 17133 ; accuracy train : 80.54718017578125\n",
      "332736 ; loss 0.51 ; sentence/s 335 ; words/s 17226 ; accuracy train : 80.51982879638672\n",
      "339136 ; loss 0.49 ; sentence/s 333 ; words/s 17508 ; accuracy train : 80.51739501953125\n",
      "345536 ; loss 0.5 ; sentence/s 333 ; words/s 17716 ; accuracy train : 80.51417541503906\n",
      "351936 ; loss 0.5 ; sentence/s 334 ; words/s 17569 ; accuracy train : 80.48834991455078\n",
      "358336 ; loss 0.49 ; sentence/s 329 ; words/s 17301 ; accuracy train : 80.49107360839844\n",
      "364736 ; loss 0.49 ; sentence/s 330 ; words/s 17573 ; accuracy train : 80.49671173095703\n",
      "371136 ; loss 0.48 ; sentence/s 326 ; words/s 17330 ; accuracy train : 80.50942993164062\n",
      "377536 ; loss 0.48 ; sentence/s 335 ; words/s 17265 ; accuracy train : 80.52277374267578\n",
      "383936 ; loss 0.49 ; sentence/s 329 ; words/s 17574 ; accuracy train : 80.5179672241211\n",
      "390336 ; loss 0.5 ; sentence/s 331 ; words/s 17384 ; accuracy train : 80.5099868774414\n",
      "396736 ; loss 0.5 ; sentence/s 332 ; words/s 17731 ; accuracy train : 80.49748229980469\n",
      "403136 ; loss 0.49 ; sentence/s 331 ; words/s 17459 ; accuracy train : 80.50173950195312\n",
      "409536 ; loss 0.48 ; sentence/s 328 ; words/s 17427 ; accuracy train : 80.508544921875\n",
      "415936 ; loss 0.51 ; sentence/s 334 ; words/s 17553 ; accuracy train : 80.49735260009766\n",
      "422336 ; loss 0.49 ; sentence/s 329 ; words/s 17642 ; accuracy train : 80.50141906738281\n",
      "428736 ; loss 0.48 ; sentence/s 329 ; words/s 16954 ; accuracy train : 80.51189422607422\n",
      "435136 ; loss 0.48 ; sentence/s 330 ; words/s 17544 ; accuracy train : 80.52228546142578\n",
      "441536 ; loss 0.48 ; sentence/s 329 ; words/s 17381 ; accuracy train : 80.52355194091797\n",
      "447936 ; loss 0.5 ; sentence/s 334 ; words/s 17546 ; accuracy train : 80.51875305175781\n",
      "454336 ; loss 0.49 ; sentence/s 325 ; words/s 17100 ; accuracy train : 80.5165023803711\n",
      "460736 ; loss 0.51 ; sentence/s 322 ; words/s 17002 ; accuracy train : 80.50498962402344\n",
      "467136 ; loss 0.49 ; sentence/s 328 ; words/s 17370 ; accuracy train : 80.49978637695312\n",
      "473536 ; loss 0.49 ; sentence/s 335 ; words/s 17163 ; accuracy train : 80.49788665771484\n",
      "479936 ; loss 0.5 ; sentence/s 333 ; words/s 17696 ; accuracy train : 80.49458312988281\n",
      "486336 ; loss 0.48 ; sentence/s 328 ; words/s 17121 ; accuracy train : 80.50225830078125\n",
      "492736 ; loss 0.49 ; sentence/s 329 ; words/s 17536 ; accuracy train : 80.49533081054688\n",
      "499136 ; loss 0.49 ; sentence/s 332 ; words/s 17678 ; accuracy train : 80.49779510498047\n",
      "505536 ; loss 0.49 ; sentence/s 337 ; words/s 17469 ; accuracy train : 80.50079345703125\n",
      "511936 ; loss 0.48 ; sentence/s 333 ; words/s 17538 ; accuracy train : 80.5082015991211\n",
      "518336 ; loss 0.48 ; sentence/s 331 ; words/s 17365 ; accuracy train : 80.51331329345703\n",
      "524736 ; loss 0.5 ; sentence/s 325 ; words/s 16723 ; accuracy train : 80.50038146972656\n",
      "531136 ; loss 0.49 ; sentence/s 326 ; words/s 17259 ; accuracy train : 80.50470733642578\n",
      "537536 ; loss 0.5 ; sentence/s 327 ; words/s 16574 ; accuracy train : 80.49944305419922\n",
      "543936 ; loss 0.48 ; sentence/s 319 ; words/s 16992 ; accuracy train : 80.50386047363281\n",
      "results : epoch 18 ; mean accuracy train : 80.50064849853516\n",
      "\n",
      "VALIDATION : Epoch 18\n",
      "togrep : results : epoch 18 ; mean accuracy valid :              78.03292083740234\n",
      "saving model at epoch 18\n",
      "\n",
      "TRAINING : Epoch 19\n",
      "Learning rate : 0.00066761100916007\n",
      "6336 ; loss 0.48 ; sentence/s 319 ; words/s 16949 ; accuracy train : 81.234375\n",
      "12736 ; loss 0.48 ; sentence/s 324 ; words/s 16964 ; accuracy train : 81.203125\n",
      "19136 ; loss 0.48 ; sentence/s 327 ; words/s 16782 ; accuracy train : 81.09375\n",
      "25536 ; loss 0.5 ; sentence/s 326 ; words/s 17275 ; accuracy train : 80.78515625\n",
      "31936 ; loss 0.48 ; sentence/s 338 ; words/s 17373 ; accuracy train : 80.81562805175781\n",
      "38336 ; loss 0.48 ; sentence/s 331 ; words/s 17876 ; accuracy train : 80.89322662353516\n",
      "44736 ; loss 0.48 ; sentence/s 326 ; words/s 17068 ; accuracy train : 80.92411041259766\n",
      "51136 ; loss 0.49 ; sentence/s 315 ; words/s 16717 ; accuracy train : 80.84765625\n",
      "57536 ; loss 0.48 ; sentence/s 324 ; words/s 16911 ; accuracy train : 80.88888549804688\n",
      "63936 ; loss 0.49 ; sentence/s 324 ; words/s 16722 ; accuracy train : 80.8843765258789\n",
      "70336 ; loss 0.48 ; sentence/s 322 ; words/s 16868 ; accuracy train : 80.875\n",
      "76736 ; loss 0.48 ; sentence/s 320 ; words/s 17019 ; accuracy train : 80.86458587646484\n",
      "83136 ; loss 0.5 ; sentence/s 326 ; words/s 16877 ; accuracy train : 80.78125\n",
      "89536 ; loss 0.49 ; sentence/s 323 ; words/s 16885 ; accuracy train : 80.77455139160156\n",
      "95936 ; loss 0.5 ; sentence/s 321 ; words/s 17048 ; accuracy train : 80.7562484741211\n",
      "102336 ; loss 0.49 ; sentence/s 323 ; words/s 16793 ; accuracy train : 80.744140625\n",
      "108736 ; loss 0.49 ; sentence/s 332 ; words/s 17235 ; accuracy train : 80.76378631591797\n",
      "115136 ; loss 0.48 ; sentence/s 327 ; words/s 16995 ; accuracy train : 80.77951049804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121536 ; loss 0.48 ; sentence/s 324 ; words/s 17301 ; accuracy train : 80.76973724365234\n",
      "127936 ; loss 0.49 ; sentence/s 324 ; words/s 17050 ; accuracy train : 80.75077819824219\n",
      "134336 ; loss 0.49 ; sentence/s 334 ; words/s 17390 ; accuracy train : 80.73958587646484\n",
      "140736 ; loss 0.49 ; sentence/s 330 ; words/s 17221 ; accuracy train : 80.72372436523438\n",
      "147136 ; loss 0.5 ; sentence/s 336 ; words/s 17367 ; accuracy train : 80.69837188720703\n",
      "153536 ; loss 0.49 ; sentence/s 334 ; words/s 17599 ; accuracy train : 80.69075775146484\n",
      "159936 ; loss 0.49 ; sentence/s 332 ; words/s 17410 ; accuracy train : 80.69437408447266\n",
      "166336 ; loss 0.49 ; sentence/s 335 ; words/s 17722 ; accuracy train : 80.70673370361328\n",
      "172736 ; loss 0.49 ; sentence/s 329 ; words/s 17066 ; accuracy train : 80.69791412353516\n",
      "179136 ; loss 0.48 ; sentence/s 321 ; words/s 16763 ; accuracy train : 80.72098541259766\n",
      "185536 ; loss 0.49 ; sentence/s 335 ; words/s 17724 ; accuracy train : 80.70097351074219\n",
      "191936 ; loss 0.47 ; sentence/s 335 ; words/s 17575 ; accuracy train : 80.7328109741211\n",
      "198336 ; loss 0.49 ; sentence/s 321 ; words/s 16701 ; accuracy train : 80.72832489013672\n",
      "204736 ; loss 0.48 ; sentence/s 325 ; words/s 17454 ; accuracy train : 80.73974609375\n",
      "211136 ; loss 0.5 ; sentence/s 332 ; words/s 17126 ; accuracy train : 80.70170593261719\n",
      "217536 ; loss 0.49 ; sentence/s 317 ; words/s 16772 ; accuracy train : 80.6897964477539\n",
      "223936 ; loss 0.5 ; sentence/s 318 ; words/s 16557 ; accuracy train : 80.67901611328125\n",
      "230336 ; loss 0.49 ; sentence/s 321 ; words/s 16506 ; accuracy train : 80.66189575195312\n",
      "236736 ; loss 0.48 ; sentence/s 316 ; words/s 16618 ; accuracy train : 80.68158721923828\n",
      "243136 ; loss 0.5 ; sentence/s 319 ; words/s 16594 ; accuracy train : 80.67269897460938\n",
      "249536 ; loss 0.48 ; sentence/s 317 ; words/s 16647 ; accuracy train : 80.68109130859375\n",
      "255936 ; loss 0.5 ; sentence/s 333 ; words/s 17582 ; accuracy train : 80.66796875\n",
      "262336 ; loss 0.49 ; sentence/s 340 ; words/s 17420 ; accuracy train : 80.65015411376953\n",
      "268736 ; loss 0.49 ; sentence/s 336 ; words/s 17691 ; accuracy train : 80.64099884033203\n",
      "275136 ; loss 0.48 ; sentence/s 333 ; words/s 17635 ; accuracy train : 80.63990020751953\n",
      "281536 ; loss 0.49 ; sentence/s 336 ; words/s 17761 ; accuracy train : 80.63459014892578\n",
      "287936 ; loss 0.48 ; sentence/s 337 ; words/s 17820 ; accuracy train : 80.64305877685547\n",
      "294336 ; loss 0.48 ; sentence/s 336 ; words/s 17517 ; accuracy train : 80.64537811279297\n",
      "300736 ; loss 0.48 ; sentence/s 334 ; words/s 17830 ; accuracy train : 80.65292358398438\n",
      "307136 ; loss 0.49 ; sentence/s 341 ; words/s 17327 ; accuracy train : 80.63736724853516\n",
      "313536 ; loss 0.49 ; sentence/s 336 ; words/s 17587 ; accuracy train : 80.63998413085938\n",
      "319936 ; loss 0.5 ; sentence/s 335 ; words/s 17704 ; accuracy train : 80.62812805175781\n",
      "326336 ; loss 0.51 ; sentence/s 335 ; words/s 17650 ; accuracy train : 80.5943603515625\n",
      "332736 ; loss 0.49 ; sentence/s 333 ; words/s 17762 ; accuracy train : 80.58383178710938\n",
      "339136 ; loss 0.49 ; sentence/s 335 ; words/s 17500 ; accuracy train : 80.57694244384766\n",
      "345536 ; loss 0.49 ; sentence/s 332 ; words/s 17821 ; accuracy train : 80.58043670654297\n",
      "351936 ; loss 0.48 ; sentence/s 335 ; words/s 17526 ; accuracy train : 80.58920288085938\n",
      "358336 ; loss 0.49 ; sentence/s 335 ; words/s 17733 ; accuracy train : 80.58872985839844\n",
      "364736 ; loss 0.5 ; sentence/s 337 ; words/s 17500 ; accuracy train : 80.57127380371094\n",
      "371136 ; loss 0.49 ; sentence/s 336 ; words/s 17448 ; accuracy train : 80.56977081298828\n",
      "377536 ; loss 0.48 ; sentence/s 334 ; words/s 17390 ; accuracy train : 80.57547760009766\n",
      "383936 ; loss 0.48 ; sentence/s 339 ; words/s 17486 ; accuracy train : 80.58020782470703\n",
      "390336 ; loss 0.5 ; sentence/s 338 ; words/s 17532 ; accuracy train : 80.56403350830078\n",
      "396736 ; loss 0.51 ; sentence/s 332 ; words/s 17642 ; accuracy train : 80.54637145996094\n",
      "403136 ; loss 0.49 ; sentence/s 331 ; words/s 17934 ; accuracy train : 80.5376968383789\n",
      "409536 ; loss 0.49 ; sentence/s 332 ; words/s 17872 ; accuracy train : 80.533935546875\n",
      "415936 ; loss 0.5 ; sentence/s 333 ; words/s 17518 ; accuracy train : 80.52043151855469\n",
      "422336 ; loss 0.48 ; sentence/s 336 ; words/s 17591 ; accuracy train : 80.53148651123047\n",
      "428736 ; loss 0.49 ; sentence/s 335 ; words/s 17504 ; accuracy train : 80.52985382080078\n",
      "435136 ; loss 0.48 ; sentence/s 333 ; words/s 17688 ; accuracy train : 80.53240203857422\n",
      "441536 ; loss 0.49 ; sentence/s 334 ; words/s 17579 ; accuracy train : 80.53396606445312\n",
      "447936 ; loss 0.48 ; sentence/s 339 ; words/s 17354 ; accuracy train : 80.54486846923828\n",
      "454336 ; loss 0.49 ; sentence/s 329 ; words/s 17603 ; accuracy train : 80.53961181640625\n",
      "460736 ; loss 0.5 ; sentence/s 338 ; words/s 17548 ; accuracy train : 80.53146362304688\n",
      "467136 ; loss 0.49 ; sentence/s 335 ; words/s 17648 ; accuracy train : 80.52867889404297\n",
      "473536 ; loss 0.48 ; sentence/s 336 ; words/s 17875 ; accuracy train : 80.52660369873047\n",
      "479936 ; loss 0.49 ; sentence/s 335 ; words/s 17882 ; accuracy train : 80.5250015258789\n",
      "486336 ; loss 0.48 ; sentence/s 336 ; words/s 17682 ; accuracy train : 80.53104400634766\n",
      "492736 ; loss 0.5 ; sentence/s 337 ; words/s 17894 ; accuracy train : 80.52495574951172\n",
      "499136 ; loss 0.48 ; sentence/s 337 ; words/s 17693 ; accuracy train : 80.53285217285156\n",
      "505536 ; loss 0.49 ; sentence/s 339 ; words/s 17552 ; accuracy train : 80.53025817871094\n",
      "511936 ; loss 0.5 ; sentence/s 339 ; words/s 17388 ; accuracy train : 80.5240249633789\n",
      "518336 ; loss 0.49 ; sentence/s 337 ; words/s 17726 ; accuracy train : 80.5216064453125\n",
      "524736 ; loss 0.49 ; sentence/s 340 ; words/s 17616 ; accuracy train : 80.51981353759766\n",
      "531136 ; loss 0.49 ; sentence/s 339 ; words/s 17616 ; accuracy train : 80.52183532714844\n",
      "537536 ; loss 0.49 ; sentence/s 334 ; words/s 17772 ; accuracy train : 80.52064514160156\n",
      "543936 ; loss 0.49 ; sentence/s 336 ; words/s 17866 ; accuracy train : 80.52132415771484\n",
      "results : epoch 19 ; mean accuracy train : 80.52267456054688\n",
      "\n",
      "VALIDATION : Epoch 19\n",
      "togrep : results : epoch 19 ; mean accuracy valid :              77.96179962158203\n",
      "Shrinking lr by : 5. New lr = 0.000133522201832014\n",
      "\n",
      "TRAINING : Epoch 20\n",
      "Learning rate : 0.00013218697981369387\n",
      "6336 ; loss 0.5 ; sentence/s 332 ; words/s 17807 ; accuracy train : 80.265625\n",
      "12736 ; loss 0.5 ; sentence/s 342 ; words/s 17361 ; accuracy train : 80.3515625\n",
      "19136 ; loss 0.5 ; sentence/s 333 ; words/s 17886 ; accuracy train : 80.125\n",
      "25536 ; loss 0.49 ; sentence/s 337 ; words/s 17643 ; accuracy train : 80.15625\n",
      "31936 ; loss 0.49 ; sentence/s 343 ; words/s 17511 ; accuracy train : 80.390625\n",
      "38336 ; loss 0.48 ; sentence/s 340 ; words/s 17790 ; accuracy train : 80.48697662353516\n",
      "44736 ; loss 0.48 ; sentence/s 337 ; words/s 17873 ; accuracy train : 80.61161041259766\n",
      "51136 ; loss 0.47 ; sentence/s 339 ; words/s 17698 ; accuracy train : 80.6953125\n",
      "57536 ; loss 0.49 ; sentence/s 338 ; words/s 17827 ; accuracy train : 80.640625\n",
      "63936 ; loss 0.49 ; sentence/s 339 ; words/s 17642 ; accuracy train : 80.703125\n",
      "70336 ; loss 0.48 ; sentence/s 336 ; words/s 17988 ; accuracy train : 80.72869110107422\n",
      "76736 ; loss 0.49 ; sentence/s 335 ; words/s 17887 ; accuracy train : 80.72265625\n",
      "83136 ; loss 0.48 ; sentence/s 336 ; words/s 17762 ; accuracy train : 80.72115325927734\n",
      "89536 ; loss 0.49 ; sentence/s 336 ; words/s 17780 ; accuracy train : 80.70647430419922\n",
      "95936 ; loss 0.5 ; sentence/s 337 ; words/s 17695 ; accuracy train : 80.6812515258789\n",
      "102336 ; loss 0.49 ; sentence/s 339 ; words/s 17616 ; accuracy train : 80.6435546875\n",
      "108736 ; loss 0.5 ; sentence/s 339 ; words/s 17691 ; accuracy train : 80.60294342041016\n",
      "115136 ; loss 0.49 ; sentence/s 337 ; words/s 17505 ; accuracy train : 80.62326049804688\n",
      "121536 ; loss 0.48 ; sentence/s 337 ; words/s 17839 ; accuracy train : 80.6069107055664\n",
      "127936 ; loss 0.5 ; sentence/s 336 ; words/s 17617 ; accuracy train : 80.55078125\n",
      "134336 ; loss 0.5 ; sentence/s 337 ; words/s 17295 ; accuracy train : 80.51190185546875\n",
      "140736 ; loss 0.49 ; sentence/s 333 ; words/s 17742 ; accuracy train : 80.50639343261719\n",
      "147136 ; loss 0.48 ; sentence/s 337 ; words/s 17690 ; accuracy train : 80.52037811279297\n",
      "153536 ; loss 0.48 ; sentence/s 334 ; words/s 17963 ; accuracy train : 80.52278900146484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159936 ; loss 0.49 ; sentence/s 341 ; words/s 17452 ; accuracy train : 80.53312683105469\n",
      "166336 ; loss 0.49 ; sentence/s 340 ; words/s 17671 ; accuracy train : 80.52884674072266\n",
      "172736 ; loss 0.48 ; sentence/s 338 ; words/s 17620 ; accuracy train : 80.54513549804688\n",
      "179136 ; loss 0.49 ; sentence/s 339 ; words/s 17538 ; accuracy train : 80.55245208740234\n",
      "185536 ; loss 0.48 ; sentence/s 338 ; words/s 17672 ; accuracy train : 80.5792007446289\n",
      "191936 ; loss 0.49 ; sentence/s 337 ; words/s 17654 ; accuracy train : 80.55677032470703\n",
      "198336 ; loss 0.48 ; sentence/s 339 ; words/s 17540 ; accuracy train : 80.5826644897461\n",
      "204736 ; loss 0.48 ; sentence/s 333 ; words/s 18064 ; accuracy train : 80.5927734375\n",
      "211136 ; loss 0.49 ; sentence/s 337 ; words/s 17673 ; accuracy train : 80.59091186523438\n",
      "217536 ; loss 0.49 ; sentence/s 335 ; words/s 17924 ; accuracy train : 80.59788513183594\n",
      "223936 ; loss 0.49 ; sentence/s 336 ; words/s 17740 ; accuracy train : 80.60044860839844\n",
      "230336 ; loss 0.49 ; sentence/s 332 ; words/s 17983 ; accuracy train : 80.60850524902344\n",
      "236736 ; loss 0.49 ; sentence/s 335 ; words/s 17749 ; accuracy train : 80.61528778076172\n",
      "243136 ; loss 0.5 ; sentence/s 342 ; words/s 17405 ; accuracy train : 80.59004974365234\n",
      "249536 ; loss 0.49 ; sentence/s 338 ; words/s 17593 ; accuracy train : 80.5909423828125\n",
      "255936 ; loss 0.49 ; sentence/s 337 ; words/s 17927 ; accuracy train : 80.6089859008789\n",
      "262336 ; loss 0.48 ; sentence/s 338 ; words/s 17785 ; accuracy train : 80.60213470458984\n",
      "268736 ; loss 0.5 ; sentence/s 336 ; words/s 17688 ; accuracy train : 80.56993865966797\n",
      "275136 ; loss 0.5 ; sentence/s 339 ; words/s 17570 ; accuracy train : 80.5497817993164\n",
      "281536 ; loss 0.49 ; sentence/s 338 ; words/s 17734 ; accuracy train : 80.54084014892578\n",
      "287936 ; loss 0.48 ; sentence/s 336 ; words/s 17697 ; accuracy train : 80.53993225097656\n",
      "294336 ; loss 0.49 ; sentence/s 338 ; words/s 17589 ; accuracy train : 80.53328704833984\n",
      "300736 ; loss 0.48 ; sentence/s 338 ; words/s 17552 ; accuracy train : 80.54288482666016\n",
      "307136 ; loss 0.48 ; sentence/s 340 ; words/s 17322 ; accuracy train : 80.54654693603516\n",
      "313536 ; loss 0.48 ; sentence/s 331 ; words/s 17854 ; accuracy train : 80.5625\n",
      "319936 ; loss 0.49 ; sentence/s 335 ; words/s 17703 ; accuracy train : 80.5668716430664\n",
      "326336 ; loss 0.48 ; sentence/s 334 ; words/s 18152 ; accuracy train : 80.57537078857422\n",
      "332736 ; loss 0.48 ; sentence/s 335 ; words/s 17784 ; accuracy train : 80.57572174072266\n",
      "339136 ; loss 0.49 ; sentence/s 338 ; words/s 17702 ; accuracy train : 80.57959747314453\n",
      "345536 ; loss 0.49 ; sentence/s 332 ; words/s 17937 ; accuracy train : 80.58824920654297\n",
      "351936 ; loss 0.5 ; sentence/s 334 ; words/s 17767 ; accuracy train : 80.5789794921875\n",
      "358336 ; loss 0.49 ; sentence/s 341 ; words/s 17641 ; accuracy train : 80.57366180419922\n",
      "364736 ; loss 0.47 ; sentence/s 338 ; words/s 17669 ; accuracy train : 80.58141326904297\n",
      "371136 ; loss 0.5 ; sentence/s 336 ; words/s 17769 ; accuracy train : 80.57166290283203\n",
      "377536 ; loss 0.5 ; sentence/s 333 ; words/s 18045 ; accuracy train : 80.55879211425781\n",
      "383936 ; loss 0.5 ; sentence/s 335 ; words/s 17875 ; accuracy train : 80.5445327758789\n",
      "390336 ; loss 0.49 ; sentence/s 337 ; words/s 17654 ; accuracy train : 80.5443115234375\n",
      "396736 ; loss 0.47 ; sentence/s 340 ; words/s 17445 ; accuracy train : 80.54788208007812\n",
      "403136 ; loss 0.49 ; sentence/s 338 ; words/s 17443 ; accuracy train : 80.54092407226562\n",
      "409536 ; loss 0.5 ; sentence/s 342 ; words/s 17621 ; accuracy train : 80.53515625\n",
      "415936 ; loss 0.48 ; sentence/s 339 ; words/s 17599 ; accuracy train : 80.5389404296875\n",
      "422336 ; loss 0.48 ; sentence/s 338 ; words/s 17695 ; accuracy train : 80.5397720336914\n",
      "428736 ; loss 0.49 ; sentence/s 333 ; words/s 17736 ; accuracy train : 80.53218078613281\n",
      "435136 ; loss 0.47 ; sentence/s 338 ; words/s 17678 ; accuracy train : 80.54825592041016\n",
      "441536 ; loss 0.49 ; sentence/s 338 ; words/s 17652 ; accuracy train : 80.5450668334961\n",
      "447936 ; loss 0.48 ; sentence/s 332 ; words/s 17936 ; accuracy train : 80.54308319091797\n",
      "454336 ; loss 0.47 ; sentence/s 335 ; words/s 17673 ; accuracy train : 80.55083465576172\n",
      "460736 ; loss 0.5 ; sentence/s 339 ; words/s 17446 ; accuracy train : 80.546875\n",
      "467136 ; loss 0.48 ; sentence/s 337 ; words/s 17728 ; accuracy train : 80.54280853271484\n",
      "473536 ; loss 0.49 ; sentence/s 342 ; words/s 17582 ; accuracy train : 80.54011535644531\n",
      "479936 ; loss 0.49 ; sentence/s 341 ; words/s 17537 ; accuracy train : 80.54187774658203\n",
      "486336 ; loss 0.48 ; sentence/s 334 ; words/s 17921 ; accuracy train : 80.54379272460938\n",
      "492736 ; loss 0.49 ; sentence/s 335 ; words/s 17742 ; accuracy train : 80.54463958740234\n",
      "499136 ; loss 0.51 ; sentence/s 338 ; words/s 17676 ; accuracy train : 80.53565979003906\n",
      "505536 ; loss 0.5 ; sentence/s 336 ; words/s 17639 ; accuracy train : 80.53244018554688\n",
      "511936 ; loss 0.5 ; sentence/s 345 ; words/s 17482 ; accuracy train : 80.52461242675781\n",
      "518336 ; loss 0.49 ; sentence/s 337 ; words/s 17752 ; accuracy train : 80.53047943115234\n",
      "524736 ; loss 0.5 ; sentence/s 338 ; words/s 17567 ; accuracy train : 80.5243911743164\n",
      "531136 ; loss 0.5 ; sentence/s 335 ; words/s 17840 ; accuracy train : 80.5188217163086\n",
      "537536 ; loss 0.49 ; sentence/s 340 ; words/s 17557 ; accuracy train : 80.51766967773438\n",
      "543936 ; loss 0.49 ; sentence/s 327 ; words/s 17086 ; accuracy train : 80.51103210449219\n",
      "results : epoch 20 ; mean accuracy train : 80.51521301269531\n",
      "\n",
      "VALIDATION : Epoch 20\n",
      "togrep : results : epoch 20 ; mean accuracy valid :              77.99227905273438\n",
      "Shrinking lr by : 5. New lr = 2.6437395962738774e-05\n",
      "\n",
      "TRAINING : Epoch 21\n",
      "Learning rate : 2.6173022003111388e-05\n",
      "6336 ; loss 0.49 ; sentence/s 336 ; words/s 17453 ; accuracy train : 80.5625\n",
      "12736 ; loss 0.48 ; sentence/s 325 ; words/s 17272 ; accuracy train : 81.015625\n",
      "19136 ; loss 0.49 ; sentence/s 333 ; words/s 17506 ; accuracy train : 80.78125\n",
      "25536 ; loss 0.47 ; sentence/s 337 ; words/s 17501 ; accuracy train : 80.9296875\n",
      "31936 ; loss 0.48 ; sentence/s 334 ; words/s 17246 ; accuracy train : 80.8968734741211\n",
      "38336 ; loss 0.5 ; sentence/s 321 ; words/s 17147 ; accuracy train : 80.7578125\n",
      "44736 ; loss 0.5 ; sentence/s 329 ; words/s 17174 ; accuracy train : 80.64955139160156\n",
      "51136 ; loss 0.48 ; sentence/s 336 ; words/s 17535 ; accuracy train : 80.658203125\n",
      "57536 ; loss 0.5 ; sentence/s 332 ; words/s 17649 ; accuracy train : 80.546875\n",
      "63936 ; loss 0.48 ; sentence/s 336 ; words/s 17480 ; accuracy train : 80.5796890258789\n",
      "70336 ; loss 0.5 ; sentence/s 330 ; words/s 16819 ; accuracy train : 80.47869110107422\n",
      "76736 ; loss 0.49 ; sentence/s 330 ; words/s 17464 ; accuracy train : 80.47525787353516\n",
      "83136 ; loss 0.49 ; sentence/s 335 ; words/s 17665 ; accuracy train : 80.51201629638672\n",
      "89536 ; loss 0.49 ; sentence/s 331 ; words/s 17637 ; accuracy train : 80.46540069580078\n",
      "95936 ; loss 0.48 ; sentence/s 323 ; words/s 16338 ; accuracy train : 80.53020477294922\n",
      "102336 ; loss 0.5 ; sentence/s 318 ; words/s 16790 ; accuracy train : 80.4765625\n",
      "108736 ; loss 0.49 ; sentence/s 319 ; words/s 17092 ; accuracy train : 80.46139526367188\n",
      "115136 ; loss 0.48 ; sentence/s 320 ; words/s 16915 ; accuracy train : 80.47222137451172\n",
      "121536 ; loss 0.5 ; sentence/s 324 ; words/s 16847 ; accuracy train : 80.4350357055664\n",
      "127936 ; loss 0.49 ; sentence/s 327 ; words/s 17262 ; accuracy train : 80.43281555175781\n",
      "134336 ; loss 0.48 ; sentence/s 325 ; words/s 16648 ; accuracy train : 80.46131134033203\n",
      "140736 ; loss 0.49 ; sentence/s 320 ; words/s 17385 ; accuracy train : 80.44744110107422\n",
      "147136 ; loss 0.49 ; sentence/s 326 ; words/s 16910 ; accuracy train : 80.44361114501953\n",
      "153536 ; loss 0.48 ; sentence/s 329 ; words/s 16907 ; accuracy train : 80.45442962646484\n",
      "159936 ; loss 0.49 ; sentence/s 327 ; words/s 17319 ; accuracy train : 80.45874786376953\n",
      "166336 ; loss 0.48 ; sentence/s 321 ; words/s 16759 ; accuracy train : 80.48377227783203\n",
      "172736 ; loss 0.49 ; sentence/s 330 ; words/s 16957 ; accuracy train : 80.47801208496094\n",
      "179136 ; loss 0.49 ; sentence/s 322 ; words/s 16613 ; accuracy train : 80.49720764160156\n",
      "185536 ; loss 0.49 ; sentence/s 321 ; words/s 17031 ; accuracy train : 80.4989242553711\n",
      "191936 ; loss 0.49 ; sentence/s 317 ; words/s 16755 ; accuracy train : 80.50312805175781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198336 ; loss 0.48 ; sentence/s 328 ; words/s 16850 ; accuracy train : 80.52368927001953\n",
      "204736 ; loss 0.49 ; sentence/s 324 ; words/s 17188 ; accuracy train : 80.53369140625\n",
      "211136 ; loss 0.51 ; sentence/s 318 ; words/s 16597 ; accuracy train : 80.50757598876953\n",
      "217536 ; loss 0.49 ; sentence/s 324 ; words/s 17184 ; accuracy train : 80.50827026367188\n",
      "223936 ; loss 0.49 ; sentence/s 327 ; words/s 17256 ; accuracy train : 80.51473236083984\n",
      "230336 ; loss 0.49 ; sentence/s 331 ; words/s 17190 ; accuracy train : 80.50347137451172\n",
      "236736 ; loss 0.5 ; sentence/s 324 ; words/s 16483 ; accuracy train : 80.48690795898438\n",
      "243136 ; loss 0.5 ; sentence/s 326 ; words/s 17009 ; accuracy train : 80.48026275634766\n",
      "249536 ; loss 0.49 ; sentence/s 325 ; words/s 17076 ; accuracy train : 80.47996520996094\n",
      "255936 ; loss 0.49 ; sentence/s 319 ; words/s 16535 ; accuracy train : 80.486328125\n",
      "262336 ; loss 0.48 ; sentence/s 315 ; words/s 16348 ; accuracy train : 80.48704528808594\n",
      "268736 ; loss 0.48 ; sentence/s 319 ; words/s 16791 ; accuracy train : 80.5014877319336\n",
      "275136 ; loss 0.49 ; sentence/s 320 ; words/s 17008 ; accuracy train : 80.50181579589844\n",
      "281536 ; loss 0.49 ; sentence/s 333 ; words/s 17309 ; accuracy train : 80.5085220336914\n",
      "287936 ; loss 0.48 ; sentence/s 321 ; words/s 16850 ; accuracy train : 80.51875305175781\n",
      "294336 ; loss 0.47 ; sentence/s 329 ; words/s 17065 ; accuracy train : 80.5268325805664\n",
      "300736 ; loss 0.49 ; sentence/s 319 ; words/s 17139 ; accuracy train : 80.5285873413086\n",
      "307136 ; loss 0.49 ; sentence/s 320 ; words/s 16533 ; accuracy train : 80.51790618896484\n",
      "313536 ; loss 0.46 ; sentence/s 326 ; words/s 17098 ; accuracy train : 80.56058502197266\n",
      "319936 ; loss 0.49 ; sentence/s 330 ; words/s 17318 ; accuracy train : 80.56718444824219\n",
      "326336 ; loss 0.49 ; sentence/s 329 ; words/s 17308 ; accuracy train : 80.56494903564453\n",
      "332736 ; loss 0.5 ; sentence/s 322 ; words/s 17098 ; accuracy train : 80.56159973144531\n",
      "339136 ; loss 0.49 ; sentence/s 323 ; words/s 16778 ; accuracy train : 80.55158996582031\n",
      "345536 ; loss 0.49 ; sentence/s 324 ; words/s 16665 ; accuracy train : 80.54051208496094\n",
      "351936 ; loss 0.5 ; sentence/s 322 ; words/s 17019 ; accuracy train : 80.5397720336914\n",
      "358336 ; loss 0.5 ; sentence/s 317 ; words/s 16725 ; accuracy train : 80.53153228759766\n",
      "364736 ; loss 0.49 ; sentence/s 322 ; words/s 16745 ; accuracy train : 80.52111053466797\n",
      "371136 ; loss 0.48 ; sentence/s 323 ; words/s 16246 ; accuracy train : 80.5315170288086\n",
      "377536 ; loss 0.48 ; sentence/s 320 ; words/s 17028 ; accuracy train : 80.53495788574219\n",
      "383936 ; loss 0.5 ; sentence/s 323 ; words/s 17337 ; accuracy train : 80.51875305175781\n",
      "390336 ; loss 0.5 ; sentence/s 330 ; words/s 17152 ; accuracy train : 80.51536560058594\n",
      "396736 ; loss 0.49 ; sentence/s 331 ; words/s 17118 ; accuracy train : 80.50932312011719\n",
      "403136 ; loss 0.48 ; sentence/s 316 ; words/s 17023 ; accuracy train : 80.51512908935547\n",
      "409536 ; loss 0.49 ; sentence/s 328 ; words/s 17225 ; accuracy train : 80.519775390625\n",
      "415936 ; loss 0.49 ; sentence/s 328 ; words/s 17035 ; accuracy train : 80.51105499267578\n",
      "422336 ; loss 0.48 ; sentence/s 321 ; words/s 17193 ; accuracy train : 80.51893615722656\n",
      "428736 ; loss 0.48 ; sentence/s 336 ; words/s 17216 ; accuracy train : 80.52238464355469\n",
      "435136 ; loss 0.47 ; sentence/s 324 ; words/s 17336 ; accuracy train : 80.53056335449219\n",
      "441536 ; loss 0.47 ; sentence/s 330 ; words/s 17378 ; accuracy train : 80.54166412353516\n",
      "447936 ; loss 0.48 ; sentence/s 328 ; words/s 17308 ; accuracy train : 80.54017639160156\n",
      "454336 ; loss 0.48 ; sentence/s 334 ; words/s 17134 ; accuracy train : 80.54511260986328\n",
      "460736 ; loss 0.49 ; sentence/s 330 ; words/s 17329 ; accuracy train : 80.53689575195312\n",
      "467136 ; loss 0.5 ; sentence/s 327 ; words/s 17589 ; accuracy train : 80.51798248291016\n",
      "473536 ; loss 0.5 ; sentence/s 332 ; words/s 17056 ; accuracy train : 80.50739288330078\n",
      "479936 ; loss 0.47 ; sentence/s 322 ; words/s 17043 ; accuracy train : 80.52791595458984\n",
      "486336 ; loss 0.5 ; sentence/s 333 ; words/s 17080 ; accuracy train : 80.5174789428711\n",
      "492736 ; loss 0.49 ; sentence/s 322 ; words/s 17283 ; accuracy train : 80.52495574951172\n",
      "499136 ; loss 0.49 ; sentence/s 327 ; words/s 17480 ; accuracy train : 80.52183532714844\n",
      "505536 ; loss 0.49 ; sentence/s 327 ; words/s 17270 ; accuracy train : 80.52156066894531\n",
      "511936 ; loss 0.49 ; sentence/s 321 ; words/s 16964 ; accuracy train : 80.5169906616211\n",
      "518336 ; loss 0.49 ; sentence/s 318 ; words/s 16853 ; accuracy train : 80.5079116821289\n",
      "524736 ; loss 0.48 ; sentence/s 324 ; words/s 17488 ; accuracy train : 80.50952911376953\n",
      "531136 ; loss 0.49 ; sentence/s 327 ; words/s 17079 ; accuracy train : 80.509033203125\n",
      "537536 ; loss 0.48 ; sentence/s 328 ; words/s 16955 ; accuracy train : 80.51395416259766\n",
      "543936 ; loss 0.49 ; sentence/s 330 ; words/s 17376 ; accuracy train : 80.51378631591797\n",
      "results : epoch 21 ; mean accuracy train : 80.51102447509766\n",
      "\n",
      "VALIDATION : Epoch 21\n",
      "togrep : results : epoch 21 ; mean accuracy valid :              77.99227905273438\n",
      "Shrinking lr by : 5. New lr = 5.2346044006222775e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST : Epoch 22\n",
      "\n",
      "VALIDATION : Epoch 1000000.0\n",
      "finalgrep : accuracy valid : 78.03292083740234\n",
      "finalgrep : accuracy test : 78.7866439819336\n"
     ]
    }
   ],
   "source": [
    "# Run best model on test set.\n",
    "nli_net.load_state_dict(torch.load(os.path.join(params.outputdir, params.outputmodelname)))\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "evaluate(1e6, 'valid', True)\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save encoder instead of full model\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
