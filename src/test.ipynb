{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS4248 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Kim Thow:\n",
    "\n",
    "### Common Tasks\n",
    "\n",
    "- Pre-processing (tokenization, normalization, etc.)\n",
    "\n",
    "- Data quality improvements (in subsequent iterations, compare with baseline models to show improvement)\n",
    "\n",
    "  - First perform error analysis with baseline models\n",
    "  \n",
    "  - When our models become available:\n",
    "  \n",
    "    - Perform error analysis using our model(s)\n",
    "\n",
    "    - Show improvements after implementing proposed method\n",
    "  \n",
    "- Evaluation metrics (evaluation code referencing e-SNLI repo)\n",
    "\n",
    "  - Maybe propose new evaluation metric?\n",
    "\n",
    "    - May need human evaluation\n",
    "\n",
    "    - Will need to consider the need for this (as opposed to using the same metric as described in the paper), as it may not bring enough value (unless we can think of a good new metric)\n",
    "\n",
    "\n",
    "\n",
    "### Baseline models\n",
    "\n",
    "#### Label only\n",
    "\n",
    "- Create baseline model from InferSent: [model](https://modelzoo.co/model/infersent)\n",
    "\n",
    "#### Label + Explanation\n",
    "\n",
    "- Baseline model 2 (built on top of PredictAndExplain)\n",
    "- Baseline model 3 (built on top of ExplainThenPredictSeq2Seq)\n",
    "- Baseline model 4 (built on top of ExplainThenPredictAttention)\n",
    "\n",
    "\n",
    "\n",
    "### New models\n",
    "\n",
    "- Use efficient pretrained LLM for transfer learning (fine-tuning)\n",
    "\n",
    "  - Explanation generation\n",
    "\n",
    "  - Label prediction using (premise, hypothesis, explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "[e-SNLI Repository](https://github.com/OanaMariaCamburu/e-SNLI)\n",
    "\n",
    "[e-SNLI Dataset on Hugging Face](https://huggingface.co/datasets/esnli)\n",
    "\n",
    "[InferSent baseline model (?)](https://modelzoo.co/model/infersent)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import libraries and set constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL_PATH_PREDICTANDEXPLAIN = './data/PredictAndExplain/state_dict_best_devacc__devACC84.370_devppl10.200__epoch_12_model.pt'\n",
    "TEST_PATH = './data/test.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model (locally downloaded from Google Drive):\n",
    "\n",
    "\\* Note: `latin1` is to remove inconsistencies between Python 2 and Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load locally downloaded model\n",
    "model_PredictAndExplain = torch.load(MODEL_PATH_PREDICTANDEXPLAIN, encoding='latin1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the e-SNLI dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the e-SNLI dataset from Hugging Face\n",
    "dataset_train = load_dataset('esnli', split='train')\n",
    "dataset_test = load_dataset('esnli', split='test')\n",
    "dataset_validation = load_dataset('esnli', split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
