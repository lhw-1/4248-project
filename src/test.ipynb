{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS4248 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Kim Thow:\n",
    "\n",
    "### Common Tasks\n",
    "\n",
    "- Pre-processing (tokenization, normalization, etc.)\n",
    "\n",
    "- Data quality improvements (in subsequent iterations, compare with baseline models to show improvement)\n",
    "\n",
    "  - First perform error analysis with baseline models\n",
    "  \n",
    "  - When our models become available:\n",
    "  \n",
    "    - Perform error analysis using our model(s)\n",
    "\n",
    "    - Show improvements after implementing proposed method\n",
    "  \n",
    "- Evaluation metrics (evaluation code referencing e-SNLI repo)\n",
    "\n",
    "  - Maybe propose new evaluation metric?\n",
    "\n",
    "    - May need human evaluation\n",
    "\n",
    "    - Will need to consider the need for this (as opposed to using the same metric as described in the paper), as it may not bring enough value (unless we can think of a good new metric)\n",
    "\n",
    "\n",
    "\n",
    "### Baseline models\n",
    "\n",
    "#### Label only\n",
    "\n",
    "- Create baseline model from InferSent: [model](https://modelzoo.co/model/infersent)\n",
    "\n",
    "#### Label + Explanation\n",
    "\n",
    "- Baseline model 2 (built on top of PredictAndExplain)\n",
    "- Baseline model 3 (built on top of ExplainThenPredictSeq2Seq)\n",
    "- Baseline model 4 (built on top of ExplainThenPredictAttention)\n",
    "\n",
    "\n",
    "\n",
    "### New models\n",
    "\n",
    "- Use efficient pretrained LLM for transfer learning (fine-tuning)\n",
    "\n",
    "  - Explanation generation\n",
    "\n",
    "  - Label prediction using (premise, hypothesis, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "[e-SNLI Repository](https://github.com/OanaMariaCamburu/e-SNLI)\n",
    "\n",
    "[e-SNLI Dataset on Hugging Face](https://huggingface.co/datasets/esnli)\n",
    "\n",
    "[InferSent baseline model (?)](https://modelzoo.co/model/infersent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import libraries and set constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL_PATH_PREDICTANDEXPLAIN = '../data/PredictAndExplain/state_dict_best_devacc__devACC84.370_devppl10.200__epoch_12_model.pt'\n",
    "TEST_PATH = '../data/esnli_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model (locally downloaded from Google Drive):\n",
    "\n",
    "\\* Note: `latin1` is to remove inconsistencies between Python 2 and Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load locally downloaded model\n",
    "model_PredictAndExplain = torch.load(MODEL_PATH_PREDICTANDEXPLAIN, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the e-SNLI dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the e-SNLI dataset from Hugging Face\n",
    "# dataset_train = load_dataset('esnli', split='train')\n",
    "# dataset_test = load_dataset('esnli', split='test')\n",
    "# dataset_validation = load_dataset('esnli', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_config_att = model_PredictAndExplain['config_model']\n",
    "model_config_att[\"max_T_encoder\":\n",
    "\n",
    "# model_config_att = {\n",
    "#     \"att_hid_dim\": 512,\n",
    "#     \"att_type\": 'dot',\n",
    "#     \"avg_every\": 100, \n",
    "#     \"bsize\": 64,\n",
    "#     \"breaking_nli_path\": '../dataset/Breaking_NLI/',\n",
    "#     \"comp_path\": '../dataset/Comp/test/',\n",
    "#     \"cudnn_deterministic\": True,\n",
    "#     \"current_run_dir\": 'results_attention_eSNLI_superlongstring',\n",
    "#     \"dec_rnn_dim\": 1024, \n",
    "#     \"decay\": 0.99, \n",
    "#     \"decoder_type\": 'lstm', \n",
    "#     \"directory_expl_to_labels\": '../expl_to_labels/results_expl_to_labels/23:05_20:17:40_sgd,lr=0.1_Enc2048_bs256_gpu2_LRdecay0.99_MAXpool_lrshrink5',\n",
    "#     \"do_image_caption\": False, \n",
    "#     \"dpout_dec\": 0.5,\n",
    "#     \"dpout_enc\": 0.0,\n",
    "#     \"dpout_fc\": 0.0, \n",
    "#     \"early_stopping_epochs\": 50, \n",
    "#     \"enc_rnn_dim\": 2048, \n",
    "#     \"encoder_type\": 'BLSTMEncoder',\n",
    "#     \"esnli_path\": '../dataset/eSNLI/', \n",
    "#     \"eval_batch_size\": 64, \n",
    "#     \"fc_dim\": 512, \n",
    "#     \"gpu\": 2, \n",
    "#     \"gpu_id\": 0,\n",
    "#     \"hard_snli_path\": '../dataset/SNLI_hard/',\n",
    "#     \"lrshrink\": 5,\n",
    "#     \"max_T_decoder\": 40, \n",
    "#     \"max_T_encoder\": 84, \n",
    "#     \"max_norm\": 5.0, \n",
    "#     \"min_freq\": 15, \n",
    "#     \"minlr\": 1e-05, \n",
    "#     \"multinli_path\": '../dataset/MultiNLI/',\n",
    "#     \"n_classes\": 3, \n",
    "#     \"n_epochs\": 20, \n",
    "#     \"n_layers_dec\": 1, \n",
    "#     \"n_train\": -1, \n",
    "#     \"optimizer\": 'sgd,lr=0.1', \n",
    "#     \"pool_type\": 'max', \n",
    "#     \"preproc_expl\": 'preproc1', \n",
    "#     \"print_every\": 500, \n",
    "#     \"results_dir\": 'results_attention_eSNLI_separate',\n",
    "#     \"save_title\": '_decLSTM_sgd,lr=0.1_Enc2048_Dec1024_bs64_gpu4__encT84__decT40_LRdecay0.99_dpout_dec0.5_MAXpool_lrshrink5_init', \n",
    "#     \"seed\": 1234, \n",
    "#     \"separate_att\": True, \n",
    "#     \"sick_path\": '../data/senteval_data/SICK/', \n",
    "#     \"state_path_expl_to_labels\": 'state_dict_best_devacc__devACC96.780__epoch_12_model.pt', \n",
    "#     \"train_set\": 'eSNLI', \n",
    "#     \"train_snli_classif\": False, \n",
    "#     \"use_init\": True, \n",
    "#     \"use_prototype_senteval\": False\n",
    "# }\n",
    "\n",
    "model_state_dict = model_PredictAndExplain['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_config_att)\n",
    "# print(model_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mutils import get_keys_from_vals, assert_sizes\n",
    "\n",
    "\n",
    "def array_all_true(arr):\n",
    "\tfor i in arr:\n",
    "\t\tif i == False:\n",
    "\t\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "\"\"\"\n",
    "AttentionDecoder for the explanation\n",
    "\"\"\"\n",
    "class AttentionDecoder(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(AttentionDecoder, self).__init__()\n",
    "\t\t\n",
    "\t\tself.decoder_type = config['decoder_type']\n",
    "\t\tself.word_emb_dim = config['word_emb_dim']\n",
    "\t\tself.dec_rnn_dim = config['dec_rnn_dim']\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.dpout_dec = config['dpout_dec']\n",
    "\t\tself.n_vocab = config['n_vocab']\n",
    "\t\tself.word_index = config['word_index']\n",
    "\t\tself.word_vec = config['word_vec']\n",
    "\t\tself.max_T_decoder = config['max_T_decoder']\n",
    "\t\tself.max_T_encoder = config['max_T_encoder']\n",
    "\t\tself.n_layers_dec = config['n_layers_dec']\n",
    "\t\t# for decoder intial state\n",
    "\t\tself.use_init = config['use_init']\n",
    "\t\t# attention type: dot product or linear layer\n",
    "\t\tself.att_type = config['att_type'] # 'lin' or 'dot'\n",
    "\t\t# whether to visualize attention weights\n",
    "\t\tself.att_hid_dim =config['att_hid_dim']\n",
    "\n",
    "\n",
    "\t\tself.sent_dim = 2 * config['enc_rnn_dim']\n",
    "\t\tif config['encoder_type'] in [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"]:\n",
    "\t\t\tself.sent_dim = 4 * self.sent_dim \n",
    "\t\tif config['encoder_type'] == \"LSTMEncoder\":\n",
    "\t\t\tself.sent_dim = self.sent_dim / 2 \n",
    "\n",
    "\t\tassert self.sent_dim == 4096, str(self.sent_dim)\n",
    "\t\t# TODO: remove this when implemented linear attention\n",
    "\t\tassert self.att_type == 'dot'\n",
    "\n",
    "\t\tself.context_proj = nn.Linear(4 * self.sent_dim, self.dec_rnn_dim)\n",
    "\n",
    "\t\tself.att_ht_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_context_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.dec_rnn_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_before_weighting_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_context_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.dec_rnn_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_before_weighting_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\n",
    "\t\tself.proj_inp_dec = nn.Linear(2 * self.att_hid_dim + self.word_emb_dim, self.dec_rnn_dim)\t\n",
    "\t\tif self.decoder_type == 'gru':\n",
    "\t\t\tself.decoder_rnn = nn.GRU(self.dec_rnn_dim, self.dec_rnn_dim, self.n_layers_dec, bidirectional=False, dropout=self.dpout_dec)\n",
    "\t\telse: # 'lstm'\n",
    "\t\t\tself.decoder_rnn = nn.LSTM(self.dec_rnn_dim, self.dec_rnn_dim, self.n_layers_dec, bidirectional=False, dropout=self.dpout_dec)\n",
    "\n",
    "\t\t# att softmax\n",
    "\t\tself.softmax_att = nn.Softmax(2)\n",
    "\n",
    "\t\t# vocab layer\n",
    "\t\tself.vocab_layer = nn.Linear(self.dec_rnn_dim, self.n_vocab)\n",
    "\n",
    "\n",
    "\tdef forward(self, expl, enc_out_s1, enc_out_s2, s1_embed, s2_embed, mode, visualize):\n",
    "\t\t# expl: Variable(seqlen x bsize x worddim)\n",
    "\t\t# s1/2_embed: Variable(bsize x sent_dim)\n",
    "\t\t\n",
    "\t\tassert mode in ['forloop', 'teacher'], mode\n",
    "\n",
    "\t\tcurrent_T_dec = expl.size(0)\n",
    "\t\tbatch_size = expl.size(1)\n",
    "\t\tassert_sizes(s1_embed, 2, [batch_size, self.sent_dim])\n",
    "\t\tassert_sizes(s2_embed, 2, [batch_size, self.sent_dim])\n",
    "\t\tassert_sizes(expl, 3, [current_T_dec, batch_size, self.word_emb_dim])\n",
    "\t\tassert_sizes(enc_out_s1, 3, [self.max_T_encoder, batch_size, 2 * self.enc_rnn_dim])\n",
    "\t\tassert_sizes(enc_out_s2, 3, [self.max_T_encoder, batch_size, 2 * self.enc_rnn_dim])\n",
    "\n",
    "\t\tcontext = torch.cat([s1_embed, s2_embed, torch.abs(s1_embed - s2_embed), s1_embed * s2_embed], 1).unsqueeze(0)\n",
    "\t\tassert_sizes(context, 3, [1, batch_size, 4 * self.sent_dim])\n",
    "\n",
    "\t\t# init decoder\n",
    "\t\tif self.use_init:\n",
    "\t\t\tinit_0 = self.context_proj(context).expand(self.n_layers_dec, batch_size, self.dec_rnn_dim)\n",
    "\t\telse:\n",
    "\t\t\tinit_0 = Variable(torch.zeros(self.n_layers_dec, batch_size, self.dec_rnn_dim)).cuda()\n",
    "\n",
    "\t\tinit_state = init_0\n",
    "\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\tinit_state = (init_0, init_0)\n",
    "\n",
    "\t\tself.decoder_rnn.flatten_parameters()\n",
    "\n",
    "\t\tout_expl = None\n",
    "\t\tstate_t = init_state\n",
    "\t\tcontext = self.context_proj(context)\n",
    "\t\tif mode == \"teacher\":\n",
    "\t\t\tfor t_dec in range(current_T_dec):\n",
    "\t\t\t\t# attention over premise\n",
    "\t\t\t\tcontext1 = self.att_context_proj1(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinp_att_1 = self.att_ht_proj1(enc_out_s1).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_1, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_1 = torch.bmm(context1, inp_att_1)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_1 = self.softmax_att(dot_prod_att_1)\n",
    "\t\t\t\tassert_sizes(att_weights_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_1 = torch.bmm(att_weights_1, self.att_ht_before_weighting_proj1(enc_out_s1).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_1 = att_applied_1.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_1, 3, [1, batch_size, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\t# attention over hypothesis\n",
    "\t\t\t\tcontext2 = self.att_context_proj2(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tinp_att_2 = self.att_ht_proj2(enc_out_s2).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_2, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_2 = torch.bmm(context2, inp_att_2)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_2 = self.softmax_att(dot_prod_att_2)\n",
    "\t\t\t\tassert_sizes(att_weights_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_2 = torch.bmm(att_weights_2, self.att_ht_before_weighting_proj2(enc_out_s2).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_2 = att_applied_2.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_2, 3, [1, batch_size, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinput_dec = torch.cat([expl[t_dec].unsqueeze(0), att_applied_perm_1, att_applied_perm_2], 2) \n",
    "\t\t\t\tinput_dec = nn.Dropout(self.dpout_dec)(self.proj_inp_dec(input_dec))\n",
    "\n",
    "\t\t\t\tout_dec, state_t = self.decoder_rnn(input_dec, state_t)\n",
    "\t\t\t\tassert_sizes(out_dec, 3, [1, batch_size, self.dec_rnn_dim])\n",
    "\t\t\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\t\t\tcontext = state_t[0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontext = state_t\n",
    "\n",
    "\t\t\t\tif out_expl is None:\n",
    "\t\t\t\t\tout_expl = out_dec\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tout_expl = torch.cat([out_expl, out_dec], 0)\n",
    "\n",
    "\t\t\tout_expl = self.vocab_layer(out_expl)\n",
    "\t\t\tassert_sizes(out_expl, 3, [current_T_dec, batch_size, self.n_vocab])\n",
    "\t\t\treturn out_expl\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tpred_expls = []\n",
    "\t\t\tfinished = []\n",
    "\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\tpred_expls.append(\"\")\n",
    "\t\t\t\tfinished.append(False)\n",
    "\n",
    "\t\t\tt_dec = 0\n",
    "\t\t\tword_t = expl[0].unsqueeze(0)\n",
    "\t\t\twhile t_dec < self.max_T_decoder and not array_all_true(finished):\n",
    "\t\t\t\t#print \"\\n\\n\\n t: \", t_dec\n",
    "\n",
    "\t\t\t\tassert_sizes(word_t, 3, [1, batch_size, self.word_emb_dim])\n",
    "\t\t\t\tword_embed = torch.zeros(1, batch_size, self.word_emb_dim)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# attention over premise\n",
    "\t\t\t\tcontext1 = self.att_context_proj1(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinp_att_1 = self.att_ht_proj1(enc_out_s1).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_1, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_1 = torch.bmm(context1, inp_att_1)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_1 = self.softmax_att(dot_prod_att_1)\n",
    "\t\t\t\tassert_sizes(att_weights_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_1 = torch.bmm(att_weights_1, self.att_ht_before_weighting_proj1(enc_out_s1).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_1 = att_applied_1.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_1, 3, [1, batch_size, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\t# attention over hypothesis\n",
    "\t\t\t\tcontext2 = self.att_context_proj2(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tinp_att_2 = self.att_ht_proj2(enc_out_s2).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_2, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_2 = torch.bmm(context2, inp_att_2)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_2 = self.softmax_att(dot_prod_att_2)\n",
    "\t\t\t\tassert_sizes(att_weights_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_2 = torch.bmm(att_weights_2, self.att_ht_before_weighting_proj2(enc_out_s2).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_2 = att_applied_2.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_2, 3, [1, batch_size, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinput_dec = torch.cat([word_t, att_applied_perm_1, att_applied_perm_2], 2) \n",
    "\t\t\t\tinput_dec = self.proj_inp_dec(input_dec)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#print \"att_weights_1[0] \", att_weights_1[0]\n",
    "\t\t\t\t#print \"att_weights_2[0] \", att_weights_2[0]\n",
    "\n",
    "\t\t\t\t# get one visualization from the current batch\n",
    "\t\t\t\tif visualize:\n",
    "\t\t\t\t\tif t_dec == 0:\n",
    "\t\t\t\t\t\tweights_1 = att_weights_1[0]\n",
    "\t\t\t\t\t\tweights_2 = att_weights_2[0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tweights_1 = torch.cat([weights_1, att_weights_1[0]], 0)\n",
    "\t\t\t\t\t\tweights_2 = torch.cat([weights_2, att_weights_2[0]], 0)\n",
    "\n",
    "\t\t\t\tfor ii in range(batch_size):\n",
    "\t\t\t\t\tassert abs(att_weights_1[ii].data.sum() - 1) < 1e-5, str(att_weights_1[ii].data.sum())\n",
    "\t\t\t\t\tassert abs(att_weights_2[ii].data.sum() - 1) < 1e-5, str(att_weights_2[ii].data.sum())\n",
    "\n",
    "\t\t\t\tout_t, state_t = self.decoder_rnn(input_dec, state_t)\n",
    "\t\t\t\tassert_sizes(out_t, 3, [1, batch_size, self.dec_rnn_dim])\n",
    "\t\t\t\tout_t = self.vocab_layer(out_t)\n",
    "\t\t\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\t\t\tcontext = state_t[0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontext = state_t\n",
    "\n",
    "\t\t\t\ti_t = torch.max(out_t, 2)[1].data\n",
    "\t\t\t\tassert_sizes(i_t, 2, [1, batch_size])\n",
    "\t\t\t\tpred_words = get_keys_from_vals(i_t, self.word_index) # array of bs of words at current timestep\n",
    "\t\t\t\tassert len(pred_words) == batch_size, \"pred_words \" + str(len(pred_words)) + \" batch_size \" + str(batch_size)\n",
    "\t\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\t\tif pred_words[i] == '</s>':\n",
    "\t\t\t\t\t\tfinished[i] = True\n",
    "\t\t\t\t\tif not finished[i]:\n",
    "\t\t\t\t\t\tpred_expls[i] += \" \" + pred_words[i]\n",
    "\t\t\t\t\tword_embed[0, i] = torch.from_numpy(self.word_vec[pred_words[i]])\n",
    "\t\t\t\tword_t = Variable(word_embed.cuda())\n",
    "\n",
    "\t\t\t\tt_dec += 1\n",
    "\t\t\t\n",
    "\t\t\tif visualize:\n",
    "\t\t\t\tassert weights_1.dim() == 2\n",
    "\t\t\t\tassert weights_1.size(1) == self.max_T_encoder\n",
    "\t\t\t\tassert weights_2.dim() == 2\n",
    "\t\t\t\tassert weights_2.size(1) == self.max_T_encoder\n",
    "\t\t\t\tpred_expls = [pred_expls, weights_1, weights_2]\n",
    "\t\t\treturn pred_expls\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLSTM (max/mean) encoder\n",
    "\"\"\"\n",
    "class BLSTMEncoder(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(BLSTMEncoder, self).__init__()\n",
    "\t\tself.bsize = config['bsize']\n",
    "\t\tself.word_emb_dim = config['word_emb_dim']\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.pool_type = config['pool_type']\n",
    "\t\tself.dpout_enc = config['dpout_enc']\n",
    "\t\tself.max_T_encoder = config['max_T_encoder']\n",
    "\n",
    "\t\tself.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_rnn_dim, 1,\n",
    "\t\t\t\t\t\t\t\tbidirectional=True, dropout=self.dpout_enc)\n",
    "\n",
    "\tdef is_cuda(self):\n",
    "\t\t# either all weights are on cpu or they are on gpu\n",
    "\t\treturn 'cuda' in str(type(self.enc_lstm.bias_hh_l0.data))\n",
    "\n",
    "\tdef forward(self, sent_tuple):\n",
    "\t\t# sent_len: [max_len, ..., min_len] (bsize)\n",
    "\t\t# sent: Variable(seqlen x bsize x worddim)\n",
    "\t\tsent, sent_len = sent_tuple\n",
    "\t\t#assert_sizes(sent, 3, [self.max_T_encoder, sent.size(1), self.word_emb_dim])\n",
    "\n",
    "\t\t# Sort by length (keep idx)\n",
    "\t\tsent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "\t\tidx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_sort)\n",
    "\t\tsent = sent.index_select(1, Variable(idx_sort))\n",
    "\n",
    "\t\t# Handling padding in Recurrent Networks\n",
    "\t\tsent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n",
    "\t\tself.enc_lstm.flatten_parameters()\n",
    "\t\tsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "\t\tpadding_value = 0.0\n",
    "\t\tif self.pool_type == \"max\":\n",
    "\t\t\tpadding_value = -100\n",
    "\t\tsent_output_padding = nn.utils.rnn.pad_packed_sequence(sent_output, False, padding_value)[0]\n",
    "\t\tsent_output = nn.utils.rnn.pad_packed_sequence(sent_output, False, 0)[0]\n",
    "\n",
    "\t\t# Un-sort by length\n",
    "\t\tidx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_unsort)\n",
    "\t\tsent_output = sent_output.index_select(1, Variable(idx_unsort))\n",
    "\t\tsent_output_padding = sent_output_padding.index_select(1, Variable(idx_unsort))\n",
    "\t\t\n",
    "\t\t# Pooling\n",
    "\t\tif self.pool_type == \"mean\":\n",
    "\t\t\tsent_len = Variable(torch.FloatTensor(sent_len)).unsqueeze(1).cuda()\n",
    "\t\t\temb = torch.sum(sent_output_padding, 0).squeeze(0)\n",
    "\t\t\temb = emb / sent_len.expand_as(emb)\n",
    "\t\telif self.pool_type == \"max\":\n",
    "\t\t\temb = torch.max(sent_output_padding, 0)[0]\n",
    "\t\t\tif emb.ndimension() == 3:\n",
    "\t\t\t\temb = emb.squeeze(0)\n",
    "\t\t\t\tassert emb.ndimension() == 2, \"emb.ndimension()=\" + str(emb.ndimension())\n",
    "\n",
    "\t\t# pad with zeros so that max length is the same for all, needed for attention\n",
    "\t\tif sent_output.size(0) < self.max_T_encoder:\n",
    "\t\t\tpad_tensor = Variable(torch.zeros(self.max_T_encoder - sent_output.size(0), sent_output.size(1), sent_output.size(2)).cuda())\n",
    "\t\t\tsent_output = torch.cat([sent_output, pad_tensor], 0)\n",
    "\t\t\n",
    "\t\treturn sent_output, emb\n",
    "\n",
    "\n",
    "\n",
    "\tdef set_glove_path(self, glove_path):\n",
    "\t\tself.glove_path = glove_path\n",
    "\n",
    "\tdef get_word_dict(self, sentences, tokenize=True):\n",
    "\t\t# create vocab of words\n",
    "\t\tword_dict = {}\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [s.split() if not tokenize else word_tokenize(s)\n",
    "\t\t\t\t\t for s in sentences]\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tfor word in sent:\n",
    "\t\t\t\tif word not in word_dict:\n",
    "\t\t\t\t\tword_dict[word] = ''\n",
    "\t\tword_dict['<s>'] = ''\n",
    "\t\tword_dict['</s>'] = ''\n",
    "\t\treturn word_dict\n",
    "\n",
    "\tdef get_glove(self, word_dict):\n",
    "\t\tassert hasattr(self, 'glove_path'), \\\n",
    "\t\t\t   'warning : you need to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with glove vectors\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif word in word_dict:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\tprint('Found {0}(/{1}) words with glove vectors'.format(\n",
    "\t\t\t\t\tlen(word_vec), len(word_dict)))\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef get_glove_k(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with k first glove vectors\n",
    "\t\tk = 0\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif k <= K:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\t\t\t\tk += 1\n",
    "\t\t\t\tif k > K:\n",
    "\t\t\t\t\tif word in ['<s>', '</s>']:\n",
    "\t\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "\t\t\t\tif k > K and all([w in word_vec for w in ['<s>', '</s>']]):\n",
    "\t\t\t\t\tbreak\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef build_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\t\tself.word_vec = self.get_glove(word_dict)\n",
    "\t\tprint('Vocab size from within BLSTMEncoder : {0}'.format(len(self.word_vec)))\n",
    "\n",
    "\t# build GloVe vocab with k most frequent words\n",
    "\tdef build_vocab_k_words(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tself.word_vec = self.get_glove_k(K)\n",
    "\t\tprint('Vocab size : {0}'.format(K))\n",
    "\n",
    "\tdef update_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tassert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "\t\t# keep only new words\n",
    "\t\tfor word in self.word_vec:\n",
    "\t\t\tif word in word_dict:\n",
    "\t\t\t\tdel word_dict[word]\n",
    "\n",
    "\t\t# udpate vocabulary\n",
    "\t\tif word_dict:\n",
    "\t\t\tnew_word_vec = self.get_glove(word_dict)\n",
    "\t\t\tself.word_vec.update(new_word_vec)\n",
    "\t\tprint('New vocab size : {0} (added {1} words)'.format(\n",
    "\t\t\t\t\t\tlen(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "\tdef get_batch(self, batch):\n",
    "\t\t# sent in batch in decreasing order of lengths\n",
    "\t\t# batch: (bsize, max_len, word_dim)\n",
    "\t\tembed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "\t\tfor i in range(len(batch)):\n",
    "\t\t\tfor j in range(len(batch[i])):\n",
    "\t\t\t\tembed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "\t\treturn torch.FloatTensor(embed)\n",
    "\n",
    "\tdef prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [['<s>'] + s.split() + ['</s>'] if not tokenize else\n",
    "\t\t\t\t\t ['<s>']+word_tokenize(s)+['</s>'] for s in sentences]\n",
    "\t\tn_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "\t\t# filters words without glove vectors\n",
    "\t\tfor i in range(len(sentences)):\n",
    "\t\t\ts_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "\t\t\tif not s_f:\n",
    "\t\t\t\timport warnings\n",
    "\t\t\t\twarnings.warn('No words in \"{0}\" (idx={1}) have glove vectors. \\\n",
    "\t\t\t\t\t\t\t   Replacing by \"</s>\"..'.format(sentences[i], i))\n",
    "\t\t\t\ts_f = ['</s>']\n",
    "\t\t\tsentences[i] = s_f\n",
    "\n",
    "\t\tlengths = np.array([len(s) for s in sentences])\n",
    "\t\tn_wk = np.sum(lengths)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Nb words kept : {0}/{1} ({2} %)'.format(\n",
    "\t\t\t\t\t\tn_wk, n_w, round((100.0 * n_wk) / n_w, 2)))\n",
    "\n",
    "\t\t# sort by decreasing length\n",
    "\t\tlengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "\t\tsentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "\t\treturn sentences, lengths, idx_sort\n",
    "\n",
    "\tdef encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "\t\ttic = time.time()\n",
    "\t\tsentences, lengths, idx_sort = self.prepare_samples(\n",
    "\t\t\t\t\t\tsentences, bsize, tokenize, verbose)\n",
    "\n",
    "\t\tembeddings = []\n",
    "\t\tfor stidx in range(0, len(sentences), bsize):\n",
    "\t\t\tbatch = Variable(self.get_batch(\n",
    "\t\t\t\t\t\tsentences[stidx:stidx + bsize]), volatile=True)\n",
    "\t\t\tif self.is_cuda():\n",
    "\t\t\t\tbatch = batch.cuda()\n",
    "\t\t\tbatch = self.forward(\n",
    "\t\t\t\t(batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "\t\t\tembeddings.append(batch)\n",
    "\t\tembeddings = np.vstack(embeddings)\n",
    "\n",
    "\t\t# unsort\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\t\tembeddings = embeddings[idx_unsort]\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Speed : {0} sentences/s ({1} mode, bsize={2})'.format(\n",
    "\t\t\t\t\tround(len(embeddings)/(time.time()-tic), 2),\n",
    "\t\t\t\t\t'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "\t\treturn embeddings\n",
    "\n",
    "\tdef visualize(self, sent, tokenize=True):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\n",
    "\t\tsent = sent.split() if not tokenize else word_tokenize(sent)\n",
    "\t\tsent = [['<s>'] + [word for word in sent if word in self.word_vec] +\n",
    "\t\t\t\t['</s>']]\n",
    "\n",
    "\t\tif ' '.join(sent[0]) == '<s> </s>':\n",
    "\t\t\timport warnings\n",
    "\t\t\twarnings.warn('No words in \"{0}\" have glove vectors. Replacing \\\n",
    "\t\t\t\t\t\t   by \"<s> </s>\"..'.format(sent))\n",
    "\t\tbatch = Variable(self.get_batch(sent), volatile=True)\n",
    "\n",
    "\t\tif self.is_cuda():\n",
    "\t\t\tbatch = batch.cuda()\n",
    "\t\toutput = self.enc_lstm(batch)[0]\n",
    "\t\toutput, idxs = torch.max(output, 0)\n",
    "\t\t# output, idxs = output.squeeze(), idxs.squeeze()\n",
    "\t\tidxs = idxs.data.cpu().numpy()\n",
    "\t\targmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "\t\t# visualize model\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\tx = range(len(sent[0]))\n",
    "\t\ty = [100.0*n/np.sum(argmaxs) for n in argmaxs]\n",
    "\t\tplt.xticks(x, sent[0], rotation=45)\n",
    "\t\tplt.bar(x, y)\n",
    "\t\tplt.ylabel('%')\n",
    "\t\tplt.title('Visualisation of words importance')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\treturn output, idxs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main module for Natural Language Inference\n",
    "\"\"\"\n",
    "class eSNLIAttention(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(eSNLIAttention, self).__init__()\n",
    "\t\tself.encoder_type = config['encoder_type']\n",
    "\n",
    "\t\tself.encoder = eval(self.encoder_type)(config)\n",
    "\t\tself.decoder = AttentionDecoder(config)\n",
    "\n",
    "\tdef forward(self, s1, s2, expl, mode, visualize):\n",
    "\t\t# s1 : (s1, s1_len)\n",
    "\t\t# s2 : (s2, s2_len)\n",
    "\t\t# expl : Variable(T x bs x 300)\n",
    "\n",
    "\t\tu, u_emb = self.encoder(s1) # u = max_T_enc x bs x (2 * enc_dim) ; u_emb = 1 x bs x (2 * enc_dim)\n",
    "\t\tv, v_emb = self.encoder(s2) \n",
    "\n",
    "\t\tout_expl = self.decoder(expl, u, v, u_emb, v_emb, mode, visualize)\n",
    "\t\t\n",
    "\t\treturn out_expl\n",
    "\n",
    "\tdef encode(self, s1):\n",
    "\t\temb = self.encoder(s1)\n",
    "\t\treturn emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'max_T_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m att_net \u001b[38;5;241m=\u001b[39m \u001b[43meSNLIAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config_att\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      2\u001b[0m att_net\u001b[38;5;241m.\u001b[39mload_state_dict(model_state_dict)\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m state_att[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36meSNLIAttention.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28msuper\u001b[39m(eSNLIAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_type \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m AttentionDecoder(config)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mBLSTMEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_type \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpout_enc \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdpout_enc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_T_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_T_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_lstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_emb_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_rnn_dim, \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    309\u001b[0m \t\t\t\t\t\tbidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpout_enc)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'max_T_encoder'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "att_net = eSNLIAttention(model_config_att).cuda()\n",
    "att_net.load_state_dict(model_state_dict)\n",
    "params = state_att['params']\n",
    "assert params.separate_att == eval_params.separate_att, \"params.separate_att \" + str(params.separate_att)\n",
    "params.word_vec_expl = model_config_att['word_vec']\n",
    "params.current_run_dir = eval_params.directory\n",
    "params.eval_batch_size = eval_params.eval_batch_size\n",
    "params.eval_just_snli  = eval_params.eval_just_snli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
