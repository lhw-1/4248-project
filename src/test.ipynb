{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS4248 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Kim Thow:\n",
    "\n",
    "### Common Tasks\n",
    "\n",
    "- Pre-processing (tokenization, normalization, etc.)\n",
    "\n",
    "- Data quality improvements (in subsequent iterations, compare with baseline models to show improvement)\n",
    "\n",
    "  - First perform error analysis with baseline models\n",
    "  \n",
    "  - When our models become available:\n",
    "  \n",
    "    - Perform error analysis using our model(s)\n",
    "\n",
    "    - Show improvements after implementing proposed method\n",
    "  \n",
    "- Evaluation metrics (evaluation code referencing e-SNLI repo)\n",
    "\n",
    "  - Maybe propose new evaluation metric?\n",
    "\n",
    "    - May need human evaluation\n",
    "\n",
    "    - Will need to consider the need for this (as opposed to using the same metric as described in the paper), as it may not bring enough value (unless we can think of a good new metric)\n",
    "\n",
    "\n",
    "\n",
    "### Baseline models\n",
    "\n",
    "#### Label only\n",
    "\n",
    "- Create baseline model from InferSent: [model](https://modelzoo.co/model/infersent)\n",
    "\n",
    "#### Label + Explanation\n",
    "\n",
    "- Baseline model 2 (built on top of PredictAndExplain)\n",
    "- Baseline model 3 (built on top of ExplainThenPredictSeq2Seq)\n",
    "- Baseline model 4 (built on top of ExplainThenPredictAttention)\n",
    "\n",
    "\n",
    "\n",
    "### New models\n",
    "\n",
    "- Use efficient pretrained LLM for transfer learning (fine-tuning)\n",
    "\n",
    "  - Explanation generation\n",
    "\n",
    "  - Label prediction using (premise, hypothesis, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "[e-SNLI Repository](https://github.com/OanaMariaCamburu/e-SNLI)\n",
    "\n",
    "[e-SNLI Dataset on Hugging Face](https://huggingface.co/datasets/esnli)\n",
    "\n",
    "[InferSent baseline model (?)](https://modelzoo.co/model/infersent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import libraries and set constants:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the model (by Elvis)\n",
    "\n",
    "Download everything from https://github.com/OanaMariaCamburu/e-SNLI/tree/master/dataset/eSNLI and put it in /data/eSNLI/\n",
    "    \n",
    "Download glove.840B.300d.txt Glove word embeddings and put it in /dataset/GloVe/\n",
    "\n",
    "Run all the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL_PATH_PREDICTANDEXPLAIN = '../data/PredictAndExplain/state_dict_best_devacc__devACC84.370_devppl10.200__epoch_12_model.pt'\n",
    "MODEL_PATH_EXPLAINTHENPREDICTATTENTION = '../data/ExplainThenPredictAttention/state_dict_best_devppl__devPPL6.082__epoch_19_model.pt'\n",
    "MODEL_PATH_EXPLSTOLABELS = '../data/ExplsToLabels/state_dict_best_devacc__devACC96.780__epoch_12_model.pt'\n",
    "TEST_PATH = '../data/esnli_test.csv'\n",
    "ESNLI_PATH = '../data/eSNLI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model (locally downloaded from Google Drive):\n",
    "\n",
    "\\* Note: `latin1` is to remove inconsistencies between Python 2 and Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load locally downloaded model\n",
    "# model_PredictAndExplain = torch.load(MODEL_PATH_PREDICTANDEXPLAIN, encoding='latin1')\n",
    "model_ExplainThenPredictAttention = torch.load(MODEL_PATH_EXPLAINTHENPREDICTATTENTION, encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the e-SNLI dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the e-SNLI dataset from Hugging Face\n",
    "# dataset_train = load_dataset('esnli', split='train')\n",
    "# dataset_test = load_dataset('esnli', split='test')\n",
    "# dataset_validation = load_dataset('esnli', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_config_att = model_ExplainThenPredictAttention['config_model']\n",
    "# model_config_att[\"max_T_encoder\"] = 84\n",
    "# model_config_att[\"att_type\"] = 'dot'\n",
    "# model_config_att[\"att_hid_dim\"] = 512\n",
    "\n",
    "# print(model_config_att.keys())\n",
    "\n",
    "# model_config_att = {\n",
    "#     \"att_hid_dim\": 512,\n",
    "#     \"att_type\": 'dot',\n",
    "#     \"avg_every\": 100, \n",
    "#     \"bsize\": 64,\n",
    "#     \"breaking_nli_path\": '../dataset/Breaking_NLI/',\n",
    "#     \"comp_path\": '../dataset/Comp/test/',\n",
    "#     \"cudnn_deterministic\": True,\n",
    "#     \"current_run_dir\": 'results_attention_eSNLI_superlongstring',\n",
    "#     \"dec_rnn_dim\": 1024, \n",
    "#     \"decay\": 0.99, \n",
    "#     \"decoder_type\": 'lstm', \n",
    "#     \"directory_expl_to_labels\": '../expl_to_labels/results_expl_to_labels/23:05_20:17:40_sgd,lr=0.1_Enc2048_bs256_gpu2_LRdecay0.99_MAXpool_lrshrink5',\n",
    "#     \"do_image_caption\": False, \n",
    "#     \"dpout_dec\": 0.5,\n",
    "#     \"dpout_enc\": 0.0,\n",
    "#     \"dpout_fc\": 0.0, \n",
    "#     \"early_stopping_epochs\": 50, \n",
    "#     \"enc_rnn_dim\": 2048, \n",
    "#     \"encoder_type\": 'BLSTMEncoder',\n",
    "#     \"esnli_path\": '../dataset/eSNLI/', \n",
    "#     \"eval_batch_size\": 64, \n",
    "#     \"fc_dim\": 512, \n",
    "#     \"gpu\": 2, \n",
    "#     \"gpu_id\": 0,\n",
    "#     \"hard_snli_path\": '../dataset/SNLI_hard/',\n",
    "#     \"lrshrink\": 5,\n",
    "#     \"max_T_decoder\": 40, \n",
    "#     \"max_T_encoder\": 84, \n",
    "#     \"max_norm\": 5.0, \n",
    "#     \"min_freq\": 15, \n",
    "#     \"minlr\": 1e-05, \n",
    "#     \"multinli_path\": '../dataset/MultiNLI/',\n",
    "#     \"n_classes\": 3, \n",
    "#     \"n_epochs\": 20, \n",
    "#     \"n_layers_dec\": 1, \n",
    "#     \"n_train\": -1, \n",
    "#     \"optimizer\": 'sgd,lr=0.1', \n",
    "#     \"pool_type\": 'max', \n",
    "#     \"preproc_expl\": 'preproc1', \n",
    "#     \"print_every\": 500, \n",
    "#     \"results_dir\": 'results_attention_eSNLI_separate',\n",
    "#     \"save_title\": '_decLSTM_sgd,lr=0.1_Enc2048_Dec1024_bs64_gpu4__encT84__decT40_LRdecay0.99_dpout_dec0.5_MAXpool_lrshrink5_init', \n",
    "#     \"seed\": 1234, \n",
    "#     \"separate_att\": True, \n",
    "#     \"sick_path\": '../data/senteval_data/SICK/', \n",
    "#     \"state_path_expl_to_labels\": 'state_dict_best_devacc__devACC96.780__epoch_12_model.pt', \n",
    "#     \"train_set\": 'eSNLI', \n",
    "#     \"train_snli_classif\": False, \n",
    "#     \"use_init\": True, \n",
    "#     \"use_prototype_senteval\": False\n",
    "# }\n",
    "\n",
    "model_state_dict = model_ExplainThenPredictAttention['model_state']\n",
    "model_state_dict[\"decoder.context_proj.weight\"] = model_state_dict.pop(\"decoder.init_proj.weight\")\n",
    "model_state_dict[\"decoder.context_proj.bias\"] = model_state_dict.pop(\"decoder.init_proj.bias\")\n",
    "\n",
    "# print(model_state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_config_att)\n",
    "# print(model_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mutils import get_keys_from_vals, assert_sizes\n",
    "\n",
    "\n",
    "def array_all_true(arr):\n",
    "\tfor i in arr:\n",
    "\t\tif i == False:\n",
    "\t\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "\"\"\"\n",
    "AttentionDecoder for the explanation\n",
    "\"\"\"\n",
    "class AttentionDecoder(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(AttentionDecoder, self).__init__()\n",
    "\t\t\n",
    "\t\tself.decoder_type = config['decoder_type']\n",
    "\t\tself.word_emb_dim = config['word_emb_dim']\n",
    "\t\tself.dec_rnn_dim = config['dec_rnn_dim']\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.dpout_dec = config['dpout_dec']\n",
    "\t\tself.n_vocab = config['n_vocab']\n",
    "\t\tself.word_index = config['word_index']\n",
    "\t\tself.word_vec = config['word_vec']\n",
    "\t\tself.max_T_decoder = config['max_T_decoder']\n",
    "\t\tself.max_T_encoder = config['max_T_encoder']\n",
    "\t\tself.n_layers_dec = config['n_layers_dec']\n",
    "\t\t# for decoder intial state\n",
    "\t\tself.use_init = config['use_init']\n",
    "\t\t# attention type: dot product or linear layer\n",
    "\t\tself.att_type = config['att_type'] # 'lin' or 'dot'\n",
    "\t\t# whether to visualize attention weights\n",
    "\t\tself.att_hid_dim =config['att_hid_dim']\n",
    "\n",
    "\n",
    "\t\tself.sent_dim = 2 * config['enc_rnn_dim']\n",
    "\t\tif config['encoder_type'] in [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"]:\n",
    "\t\t\tself.sent_dim = 4 * self.sent_dim \n",
    "\t\tif config['encoder_type'] == \"LSTMEncoder\":\n",
    "\t\t\tself.sent_dim = self.sent_dim / 2 \n",
    "\n",
    "\t\tassert self.sent_dim == 4096, str(self.sent_dim)\n",
    "\t\t# TODO: remove this when implemented linear attention\n",
    "\t\tassert self.att_type == 'dot'\n",
    "\n",
    "\t\tself.context_proj = nn.Linear(4 * self.sent_dim, self.dec_rnn_dim)\n",
    "\n",
    "\t\tself.att_ht_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_context_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.dec_rnn_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_before_weighting_proj1 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_context_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.dec_rnn_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.att_ht_before_weighting_proj2 = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.sent_dim, self.att_hid_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t)\n",
    "\n",
    "\n",
    "\t\tself.proj_inp_dec = nn.Linear(2 * self.att_hid_dim + self.word_emb_dim, self.dec_rnn_dim)\t\n",
    "\t\tif self.decoder_type == 'gru':\n",
    "\t\t\tself.decoder_rnn = nn.GRU(self.dec_rnn_dim, self.dec_rnn_dim, self.n_layers_dec, bidirectional=False, dropout=self.dpout_dec)\n",
    "\t\telse: # 'lstm'\n",
    "\t\t\tself.decoder_rnn = nn.LSTM(self.dec_rnn_dim, self.dec_rnn_dim, self.n_layers_dec, bidirectional=False, dropout=self.dpout_dec)\n",
    "\n",
    "\t\t# att softmax\n",
    "\t\tself.softmax_att = nn.Softmax(2)\n",
    "\n",
    "\t\t# vocab layer\n",
    "\t\tself.vocab_layer = nn.Linear(self.dec_rnn_dim, self.n_vocab)\n",
    "\n",
    "\n",
    "\tdef forward(self, expl, enc_out_s1, enc_out_s2, s1_embed, s2_embed, mode, visualize):\n",
    "\t\t# expl: Variable(seqlen x bsize x worddim)\n",
    "\t\t# s1/2_embed: Variable(bsize x sent_dim)\n",
    "\t\t\n",
    "\t\tassert mode in ['forloop', 'teacher'], mode\n",
    "\n",
    "\t\tcurrent_T_dec = expl.size(0)\n",
    "\t\tbatch_size = expl.size(1)\n",
    "\t\tassert_sizes(s1_embed, 2, [batch_size, self.sent_dim])\n",
    "\t\tassert_sizes(s2_embed, 2, [batch_size, self.sent_dim])\n",
    "\t\tassert_sizes(expl, 3, [current_T_dec, batch_size, self.word_emb_dim])\n",
    "\t\tassert_sizes(enc_out_s1, 3, [self.max_T_encoder, batch_size, 2 * self.enc_rnn_dim])\n",
    "\t\tassert_sizes(enc_out_s2, 3, [self.max_T_encoder, batch_size, 2 * self.enc_rnn_dim])\n",
    "\n",
    "\t\tcontext = torch.cat([s1_embed, s2_embed, torch.abs(s1_embed - s2_embed), s1_embed * s2_embed], 1).unsqueeze(0)\n",
    "\t\tassert_sizes(context, 3, [1, batch_size, 4 * self.sent_dim])\n",
    "\n",
    "\t\t# init decoder\n",
    "\t\tif self.use_init:\n",
    "\t\t\tinit_0 = self.context_proj(context).expand(self.n_layers_dec, batch_size, self.dec_rnn_dim)\n",
    "\t\telse:\n",
    "\t\t\tinit_0 = Variable(torch.zeros(self.n_layers_dec, batch_size, self.dec_rnn_dim).cuda()).cuda()\n",
    "\n",
    "\t\tinit_state = init_0\n",
    "\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\tinit_state = (init_0, init_0)\n",
    "\n",
    "\t\tself.decoder_rnn.flatten_parameters()\n",
    "\n",
    "\t\tout_expl = None\n",
    "\t\tstate_t = init_state\n",
    "\t\tcontext = self.context_proj(context)\n",
    "\t\tif mode == \"teacher\":\n",
    "\t\t\tfor t_dec in range(current_T_dec):\n",
    "\t\t\t\t# attention over premise\n",
    "\t\t\t\tcontext1 = self.att_context_proj1(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinp_att_1 = self.att_ht_proj1(enc_out_s1).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_1, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_1 = torch.bmm(context1, inp_att_1)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_1 = self.softmax_att(dot_prod_att_1)\n",
    "\t\t\t\tassert_sizes(att_weights_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_1 = torch.bmm(att_weights_1, self.att_ht_before_weighting_proj1(enc_out_s1).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_1 = att_applied_1.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_1, 3, [1, batch_size, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\t# attention over hypothesis\n",
    "\t\t\t\tcontext2 = self.att_context_proj2(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tinp_att_2 = self.att_ht_proj2(enc_out_s2).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_2, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_2 = torch.bmm(context2, inp_att_2)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_2 = self.softmax_att(dot_prod_att_2)\n",
    "\t\t\t\tassert_sizes(att_weights_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_2 = torch.bmm(att_weights_2, self.att_ht_before_weighting_proj2(enc_out_s2).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_2 = att_applied_2.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_2, 3, [1, batch_size, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinput_dec = torch.cat([expl[t_dec].unsqueeze(0), att_applied_perm_1, att_applied_perm_2], 2) \n",
    "\t\t\t\tinput_dec = nn.Dropout(self.dpout_dec)(self.proj_inp_dec(input_dec))\n",
    "\n",
    "\t\t\t\tout_dec, state_t = self.decoder_rnn(input_dec, state_t)\n",
    "\t\t\t\tassert_sizes(out_dec, 3, [1, batch_size, self.dec_rnn_dim])\n",
    "\t\t\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\t\t\tcontext = state_t[0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontext = state_t\n",
    "\n",
    "\t\t\t\tif out_expl is None:\n",
    "\t\t\t\t\tout_expl = out_dec\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tout_expl = torch.cat([out_expl, out_dec], 0)\n",
    "\n",
    "\t\t\tout_expl = self.vocab_layer(out_expl)\n",
    "\t\t\tassert_sizes(out_expl, 3, [current_T_dec, batch_size, self.n_vocab])\n",
    "\t\t\treturn out_expl\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tpred_expls = []\n",
    "\t\t\tfinished = []\n",
    "\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\tpred_expls.append(\"\")\n",
    "\t\t\t\tfinished.append(False)\n",
    "\n",
    "\t\t\tt_dec = 0\n",
    "\t\t\tword_t = expl[0].unsqueeze(0)\n",
    "\t\t\twhile t_dec < self.max_T_decoder and not array_all_true(finished):\n",
    "\t\t\t\t#print \"\\n\\n\\n t: \", t_dec\n",
    "\n",
    "\t\t\t\tassert_sizes(word_t, 3, [1, batch_size, self.word_emb_dim])\n",
    "\t\t\t\tword_embed = torch.zeros(1, batch_size, self.word_emb_dim)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# attention over premise\n",
    "\t\t\t\tcontext1 = self.att_context_proj1(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinp_att_1 = self.att_ht_proj1(enc_out_s1).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_1, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_1 = torch.bmm(context1, inp_att_1)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_1 = self.softmax_att(dot_prod_att_1)\n",
    "\t\t\t\tassert_sizes(att_weights_1, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_1 = torch.bmm(att_weights_1, self.att_ht_before_weighting_proj1(enc_out_s1).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_1, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_1 = att_applied_1.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_1, 3, [1, batch_size, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\t# attention over hypothesis\n",
    "\t\t\t\tcontext2 = self.att_context_proj2(context).permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(context2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tinp_att_2 = self.att_ht_proj2(enc_out_s2).transpose(1,0).transpose(2,1)\n",
    "\t\t\t\tassert_sizes(inp_att_2, 3, [batch_size, self.att_hid_dim, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tdot_prod_att_2 = torch.bmm(context2, inp_att_2)\n",
    "\t\t\t\tassert_sizes(dot_prod_att_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_weights_2 = self.softmax_att(dot_prod_att_2)\n",
    "\t\t\t\tassert_sizes(att_weights_2, 3, [batch_size, 1, self.max_T_encoder])\n",
    "\t\t\t\t\n",
    "\t\t\t\tatt_applied_2 = torch.bmm(att_weights_2, self.att_ht_before_weighting_proj2(enc_out_s2).permute(1, 0, 2))\n",
    "\t\t\t\tassert_sizes(att_applied_2, 3, [batch_size, 1, self.att_hid_dim])\n",
    "\n",
    "\t\t\t\tatt_applied_perm_2 = att_applied_2.permute(1, 0, 2)\n",
    "\t\t\t\tassert_sizes(att_applied_perm_2, 3, [1, batch_size, self.att_hid_dim])\n",
    "\t\t\t\t\n",
    "\t\t\t\tinput_dec = torch.cat([word_t, att_applied_perm_1, att_applied_perm_2], 2) \n",
    "\t\t\t\tinput_dec = self.proj_inp_dec(input_dec)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#print \"att_weights_1[0] \", att_weights_1[0]\n",
    "\t\t\t\t#print \"att_weights_2[0] \", att_weights_2[0]\n",
    "\n",
    "\t\t\t\t# get one visualization from the current batch\n",
    "\t\t\t\tif visualize:\n",
    "\t\t\t\t\tif t_dec == 0:\n",
    "\t\t\t\t\t\tweights_1 = att_weights_1[0]\n",
    "\t\t\t\t\t\tweights_2 = att_weights_2[0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tweights_1 = torch.cat([weights_1, att_weights_1[0]], 0)\n",
    "\t\t\t\t\t\tweights_2 = torch.cat([weights_2, att_weights_2[0]], 0)\n",
    "\n",
    "\t\t\t\tfor ii in range(batch_size):\n",
    "\t\t\t\t\tassert abs(att_weights_1[ii].data.sum() - 1) < 1e-5, str(att_weights_1[ii].data.sum())\n",
    "\t\t\t\t\tassert abs(att_weights_2[ii].data.sum() - 1) < 1e-5, str(att_weights_2[ii].data.sum())\n",
    "\n",
    "\t\t\t\tout_t, state_t = self.decoder_rnn(input_dec, state_t)\n",
    "\t\t\t\tassert_sizes(out_t, 3, [1, batch_size, self.dec_rnn_dim])\n",
    "\t\t\t\tout_t = self.vocab_layer(out_t)\n",
    "\t\t\t\tif self.decoder_type == 'lstm':\n",
    "\t\t\t\t\tcontext = state_t[0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontext = state_t\n",
    "\n",
    "\t\t\t\ti_t = torch.max(out_t, 2)[1].data\n",
    "\t\t\t\tassert_sizes(i_t, 2, [1, batch_size])\n",
    "\t\t\t\tpred_words = get_keys_from_vals(i_t, self.word_index) # array of bs of words at current timestep\n",
    "\t\t\t\tassert len(pred_words) == batch_size, \"pred_words \" + str(len(pred_words)) + \" batch_size \" + str(batch_size)\n",
    "\t\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\t\tif pred_words[i] == '</s>':\n",
    "\t\t\t\t\t\tfinished[i] = True\n",
    "\t\t\t\t\tif not finished[i]:\n",
    "\t\t\t\t\t\tpred_expls[i] += \" \" + pred_words[i]\n",
    "\t\t\t\t\tword_embed[0, i] = torch.from_numpy(self.word_vec[pred_words[i]])\n",
    "\t\t\t\tword_t = Variable(word_embed.cuda()).cuda()\n",
    "\n",
    "\t\t\t\tt_dec += 1\n",
    "\t\t\t\n",
    "\t\t\tif visualize:\n",
    "\t\t\t\tassert weights_1.dim() == 2\n",
    "\t\t\t\tassert weights_1.size(1) == self.max_T_encoder\n",
    "\t\t\t\tassert weights_2.dim() == 2\n",
    "\t\t\t\tassert weights_2.size(1) == self.max_T_encoder\n",
    "\t\t\t\tpred_expls = [pred_expls, weights_1, weights_2]\n",
    "\t\t\treturn pred_expls\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLSTM (max/mean) encoder\n",
    "\"\"\"\n",
    "class BLSTMEncoder(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(BLSTMEncoder, self).__init__()\n",
    "\t\tself.bsize = config['bsize']\n",
    "\t\tself.word_emb_dim = config['word_emb_dim']\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.pool_type = config['pool_type']\n",
    "\t\tself.dpout_enc = config['dpout_enc']\n",
    "\t\tself.max_T_encoder = config['max_T_encoder']\n",
    "\n",
    "\t\tself.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_rnn_dim, 1,\n",
    "\t\t\t\t\t\t\t\tbidirectional=True, dropout=self.dpout_enc)\n",
    "\n",
    "\tdef is_cuda(self):\n",
    "\t\t# either all weights are on cpu or they are on gpu\n",
    "\t\treturn 'cuda' in str(type(self.enc_lstm.bias_hh_l0.data))\n",
    "\n",
    "\tdef forward(self, sent_tuple):\n",
    "\t\t# sent_len: [max_len, ..., min_len] (bsize)\n",
    "\t\t# sent: Variable(seqlen x bsize x worddim)\n",
    "\t\tsent, sent_len = sent_tuple\n",
    "\t\t#assert_sizes(sent, 3, [self.max_T_encoder, sent.size(1), self.word_emb_dim])\n",
    "\n",
    "\t\t# Sort by length (keep idx)\n",
    "\t\tsent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "\t\tsent_len = sent_len.copy()\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "\t\tidx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_sort)\n",
    "\t\tsent = sent.index_select(1, Variable(idx_sort).cuda())\n",
    "\n",
    "\t\t# Handling padding in Recurrent Networks\n",
    "\t\tsent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n",
    "\t\tself.enc_lstm.flatten_parameters()\n",
    "\t\tsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "\t\tpadding_value = 0.0\n",
    "\t\tif self.pool_type == \"max\":\n",
    "\t\t\tpadding_value = -100\n",
    "\t\tsent_output_padding = nn.utils.rnn.pad_packed_sequence(sent_output, False, padding_value)[0]\n",
    "\t\tsent_output = nn.utils.rnn.pad_packed_sequence(sent_output, False, 0)[0]\n",
    "\n",
    "\t\t# Un-sort by length\n",
    "\t\tidx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_unsort)\n",
    "\t\tsent_output = sent_output.index_select(1, Variable(idx_unsort).cuda())\n",
    "\t\tsent_output_padding = sent_output_padding.index_select(1, Variable(idx_unsort).cuda())\n",
    "\t\t\n",
    "\t\t# Pooling\n",
    "\t\tif self.pool_type == \"mean\":\n",
    "\t\t\tsent_len = Variable(torch.FloatTensor(sent_len).cuda()).unsqueeze(1).cuda()\n",
    "\t\t\temb = torch.sum(sent_output_padding, 0).squeeze(0)\n",
    "\t\t\temb = emb / sent_len.expand_as(emb)\n",
    "\t\telif self.pool_type == \"max\":\n",
    "\t\t\temb = torch.max(sent_output_padding, 0)[0]\n",
    "\t\t\tif emb.ndimension() == 3:\n",
    "\t\t\t\temb = emb.squeeze(0)\n",
    "\t\t\t\tassert emb.ndimension() == 2, \"emb.ndimension()=\" + str(emb.ndimension())\n",
    "\n",
    "\t\t# pad with zeros so that max length is the same for all, needed for attention\n",
    "\t\tif sent_output.size(0) < self.max_T_encoder:\n",
    "\t\t\tpad_tensor = Variable(torch.zeros(self.max_T_encoder - sent_output.size(0), sent_output.size(1), sent_output.size(2)).cuda()).cuda()\n",
    "\t\t\tsent_output = torch.cat([sent_output, pad_tensor], 0)\n",
    "\t\t\n",
    "\t\treturn sent_output, emb\n",
    "\n",
    "\n",
    "\n",
    "\tdef set_glove_path(self, glove_path):\n",
    "\t\tself.glove_path = glove_path\n",
    "\n",
    "\tdef get_word_dict(self, sentences, tokenize=True):\n",
    "\t\t# create vocab of words\n",
    "\t\tword_dict = {}\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [s.split() if not tokenize else word_tokenize(s)\n",
    "\t\t\t\t\t for s in sentences]\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tfor word in sent:\n",
    "\t\t\t\tif word not in word_dict:\n",
    "\t\t\t\t\tword_dict[word] = ''\n",
    "\t\tword_dict['<s>'] = ''\n",
    "\t\tword_dict['</s>'] = ''\n",
    "\t\treturn word_dict\n",
    "\n",
    "\tdef get_glove(self, word_dict):\n",
    "\t\tassert hasattr(self, 'glove_path'), \\\n",
    "\t\t\t   'warning : you need to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with glove vectors\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif word in word_dict:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\tprint('Found {0}(/{1}) words with glove vectors'.format(\n",
    "\t\t\t\t\tlen(word_vec), len(word_dict)))\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef get_glove_k(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with k first glove vectors\n",
    "\t\tk = 0\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif k <= K:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\t\t\t\tk += 1\n",
    "\t\t\t\tif k > K:\n",
    "\t\t\t\t\tif word in ['<s>', '</s>']:\n",
    "\t\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "\t\t\t\tif k > K and all([w in word_vec for w in ['<s>', '</s>']]):\n",
    "\t\t\t\t\tbreak\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef build_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\t\tself.word_vec = self.get_glove(word_dict)\n",
    "\t\tprint('Vocab size from within BLSTMEncoder : {0}'.format(len(self.word_vec)))\n",
    "\n",
    "\t# build GloVe vocab with k most frequent words\n",
    "\tdef build_vocab_k_words(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tself.word_vec = self.get_glove_k(K)\n",
    "\t\tprint('Vocab size : {0}'.format(K))\n",
    "\n",
    "\tdef update_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tassert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "\t\t# keep only new words\n",
    "\t\tfor word in self.word_vec:\n",
    "\t\t\tif word in word_dict:\n",
    "\t\t\t\tdel word_dict[word]\n",
    "\n",
    "\t\t# udpate vocabulary\n",
    "\t\tif word_dict:\n",
    "\t\t\tnew_word_vec = self.get_glove(word_dict)\n",
    "\t\t\tself.word_vec.update(new_word_vec)\n",
    "\t\tprint('New vocab size : {0} (added {1} words)'.format(\n",
    "\t\t\t\t\t\tlen(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "\tdef get_batch(self, batch):\n",
    "\t\t# sent in batch in decreasing order of lengths\n",
    "\t\t# batch: (bsize, max_len, word_dim)\n",
    "\t\tembed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "\t\tfor i in range(len(batch)):\n",
    "\t\t\tfor j in range(len(batch[i])):\n",
    "\t\t\t\tembed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "\t\treturn torch.FloatTensor(embed)\n",
    "\n",
    "\tdef prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [['<s>'] + s.split() + ['</s>'] if not tokenize else\n",
    "\t\t\t\t\t ['<s>']+word_tokenize(s)+['</s>'] for s in sentences]\n",
    "\t\tn_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "\t\t# filters words without glove vectors\n",
    "\t\tfor i in range(len(sentences)):\n",
    "\t\t\ts_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "\t\t\tif not s_f:\n",
    "\t\t\t\timport warnings\n",
    "\t\t\t\twarnings.warn('No words in \"{0}\" (idx={1}) have glove vectors. \\\n",
    "\t\t\t\t\t\t\t   Replacing by \"</s>\"..'.format(sentences[i], i))\n",
    "\t\t\t\ts_f = ['</s>']\n",
    "\t\t\tsentences[i] = s_f\n",
    "\n",
    "\t\tlengths = np.array([len(s) for s in sentences])\n",
    "\t\tn_wk = np.sum(lengths)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Nb words kept : {0}/{1} ({2} %)'.format(\n",
    "\t\t\t\t\t\tn_wk, n_w, round((100.0 * n_wk) / n_w, 2)))\n",
    "\n",
    "\t\t# sort by decreasing length\n",
    "\t\tlengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "\t\tsentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "\t\treturn sentences, lengths, idx_sort\n",
    "\n",
    "\tdef encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "\t\ttic = time.time()\n",
    "\t\tsentences, lengths, idx_sort = self.prepare_samples(\n",
    "\t\t\t\t\t\tsentences, bsize, tokenize, verbose)\n",
    "\n",
    "\t\tembeddings = []\n",
    "\t\tfor stidx in range(0, len(sentences), bsize):\n",
    "\t\t\tbatch = Variable(self.get_batch(\n",
    "\t\t\t\t\t\tsentences[stidx:stidx + bsize]), volatile=True).cuda()\n",
    "\t\t\tif self.is_cuda():\n",
    "\t\t\t\tbatch = batch.cuda()\n",
    "\t\t\tbatch = self.forward(\n",
    "\t\t\t\t(batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "\t\t\tembeddings.append(batch)\n",
    "\t\tembeddings = np.vstack(embeddings)\n",
    "\n",
    "\t\t# unsort\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\t\tembeddings = embeddings[idx_unsort]\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Speed : {0} sentences/s ({1} mode, bsize={2})'.format(\n",
    "\t\t\t\t\tround(len(embeddings)/(time.time()-tic), 2),\n",
    "\t\t\t\t\t'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "\t\treturn embeddings\n",
    "\n",
    "\tdef visualize(self, sent, tokenize=True):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\n",
    "\t\tsent = sent.split() if not tokenize else word_tokenize(sent)\n",
    "\t\tsent = [['<s>'] + [word for word in sent if word in self.word_vec] +\n",
    "\t\t\t\t['</s>']]\n",
    "\n",
    "\t\tif ' '.join(sent[0]) == '<s> </s>':\n",
    "\t\t\timport warnings\n",
    "\t\t\twarnings.warn('No words in \"{0}\" have glove vectors. Replacing \\\n",
    "\t\t\t\t\t\t   by \"<s> </s>\"..'.format(sent))\n",
    "\t\tbatch = Variable(self.get_batch(sent), volatile=True).cuda()\n",
    "\n",
    "\t\tif self.is_cuda():\n",
    "\t\t\tbatch = batch.cuda()\n",
    "\t\toutput = self.enc_lstm(batch)[0]\n",
    "\t\toutput, idxs = torch.max(output, 0)\n",
    "\t\t# output, idxs = output.squeeze(), idxs.squeeze()\n",
    "\t\tidxs = idxs.data.cpu().numpy()\n",
    "\t\targmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "\t\t# visualize model\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\tx = range(len(sent[0]))\n",
    "\t\ty = [100.0*n/np.sum(argmaxs) for n in argmaxs]\n",
    "\t\tplt.xticks(x, sent[0], rotation=45)\n",
    "\t\tplt.bar(x, y)\n",
    "\t\tplt.ylabel('%')\n",
    "\t\tplt.title('Visualisation of words importance')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\treturn output, idxs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main module for Natural Language Inference\n",
    "\"\"\"\n",
    "class eSNLIAttention(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(eSNLIAttention, self).__init__()\n",
    "\t\tself.encoder_type = config['encoder_type']\n",
    "\n",
    "\t\tself.encoder = eval(self.encoder_type)(config)\n",
    "\t\tself.decoder = AttentionDecoder(config)\n",
    "\n",
    "\tdef forward(self, s1, s2, expl, mode, visualize):\n",
    "\t\t# s1 : (s1, s1_len)\n",
    "\t\t# s2 : (s2, s2_len)\n",
    "\t\t# expl : Variable(T x bs x 300)\n",
    "        \n",
    "\t\tprint(self.encoder(s1))\n",
    "\n",
    "\t\tu, u_emb = self.encoder(s1) # u = max_T_enc x bs x (2 * enc_dim) ; u_emb = 1 x bs x (2 * enc_dim)\n",
    "\t\tv, v_emb = self.encoder(s2) \n",
    "\n",
    "\t\tout_expl = self.decoder(expl, u, v, u_emb, v_emb, mode, visualize)\n",
    "        \n",
    "\t\t# u = self.encoder(s1) # u = max_T_enc x bs x (2 * enc_dim) ; u_emb = 1 x bs x (2 * enc_dim)\n",
    "\t\t# v = self.encoder(s2) \n",
    "\n",
    "\t\t# out_expl = self.decoder(expl, u, v, u, v, mode, visualize)\n",
    "\t\t\n",
    "\t\treturn out_expl\n",
    "\n",
    "\tdef encode(self, s1):\n",
    "\t\temb = self.encoder(s1)\n",
    "\t\treturn emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "BLSTM (max/mean) encoder\n",
    "\"\"\"\n",
    "class BLSTMEncoder(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(BLSTMEncoder, self).__init__()\n",
    "\t\tself.bsize = config['bsize']\n",
    "\t\tself.word_emb_dim = config['word_emb_dim']\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.pool_type = config['pool_type']\n",
    "\t\tself.dpout_enc = config['dpout_enc']\n",
    "\n",
    "\t\tself.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_rnn_dim, 1,\n",
    "\t\t\t\t\t\t\t\tbidirectional=True, dropout=self.dpout_enc)\n",
    "\n",
    "\tdef is_cuda(self):\n",
    "\t\t# either all weights are on cpu or they are on gpu\n",
    "\t\treturn 'cuda' in str(type(self.enc_lstm.bias_hh_l0.data))\n",
    "\n",
    "\tdef forward(self, sent_tuple):\n",
    "\t\t# sent_len: [max_len, ..., min_len] (bsize)\n",
    "\t\t# sent: Variable(seqlen x bsize x worddim)\n",
    "\t\tsent, sent_len = sent_tuple\n",
    "\n",
    "\t\t# Sort by length (keep idx)\n",
    "\t\tsent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "\t\tsent_len = sent_len.copy()\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "\t\tidx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_sort)\n",
    "\t\tsent = sent.index_select(1, Variable(idx_sort).cuda()).cuda()\n",
    "\n",
    "\t\t# Handling padding in Recurrent Networks\n",
    "\t\tsent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n",
    "\t\tself.enc_lstm.flatten_parameters()\n",
    "\t\tsent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "\t\tpadding_value = 0.0\n",
    "\t\tif self.pool_type == \"max\":\n",
    "\t\t\tpadding_value = -100\n",
    "\t\tsent_output = nn.utils.rnn.pad_packed_sequence(sent_output, False, padding_value)[0]\n",
    "\n",
    "\t\t# Un-sort by length\n",
    "\t\tidx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "\t\t\telse torch.from_numpy(idx_unsort)\n",
    "\t\tsent_output = sent_output.index_select(1, Variable(idx_unsort).cuda()).cuda()\n",
    "\t\tsent_len=sent_len[idx_unsort]\n",
    "\n",
    "\t\t# Pooling\n",
    "\t\tif self.pool_type == \"mean\":\n",
    "\t\t\tsent_len = Variable(torch.FloatTensor(sent_len)).unsqueeze(1).cuda()\n",
    "\t\t\temb = torch.sum(sent_output, 0).squeeze(0)\n",
    "\t\t\temb = emb / sent_len.expand_as(emb)\n",
    "\t\telif self.pool_type == \"max\":\n",
    "\t\t\temb = torch.max(sent_output, 0)[0]\n",
    "\t\t\tif emb.ndimension() == 3:\n",
    "\t\t\t\temb = emb.squeeze(0)\n",
    "\t\t\t\tassert emb.ndimension() == 2, \"emb.ndimension()=\" + str(emb.ndimension())\n",
    "\n",
    "\t\treturn emb\n",
    "\n",
    "\tdef set_glove_path(self, glove_path):\n",
    "\t\tself.glove_path = glove_path\n",
    "\n",
    "\tdef get_word_dict(self, sentences, tokenize=True):\n",
    "\t\t# create vocab of words\n",
    "\t\tword_dict = {}\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [s.split() if not tokenize else word_tokenize(s)\n",
    "\t\t\t\t\t for s in sentences]\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tfor word in sent:\n",
    "\t\t\t\tif word not in word_dict:\n",
    "\t\t\t\t\tword_dict[word] = ''\n",
    "\t\tword_dict['<s>'] = ''\n",
    "\t\tword_dict['</s>'] = ''\n",
    "\t\treturn word_dict\n",
    "\n",
    "\tdef get_glove(self, word_dict):\n",
    "\t\tassert hasattr(self, 'glove_path'), \\\n",
    "\t\t\t   'warning : you need to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with glove vectors\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif word in word_dict:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\tprint('Found {0}(/{1}) words with glove vectors'.format(\n",
    "\t\t\t\t\tlen(word_vec), len(word_dict)))\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef get_glove_k(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\t# create word_vec with k first glove vectors\n",
    "\t\tk = 0\n",
    "\t\tword_vec = {}\n",
    "\t\twith open(self.glove_path) as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tword, vec = line.split(' ', 1)\n",
    "\t\t\t\tif k <= K:\n",
    "\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\t\t\t\t\tk += 1\n",
    "\t\t\t\tif k > K:\n",
    "\t\t\t\t\tif word in ['<s>', '</s>']:\n",
    "\t\t\t\t\t\tword_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "\t\t\t\tif k > K and all([w in word_vec for w in ['<s>', '</s>']]):\n",
    "\t\t\t\t\tbreak\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef build_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\t\tself.word_vec = self.get_glove(word_dict)\n",
    "\t\tprint('Vocab size from within BLSTMEncoder : {0}'.format(len(self.word_vec)))\n",
    "\n",
    "\t# build GloVe vocab with k most frequent words\n",
    "\tdef build_vocab_k_words(self, K):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tself.word_vec = self.get_glove_k(K)\n",
    "\t\tprint('Vocab size : {0}'.format(K))\n",
    "\n",
    "\tdef update_vocab(self, sentences, tokenize=True):\n",
    "\t\tassert hasattr(self, 'glove_path'), 'warning : you need \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t to set_glove_path(glove_path)'\n",
    "\t\tassert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "\t\tword_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "\t\t# keep only new words\n",
    "\t\tfor word in self.word_vec:\n",
    "\t\t\tif word in word_dict:\n",
    "\t\t\t\tdel word_dict[word]\n",
    "\n",
    "\t\t# udpate vocabulary\n",
    "\t\tif word_dict:\n",
    "\t\t\tnew_word_vec = self.get_glove(word_dict)\n",
    "\t\t\tself.word_vec.update(new_word_vec)\n",
    "\t\tprint('New vocab size : {0} (added {1} words)'.format(\n",
    "\t\t\t\t\t\tlen(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "\tdef get_batch(self, batch):\n",
    "\t\t# sent in batch in decreasing order of lengths\n",
    "\t\t# batch: (bsize, max_len, word_dim)\n",
    "\t\tembed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "\t\tfor i in range(len(batch)):\n",
    "\t\t\tfor j in range(len(batch[i])):\n",
    "\t\t\t\tembed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "\t\treturn torch.FloatTensor(embed)\n",
    "\n",
    "\tdef prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\t\tsentences = [['<s>'] + s.split() + ['</s>'] if not tokenize else\n",
    "\t\t\t\t\t ['<s>']+word_tokenize(s)+['</s>'] for s in sentences]\n",
    "\t\tn_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "\t\t# filters words without glove vectors\n",
    "\t\tfor i in range(len(sentences)):\n",
    "\t\t\ts_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "\t\t\tif not s_f:\n",
    "\t\t\t\timport warnings\n",
    "\t\t\t\twarnings.warn('No words in \"{0}\" (idx={1}) have glove vectors. \\\n",
    "\t\t\t\t\t\t\t   Replacing by \"</s>\"..'.format(sentences[i], i))\n",
    "\t\t\t\ts_f = ['</s>']\n",
    "\t\t\tsentences[i] = s_f\n",
    "\n",
    "\t\tlengths = np.array([len(s) for s in sentences])\n",
    "\t\tn_wk = np.sum(lengths)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Nb words kept : {0}/{1} ({2} %)'.format(\n",
    "\t\t\t\t\t\tn_wk, n_w, round((100.0 * n_wk) / n_w, 2)))\n",
    "\n",
    "\t\t# sort by decreasing length\n",
    "\t\tlengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "\t\tsentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "\t\treturn sentences, lengths, idx_sort\n",
    "\n",
    "\tdef encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "\t\ttic = time.time()\n",
    "\t\tsentences, lengths, idx_sort = self.prepare_samples(\n",
    "\t\t\t\t\t\tsentences, bsize, tokenize, verbose)\n",
    "\n",
    "\t\tembeddings = []\n",
    "\t\tfor stidx in range(0, len(sentences), bsize):\n",
    "\t\t\tbatch = Variable(self.get_batch(\n",
    "\t\t\t\t\t\tsentences[stidx:stidx + bsize]), volatile=True).cuda()\n",
    "\t\t\tif self.is_cuda():\n",
    "\t\t\t\tbatch = batch.cuda()\n",
    "\t\t\tbatch = self.forward(\n",
    "\t\t\t\t(batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "\t\t\tembeddings.append(batch)\n",
    "\t\tembeddings = np.vstack(embeddings)\n",
    "\n",
    "\t\t# unsort\n",
    "\t\tidx_unsort = np.argsort(idx_sort)\n",
    "\t\tembeddings = embeddings[idx_unsort]\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Speed : {0} sentences/s ({1} mode, bsize={2})'.format(\n",
    "\t\t\t\t\tround(len(embeddings)/(time.time()-tic), 2),\n",
    "\t\t\t\t\t'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "\t\treturn embeddings\n",
    "\n",
    "\tdef visualize(self, sent, tokenize=True):\n",
    "\t\tif tokenize:\n",
    "\t\t\tfrom nltk.tokenize import word_tokenize\n",
    "\n",
    "\t\tsent = sent.split() if not tokenize else word_tokenize(sent)\n",
    "\t\tsent = [['<s>'] + [word for word in sent if word in self.word_vec] +\n",
    "\t\t\t\t['</s>']]\n",
    "\n",
    "\t\tif ' '.join(sent[0]) == '<s> </s>':\n",
    "\t\t\timport warnings\n",
    "\t\t\twarnings.warn('No words in \"{0}\" have glove vectors. Replacing \\\n",
    "\t\t\t\t\t\t   by \"<s> </s>\"..'.format(sent))\n",
    "\t\tbatch = Variable(self.get_batch(sent), volatile=True).cuda()\n",
    "\n",
    "\t\tif self.is_cuda():\n",
    "\t\t\tbatch = batch.cuda()\n",
    "\t\toutput = self.enc_lstm(batch)[0]\n",
    "\t\toutput, idxs = torch.max(output, 0)\n",
    "\t\t# output, idxs = output.squeeze(), idxs.squeeze()\n",
    "\t\tidxs = idxs.data.cpu().numpy()\n",
    "\t\targmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "\t\t# visualize model\n",
    "\t\timport matplotlib.pyplot as plt\n",
    "\t\tx = range(len(sent[0]))\n",
    "\t\ty = [100.0*n/np.sum(argmaxs) for n in argmaxs]\n",
    "\t\tplt.xticks(x, sent[0], rotation=45)\n",
    "\t\tplt.bar(x, y)\n",
    "\t\tplt.ylabel('%')\n",
    "\t\tplt.title('Visualisation of words importance')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\treturn output, idxs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main module for Natural Language Inference\n",
    "\"\"\"\n",
    "class ExplToLabelsNet(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(ExplToLabelsNet, self).__init__()\n",
    "\n",
    "\t\t# classifier\n",
    "\t\tself.nonlinear_fc = config['nonlinear_fc']\n",
    "\t\tself.fc_dim = config['fc_dim']\n",
    "\t\tself.n_classes = 3\n",
    "\t\tself.enc_rnn_dim = config['enc_rnn_dim']\n",
    "\t\tself.encoder_type = config['encoder_type']\n",
    "\t\tself.dpout_fc = config['dpout_fc']\n",
    "\n",
    "\t\tself.encoder = eval(self.encoder_type)(config)\n",
    "\t\tself.inputdim = 2*self.enc_rnn_dim\n",
    "\t\tself.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "\t\t\t\t\t\t[\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "\t\tself.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "\t\t\t\t\t\t\t\t\t\telse self.inputdim\n",
    "\t\tif self.nonlinear_fc:\n",
    "\t\t\tself.classifier = nn.Sequential(\n",
    "\t\t\t\tnn.Dropout(p=self.dpout_fc),\n",
    "\t\t\t\tnn.Linear(self.inputdim, self.fc_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\tnn.Dropout(p=self.dpout_fc),\n",
    "\t\t\t\tnn.Linear(self.fc_dim, self.fc_dim),\n",
    "\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\tnn.Dropout(p=self.dpout_fc),\n",
    "\t\t\t\tnn.Linear(self.fc_dim, self.n_classes),\n",
    "\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.classifier = nn.Sequential(\n",
    "\t\t\t\tnn.Linear(self.inputdim, self.fc_dim),\n",
    "\t\t\t\tnn.Linear(self.fc_dim, self.fc_dim),\n",
    "\t\t\t\tnn.Linear(self.fc_dim, self.n_classes)\n",
    "\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, expl):\n",
    "\t\t# expl : ( Variable(T x bs x 300), lens_expl)\n",
    "\n",
    "\t\tenc_out_expl = self.encoder(expl)\n",
    "\t\tout_label = self.classifier(enc_out_expl)\n",
    "\n",
    "\t\treturn out_label\n",
    "\n",
    "\tdef encode(self, s1):\n",
    "\t\temb = self.encoder(s1)\n",
    "\t\treturn emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CodingStuffNoSpace\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EvalParams(object):\n",
    "    pass\n",
    "\n",
    "eval_params = EvalParams()\n",
    "eval_params.directory = \"./\"\n",
    "eval_params.state_path = MODEL_PATH_EXPLAINTHENPREDICTATTENTION\n",
    "eval_params.eval_batch_size = 32\n",
    "eval_params.directory_expl_to_labels = \"./\"\n",
    "eval_params.state_path_expl_to_label = MODEL_PATH_EXPLSTOLABELS\n",
    "eval_params.eval_just_snli = True\n",
    "\n",
    "# setattr(eval_params, \"directory\", './')\n",
    "# setattr(eval_params, \"state_path\", MODEL_PATH_EXPLAINTHENPREDICTATTENTION)\n",
    "# setattr(eval_params, \"eval_batch_size\", 32)\n",
    "# setattr(eval_params, \"directory_expl_to_labels\", './')\n",
    "# setattr(eval_params, \"state_path_expl_to_label\", './')\n",
    "\n",
    "\n",
    "att_net = eSNLIAttention(model_config_att).cuda()\n",
    "att_net.load_state_dict(model_state_dict)\n",
    "params = model_ExplainThenPredictAttention['params']\n",
    "# assert params.separate_att == eval_params.separate_att, \"params.separate_att \" + str(params.separate_att)\n",
    "params.word_vec_expl = model_config_att['word_vec']\n",
    "params.current_run_dir = eval_params.directory\n",
    "params.eval_batch_size = eval_params.eval_batch_size\n",
    "params.eval_just_snli  = eval_params.eval_just_snli\n",
    "\n",
    "state_expl_to_labels = torch.load(MODEL_PATH_EXPLSTOLABELS, encoding = 'latin1')\n",
    "model_config_expl_to_label = state_expl_to_labels['config_model']\n",
    "model_state_expl_to_label = state_expl_to_labels['model_state']\n",
    "expl_net = ExplToLabelsNet(model_config_expl_to_label).cuda()\n",
    "expl_net.load_state_dict(model_state_expl_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i \"eval_attention.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_idx = params.word_index[\"<p>\"]\n",
    "# criterion_expl = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()\n",
    "# criterion_expl.size_average = False\n",
    "\n",
    "# params.esnli_path = ESNLI_PATH\n",
    "\n",
    "# eval_all(att_net, expl_net, criterion_expl, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CodingStuffNoSpace\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_ExplainThenPredictAttention[\"model_state\"]\n",
    "\n",
    "myModel = eSNLIAttention(model_config_att)\n",
    "myModel.load_state_dict(model_ExplainThenPredictAttention[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eSNLI test 9824\n",
      "../data/eSNLI dev 9842\n",
      "../data/eSNLI TRAIN  549367\n",
      "../data/eSNLI dev 9842\n",
      "../data/eSNLI test 9824\n",
      "Found 39146(/43686) words with glove vectors\n",
      "Vocab size : 39146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elvis\\Desktop\\Things\\Jupyter Stuff\\CS4248\\Project\\ghrepo\\4248-project\\src\\eval_attention.py:274: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  eval(data_type)[split] = np.array([['<s>'] +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNLI_TEST\n",
      "Final SNLI example from snli_test\n",
      "Sentence1:   <s> This church choir sings to the masses as they sing joyous songs from the book at a church . </s>  LENGHT:  21\n",
      "Sentence2:   <s> The church has cracks in the ceiling . </s>  LENGHT:  10\n",
      "Gold label:   neutral\n",
      "Explanation 1 :   <s> not all churches have cracks in the ceiling </s>\n",
      "Target expl 1 :   not all churches have cracks in the ceiling </s> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p>  LENGHT:  9\n",
      "Explanation 2 :   <s> there is no indication that there are cracks in the ceiling of the church . </s>\n",
      "Target expl 2 :   there is no indication that there are cracks in the ceiling of the church . </s> <p> <p> <p> <p>  LENGHT:  16\n",
      "Explanation 3 :   <s> not all churches have cracks in the ceiling . </s>\n",
      "Target expl 3 :   not all churches have cracks in the ceiling . </s> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p> <p>  LENGHT:  10\n",
      "tensor([[ 0.0955,  0.0673,  0.0350,  ...,  0.0765,  0.0103,  0.0797],\n",
      "        [ 0.0955,  0.0673,  0.0350,  ...,  0.0765,  0.0103,  0.0797],\n",
      "        [ 0.0955,  0.0673,  0.0350,  ...,  0.0765,  0.0103,  0.0797],\n",
      "        ...,\n",
      "        [-0.0324,  0.0324,  0.0155,  ...,  0.0245,  0.0059,  0.0685],\n",
      "        [ 0.0396,  0.0056,  0.1574,  ...,  0.1767,  0.0157,  0.0162],\n",
      "        [ 0.0396,  0.0056,  0.1574,  ...,  0.1767,  0.0157,  0.0162]],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "input dim: 2 required dim 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m criterion_expl\u001b[38;5;241m.\u001b[39msize_average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m params\u001b[38;5;241m.\u001b[39mesnli_path \u001b[38;5;241m=\u001b[39m ESNLI_PATH\n\u001b[1;32m---> 10\u001b[0m \u001b[43meval_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpl_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_expl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Things\\Jupyter Stuff\\CS4248\\Project\\ghrepo\\4248-project\\src\\eval_attention.py:281\u001b[0m, in \u001b[0;36meval_all\u001b[1;34m(esnli_net, expl_to_labels_net, criterion_expl, params)\u001b[0m\n\u001b[0;32m    274\u001b[0m \t\t\u001b[38;5;28meval\u001b[39m(data_type)[split] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    275\u001b[0m \t\t\t[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_vec] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    276\u001b[0m \t\t\t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28meval\u001b[39m(data_type)[split]])\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# SNLI\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m test_acc, test_bleu_score, test_ppl \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_snli_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mesnli_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpl_to_labels_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_expl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnli_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnli_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnli_dev_no_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnli_test_no_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_run_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m#final_dev_acc, dev_bleu_score, final_dev_ppl = evaluate_snli_final(esnli_net, expl_to_labels_net, criterion_expl, 'snli_dev', snli_dev, snli_dev_no_unk, snli_test_no_unk, word_vec, word_index, batch_size, print_every, current_run_dir, visualize=False)\u001b[39;00m\n\u001b[0;32m    284\u001b[0m final_dev_acc, dev_bleu_score, final_dev_ppl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\Things\\Jupyter Stuff\\CS4248\\Project\\ghrepo\\4248-project\\src\\eval_attention.py:90\u001b[0m, in \u001b[0;36mevaluate_snli_final\u001b[1;34m(esnli_net, expl_to_labels_net, criterion_expl, dataset, data, snli_dev_no_unk, snli_test_no_unk, word_vec, word_index, batch_size, print_every, current_run_dir, visualize)\u001b[0m\n\u001b[0;32m     76\u001b[0m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget expl \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m :  \u001b[39m\u001b[38;5;124m\"\u001b[39m, get_sentence_from_indices(word_index, tgt_expl_batch[:, \u001b[38;5;241m0\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m LENGHT: \u001b[39m\u001b[38;5;124m\"\u001b[39m, lens_tgt_expl[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     79\u001b[0m \t\u001b[38;5;66;03m# model forward, tgt_labels is still None bcs in test mode we get the predicted labels\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \t\u001b[38;5;66;03m# out_expl = esnli_net((s1_batch, s1_len), (s2_batch, s2_len), input_expl_batch, mode=\"teacher\", visualize=False)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \t\u001b[38;5;66;03m# # ppl\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \t\u001b[38;5;66;03m# \tprint(\"Decoded explanation \" + str(index) + \" :  \", get_sentence_from_indices(word_index, answer_idx[:, 0]))\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \t\u001b[38;5;66;03m# \tprint(\"\\n\")\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m pred_expls \u001b[38;5;241m=\u001b[39m \u001b[43mesnli_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_expl_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforloop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n\u001b[0;32m     92\u001b[0m \tweights_1 \u001b[38;5;241m=\u001b[39m pred_expls[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mD:\\CodingStuffNoSpace\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36meSNLIAttention.forward\u001b[1;34m(self, s1, s2, expl, mode, visualize)\u001b[0m\n\u001b[0;32m    574\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(s1) \u001b[38;5;66;03m# u = max_T_enc x bs x (2 * enc_dim) ; u_emb = 1 x bs x (2 * enc_dim)\u001b[39;00m\n\u001b[0;32m    575\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(s2) \n\u001b[1;32m--> 577\u001b[0m out_expl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_expl\n",
      "File \u001b[1;32mD:\\CodingStuffNoSpace\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mAttentionDecoder.forward\u001b[1;34m(self, expl, enc_out_s1, enc_out_s2, s1_embed, s2_embed, mode, visualize)\u001b[0m\n\u001b[0;32m    108\u001b[0m assert_sizes(s2_embed, \u001b[38;5;241m2\u001b[39m, [batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msent_dim])\n\u001b[0;32m    109\u001b[0m assert_sizes(expl, \u001b[38;5;241m3\u001b[39m, [current_T_dec, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_emb_dim])\n\u001b[1;32m--> 110\u001b[0m \u001b[43massert_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_T_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_rnn_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m assert_sizes(enc_out_s2, \u001b[38;5;241m3\u001b[39m, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_T_encoder, batch_size, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_rnn_dim])\n\u001b[0;32m    113\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([s1_embed, s2_embed, torch\u001b[38;5;241m.\u001b[39mabs(s1_embed \u001b[38;5;241m-\u001b[39m s2_embed), s1_embed \u001b[38;5;241m*\u001b[39m s2_embed], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Things\\Jupyter Stuff\\CS4248\\Project\\ghrepo\\4248-project\\src\\mutils.py:274\u001b[0m, in \u001b[0;36massert_sizes\u001b[1;34m(t, dims, sizes)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_sizes\u001b[39m(t, dims, sizes):\n\u001b[1;32m--> 274\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput dim: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t\u001b[38;5;241m.\u001b[39msize())) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m required dim \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(dims)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dims):\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39msize(i) \u001b[38;5;241m==\u001b[39m sizes[i], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min size \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(t\u001b[38;5;241m.\u001b[39msize(i)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(sizes[i])\n",
      "\u001b[1;31mAssertionError\u001b[0m: input dim: 2 required dim 3"
     ]
    }
   ],
   "source": [
    "# myModel((\"I like to eat\", 4), (\"I like to drink too\", 5), 856800,\"forloop\", False)\n",
    "myModel.eval()\n",
    "myModel.to('cuda')\n",
    "\n",
    "pad_idx = params.word_index[\"<p>\"]\n",
    "criterion_expl = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()\n",
    "criterion_expl.size_average = False\n",
    "\n",
    "params.esnli_path = ESNLI_PATH\n",
    "eval_all(myModel, expl_net, criterion_expl, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
